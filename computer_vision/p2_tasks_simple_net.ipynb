{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p1_tasks_simple_net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo_x1IB1DuP1",
        "colab_type": "text"
      },
      "source": [
        "**Mount directories and copy scripts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa8x1dA226aY",
        "colab_type": "code",
        "outputId": "96fa2542-3b61-40f9-8532-9dcbd991b275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# download data from google drive\n",
        "# download conversion script\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "shutil.copy(\"/content/drive/My Drive/Colab Notebooks/cifar_10_converter.py\", \"./\")  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./cifar_10_converter.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWFhyxgSD56l",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srFtnideDLFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "DATA_DIR = 'data/cifar-10'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ApNm3G29Su",
        "colab_type": "code",
        "outputId": "e3191826-3716-47f2-aa1c-9a817444ed89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# convert original CIFAR-10 data to \"usable\" pickled format\n",
        "\n",
        "import os\n",
        "from cifar_10_converter import convert\n",
        "\n",
        "if os.path.isdir(DATA_DIR):\n",
        "  shutil.rmtree(DATA_DIR)\n",
        "os.makedirs(DATA_DIR)\n",
        "convert('/content/drive/My Drive/data/cifar-10/raw', DATA_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test batch was converted\n",
            "Train batch 1 of 5 was converted\n",
            "Train batch 2 of 5 was converted\n",
            "Train batch 3 of 5 was converted\n",
            "Train batch 4 of 5 was converted\n",
            "Train batch 5 of 5 was converted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKSEPSCCECc0",
        "colab_type": "text"
      },
      "source": [
        "**Define helpers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBsDTM29ECBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrappers\n",
        "\n",
        "\n",
        "def get_weights(size, std, name): \n",
        "  weights = ...  # define variable, init with random values\n",
        "  return weights\n",
        "\n",
        "\n",
        "def get_biases(size, name):\n",
        "  biases = ...  # define variable, init with zeros or you may change\n",
        "                # function definition\n",
        "  return biases\n",
        "\n",
        "\n",
        "def add_conv_layer(tensor, k_size, stride, in_ch, out_ch, \n",
        "                   padding='SAME', act_fn=None, name=None):\n",
        "  with tf.variable_scope(name):\n",
        "    weights = get_weights(..... , ..., 'weights') # set propper arguments\n",
        "    biases =  get_biases(...., 'biases') # set proper arguments\n",
        "\n",
        "    outputs = tf.nn.conv2d(...)  # set proper arguments\n",
        "    \n",
        "    outputs = ... # add biases here\n",
        "    outputs = ... # add activation fnction here\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "\n",
        "def add_pooling_layer(tensor, k_size, stride, padding='SAME', name=None):\n",
        "  with tf.variable_scope(name):\n",
        "    outputs = tf.nn.max_pool2d(...)  # set proper parameters here\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def add_fc_layer(tensor, in_num, out_num, act_fn=None, name=None):\n",
        "  with tf.variable_scope(name):\n",
        "    weights = get_weights(..... , ..., 'weights') # set propper arguments\n",
        "    biases =  get_biases(...., 'biases') # set proper arguments\n",
        "    \n",
        "    outputs = tf.matmul(...)  # set proper arguments\n",
        "    outputs = tf.nn.bias_add(....) # set proper arguments\n",
        "    outputs = ... # set proper arguments\n",
        "    return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UELWUTNlD3sg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Define Training Routines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7-B7FlEEjVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimizer(lr_schedule, momentum):\n",
        "  global_step = tf.Variable(0, 'global_step')\n",
        "  \n",
        "  learning_rate = tf.train.exponential_decay(\n",
        "      lr_schedule['initial'],\n",
        "      global_step,\n",
        "      lr_schedule['step'],\n",
        "      lr_schedule['decay'],\n",
        "      staircase=True\n",
        "  )\n",
        "  \n",
        "  # select one of the optimizer here:\n",
        "  \n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "#   optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "  \n",
        "  # you may use Adam, AdaGrad\n",
        "  return optimizer, global_step\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-laUBOTmOt",
        "colab_type": "text"
      },
      "source": [
        "**Define Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-JOUn4hTiIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lviv_net(tensor, is_training=None, name=None):\n",
        "  af = tf.nn.relu\n",
        "  name = name if name is not None else 'lviv_net'\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "    # define networktopology here\n",
        "    # please use defined wrappers\n",
        "      \n",
        "    return tensor\n",
        "\n",
        "  \n",
        "def get_cls_loss(labels, logits, name=\"\"):\n",
        "  with tf.variable_scope(name):\n",
        "    loss = ...  # define loss here\n",
        "  return loss\n",
        "\n",
        "def add_weights_decay(loss, w_decay, name=\"\"):\n",
        "  with tf.variable_scope(name):\n",
        "    weights = tf.get_collection('WEIGHTS')  # do not forget to add weights in the collection in your wrapper\n",
        "    loss += ...  # add weights L2 regularization\n",
        "  return loss\n",
        "\n",
        "def get_acc(labels, logits, name=\"\"):\n",
        "  \n",
        "  acc = ...  # calculate accuracy here\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKkI9Zh9Kp0Z",
        "colab_type": "text"
      },
      "source": [
        "**Training iteslf**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm2llTFYIbT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from cifar_10_converter import unpickle\n",
        "\n",
        "\n",
        "# read dataset\n",
        "ds_train = unpickle(os.path.join(DATA_DIR, 'train.pkl'))\n",
        "ds_test = unpickle(os.path.join(DATA_DIR, 'test.pkl'))\n",
        "\n",
        "#setup optimization parameters\n",
        "test_num = len(ds_test['images'])\n",
        "images_num = len(ds_train['images'])\n",
        "IMAGE_SIZE = 32\n",
        "SCALE_TO = 32\n",
        "INPUT_SIZE = 32\n",
        "CLASSES_NUM = 10\n",
        "lr_schedule = {\n",
        "    'initial': 0.01,\n",
        "    'step': 100 * images_num,\n",
        "    'decay': 0.1\n",
        "}\n",
        "weights_decay = 0.0005\n",
        "momentum = 0.9\n",
        "batch_size = 250\n",
        "epoch_num = 200\n",
        "#---\n",
        "\n",
        "tf.reset_default_graph()  # will allow you to restart the cell without problem\n",
        "tf.set_random_seed(27)    # set random seed\n",
        "\n",
        "optimizer, global_step = get_optimizer(lr_schedule, momentum)\n",
        "\n",
        "\n",
        "# Create Model\n",
        "feeded_img = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_SIZE, IMAGE_SIZE, 3), name='feeded_img')\n",
        "input_lbl = tf.placeholder(tf.int32, shape=(batch_size,), name='input_lbl')\n",
        "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
        "\n",
        "# Do augmentation techniques\n",
        "# at least:\n",
        "#  - random crop\n",
        "#  - resize\n",
        "# you may also try brightness, contrast, etc.\n",
        "  \n",
        "  \n",
        "# exctract mean pixel\n",
        "# scale to [-1, 1]\n",
        "input_img -= 128.\n",
        "input_img /= 128.\n",
        "\n",
        "logits = lviv_net(input_img, is_training=is_training)\n",
        "\n",
        "loss = get_cls_loss(input_lbl, logits, name=\"loss\")\n",
        "add_weights_decay(loss, weights_decay, name=\"wdecay\")\n",
        "\n",
        "acc = get_acc(input_lbl, logits, name=\"accuracy\")\n",
        "\n",
        "# Create Training routines\n",
        "train_op = optimizer.minimize(\n",
        "    loss=loss,\n",
        "    global_step=global_step\n",
        ")\n",
        "\n",
        "#  init variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  # Run the initializer\n",
        "  sess.run(init)\n",
        "  max_acc = 0.\n",
        "  # training loop\n",
        "  for epoch in range(epoch_num):\n",
        "    for i in range(0, images_num, batch_size):\n",
        "      batch_x = ds_train['images'][i:i+batch_size]\n",
        "      batch_y = ds_train['labels'][i:i+batch_size]\n",
        "      \n",
        "      # do training step here\n",
        "      _, loss_value, acc_value = sess.run(\n",
        "          [.....],   # set proper arguments here\n",
        "          feed_dict={\n",
        "              feeded_img: ....,   # set proper arguments here\n",
        "              input_lbl: ....,   # set proper arguments here\n",
        "              is_training: True\n",
        "          }\n",
        "      )\n",
        "      \n",
        "      if i % images_num == 0:\n",
        "        test_loss = 0.\n",
        "        test_acc = 0.\n",
        "        for j in range(0, test_num, batch_size):\n",
        "          test_x = ds_test['images'][j:j+batch_size]\n",
        "          test_y = ds_test['labels'][j:j+batch_size]\n",
        "          test_loss_value, test_acc_value = sess.run(\n",
        "              [...],    # set proper arguments here\n",
        "              feed_dict={\n",
        "                  feeded_img: ...,     # set proper arguments here\n",
        "                  input_lbl: ...,      # set proper arguments here\n",
        "                  is_training: False\n",
        "              }\n",
        "          )\n",
        "          test_loss += test_loss_value\n",
        "          test_acc += test_acc_value\n",
        "        test_loss /= (test_num / batch_size)\n",
        "        test_acc /= (test_num / batch_size)\n",
        "        if max_acc < test_acc:\n",
        "          max_acc = test_acc\n",
        "        print('epoch {} done. Accuracy={:.2f}/{:.2f} ({:.2f})'\n",
        "              ' Loss={:.3f}/{:.3f}'\n",
        "              .format(epoch+1, acc_value*100, test_acc*100, max_acc*100, \n",
        "                      loss_value, test_loss))\n",
        "        \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwiaBma7WOYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}