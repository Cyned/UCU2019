{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hut8vR9t8YWn"
   },
   "source": [
    "# Typical NLP-heavy problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kuoympuf8YWp"
   },
   "source": [
    "* *Classification*: Classifying documents into particular categories (giving labels).\n",
    "* *Regression*: Predict numerical values\n",
    "* *Clustering*: Separating documents into non- overlapping subsets.\n",
    "* *Ranking*: For a given input, rank documents according to some metric. \n",
    "* *Association rule mining*: Infer likely associations patterns in data.\n",
    "* *Structured output*: Bounding boxes, parse trees, etc.\n",
    "* *Sequence-to-sequence*: For a given input/source text, generate output annotations or another text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6LV3tKgEi-g"
   },
   "source": [
    "To address many of the challenges above, several important things has to be clarified beforehand:\n",
    "\n",
    "1. Set the research/exploration goal (e.g., how heavy will be the earthquake on a given day).\n",
    "2. Make a hypothesis ( e.g., strength of the earthquake is an informative signal).\n",
    "3. Collect the data (e.g., collect historical strength of the quakes on eachday).\n",
    "4. Test the hypothesis (e.g., train a model using the data)\n",
    "5. Analyze the results (e.g., are results better than existing systems).\n",
    "6. Reach a conclusion (e.g., the model should be used or not because of A, B, C).\n",
    "7. Refine hypothesis and repeat. (e.g., time of the year could be a useful signal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C26qNlWP8YWt"
   },
   "source": [
    "Below is the table of the approaches that can be applied to the problems/applications above.\n",
    "\n",
    "| Type                   | Input   | Clarifications|\n",
    "|------------------------|---------|---------------|\n",
    "|   Rule-based           |Explicit linguistic patterns, <br>lexicons, etc.| Such an approach always have predefined behaviour and usually can't generalize usually. |\n",
    "|   Supervised           | Training examples: typically <br>tuples of the form <br>(features, label) | Here input features are usually some vectorized, transformed, normalized input representation.|\n",
    "|   Semi-supervised or <br> Pseudo-relevance <br>feedback   | Same as in supervised base, <br>but also results of the prediction <br>(e.g. with greatest confidence) <br>are used as input pairs.   | Once we trained the system on the input (feature, label), we label unknown examples <br>with the model. If the confidence of the label is high (on that unseen example), <br>we add those examples to the input set and retrain the model. |\n",
    "|   Distant supervision  | Same as for supervised learning, <br>but (feature, label) pairs does not <br> come from the annotation, but from <br>some heuristic-based annotation.   | Rather than annotating thousands of examples (documents, sentences, words, etc.), <br>we take a few examples of the class and try to generalize and match those <br>in the domain-specific corpus. Such systems first generate or extract examples that are <br> very similar or slight modifications of the labelled input examples. For example, if you need to <br> find all the sentences about Obama's marriage, you would search for all sentences that match <br> Michelle and Barack Obama. Furthermore, those examples could be used to find any sentence <br> about a marriage. Another example could be for a given list of movies, match all the sentences that have <br> movie names. Clearly, such heuristics result in noisy input sets.|\n",
    "|   Unsupervised         | Unlabeled features   | The system is expected to find some dependencies and patterns without clearly stating <br>which patterns we are looking for. E.g., clustering of documents, topics extraction <br>from the documents, etc. |\n",
    "|   Hybrid               | Varies for method <br>combinations   | Combines several approaches that are mentioned earlier for various purposes. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhDQe7Jv8YWu"
   },
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XgDa3Ya8YWu"
   },
   "source": [
    "Depending on the problem you solve or the  data (balanced number of classes or not) different metrics\n",
    "<br>might be more suitable.\n",
    "\n",
    "* **Accuracy**\n",
    "<br>\n",
    "$ Accuracy = \\frac{n_{correct}}{N}$, where $N$ is a total number of example that we were analysing,\n",
    "<br>$n_{corrent}$ is the number of examples that we have guessed the label.\n",
    "\n",
    "*a.k.a. Classification*\n",
    "\n",
    "* **Precision**\n",
    "<br>\n",
    "In case of classification, and in particular, if the classes are unbalanced, Precision should be a better measure to check.\n",
    "<br>\n",
    "$ Precision = \\frac{n_{correct\\ class\\ prediction}}{n_{class\\ predictions}} = \\frac{TP}{TP + FP}$,\n",
    "where $TP, FP, FN, TN$ are explained [here](https://en.wikipedia.org/wiki/False_positives_and_false_negatives).\n",
    "\n",
    "* **Precision@K**\n",
    "<br>\n",
    "Once your task is not simply to classify some examples, but e.g. rank them, a popular metric is P@K.\n",
    "<br>Here you use K (typically, 1,3,5, 10, etc.) and compute precision for those K results in your ranking.\n",
    "<br>For example, if you have a query for which you need to find similar documents,\n",
    "<br>P@K would be computed for the top K documents that are returned for a query as described above for those K elements.\n",
    "\n",
    "* **Recall**\n",
    "<br>\n",
    "Another very important concept in NLP, is Recall - which basically tell us how many of the class examples\n",
    "<br>or positive examples (maybe documents), our system had managed to extract.\n",
    "<br>\n",
    "$ Precision = \\frac{n_{correct\\ class\\ prediction}}{n_{positive\\ class\\ examples}} = \\frac{TP}{TP + FN}$ $\n",
    "\n",
    "* **F1**\n",
    "<br>\n",
    "[F1](https://en.wikipedia.org/wiki/F1_score) is the harmonic mean of the two above metrics.\n",
    "<br>Usually used to compare different approaches when you are not optimizing for P and R in particular\n",
    "<br>but rather overall performance.\n",
    "\n",
    "*a.k.a. Clustering* \n",
    "\n",
    "* [**Silhouette coefficient, Modularity, etc**](https://en.wikipedia.org/wiki/Cluster_analysis).\n",
    "<br>\n",
    "Once we move from the classification problems, and focus on clustering of the documents, many things can be measured depending\n",
    "<br>if you have labels or not.\n",
    "<br>\n",
    "The case where we do not have labels: \n",
    "  * For already proposed clustering of the input, *modularity* would measure how well nodes are assigned to the clusters.\n",
    "  <br> In particular, we would estimate how our current assignment is different from the assumed random graph.\n",
    "  * Silhouette coefficient estimates how average distance between objects in the same clusters differs\n",
    "  <br>from the average distance of those objects to the other clusters.\n",
    "  * [Davies–Bouldin index](https://en.wikipedia.org/wiki/Davies-Bouldin_index) measures the difference between inter- and intra-cluster similarity.\n",
    "\n",
    "*a.k.a. Language models*\n",
    "\n",
    "* **Perplexity**\n",
    "<br>\n",
    "Once we consider text generation tasks, where we typically do not have labelled examples, or multiple correct answers\n",
    "<br>are possible, *perplexity* can be used to evaluate your model. tl;dr - Perplexity estimates how surprised the model\n",
    "<br>is upon receiving an input, e.g., how the model is surprised that the next word after a current one\n",
    "<br>\"eat\" is \"me\", or \"meat\" or whatever. Typically, the lower the perplexity the more information about\n",
    "<br>the input the model has (no surprises). Another interpretation is that we compare our probability\n",
    "<br>distribution to the fair die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrdDFQVD8YWv"
   },
   "source": [
    "\n",
    "# Where text data comes from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOjbkYSR8YWv"
   },
   "source": [
    "Like anywhere in Data Science, it is important to first understand your data! Note: Of course, if you have it!\n",
    "\n",
    "In case you do not have the data:\n",
    "\n",
    "*Raw text data (Unlabeled/Non-annotated):*\n",
    "* Pay for it :)\n",
    "* Crawl it from the Web.\n",
    "<br>\n",
    "Examples: [Scrappy](https://scrapy.org/), [wiki/google crawler](http://www.netinstructions.com/how-to-make-a-web-crawler-in-under-50-lines-of-python-code/)\n",
    "<br>\n",
    "Of course, you might need to play with various IP addresses, throttling rates etc.\n",
    "* Twitter\n",
    "<br>\n",
    "You can get both historical or livestream data. For any of the two, you can get 1% of the stream for free.\n",
    "  * 1% historical tweets from [archive.org](https://archive.org/details/twitterstream?sort=-date).\n",
    "  * Similarly, you can get 1% of full twitter stream as tweets appear via Twitter API.\n",
    "  <br>You can also specify keywords (up to 2K) that would match tweets in real time - as a result you get up to\n",
    "  <br>1% total stream of messages. Note: if you have a not very popular query, you might get all the tweets about it,\n",
    "  <br>but of course, no guarantees.\n",
    "* News media\n",
    "<br>\n",
    "[News Archive](https://archive.org),\n",
    "[GDELT](https://www.gdeltproject.org)\n",
    "* Wikipedia\n",
    "<br>\n",
    "[Wikipedia dumps](https://dumps.wikimedia.org/)\n",
    "\n",
    "*Annotated data: *\n",
    "* Annotate and/or even generate your data using CrowdSourcing\n",
    "  <br>\n",
    "  [Crowdflower](https://www.figure-eight.com/), [MechanicalTurk](https://www.mturk.com/), etc.\n",
    "* Annotate using auxiliary lexical resources\n",
    "<br>\n",
    "[LIWC](http://liwc.wpengine.com/) a tool that analyses your textual input on the presence of various\n",
    "<br>\"shades\" - informal speech; syntactic structures; affect, social words; conginitive, perpetual,\n",
    "<br>biological processes; relativity; personal concerns, etc.\n",
    "\n",
    "*Finally, you have the data!\n",
    "... it is still not the final truth! *\n",
    "\n",
    "Work of [A. Olteanu](http://www.aolteanu.com/) well describes various [biases and pitfalls](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2886526) when it comes to the data analysis.\n",
    "* Biases\n",
    "* Not representative\n",
    "* Noisy data\n",
    "* Incomplete data\n",
    "* Incorrect data\n",
    "* Missing data, etc.\n",
    "\n",
    "So, first, get to know your data!\n",
    "\n",
    "Note: In this class, we will be working with the data that fits into the memory.\n",
    "<br>However, once it does not, you should adapt other methods to scale your pipelines - Flume, Spark, hdfs, etc. -\n",
    "<br>which are out of the scope of this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9t-jj_d8YWx"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shr220FzedMt"
   },
   "source": [
    "---\n",
    "\n",
    "# Dataset for the course: Tweets about natural disasters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5QCiVenFZBD"
   },
   "source": [
    "*Goal of our 1st application*: Analyze tweets about natural disasters. \n",
    "\n",
    "We will use publicly shared twitter data with provided human annotations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmm9hUSyh8oS"
   },
   "source": [
    "[Disasters on Social Media](https://data.world/crowdflower/disasters-on-social-media) from https://www.figure-eight.com/data-for-everyone/\n",
    "\n",
    "Description from the website: `Contributors looked at over 10,000 tweets culled with a variety of searches like \"ablaze\", \"quarantine\", and \"pandemonium\", then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JJ8tP4GvL3Y"
   },
   "outputs": [],
   "source": [
    "# You can download these data from the following link and store somewhere locally:\n",
    "\n",
    "csv_file = 'https://www.figure-eight.com/wp-content/uploads/2016/03/socialmedia-disaster-tweets-DFE.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31200,
     "status": "ok",
     "timestamp": 1563881186207,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "3LyC7367GWCi",
    "outputId": "09321dca-bdab-4a98-815d-058dc3a3cb0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-1c481529-cc4f-49be-b6ee-793c1183268d\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-1c481529-cc4f-49be-b6ee-793c1183268d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving socialmedia-disaster-tweets-DFE.csv to socialmedia-disaster-tweets-DFE.csv\n",
      "Features:\n",
      " Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
      "       '_last_judgment_at', 'choose_one', 'choose_one:confidence',\n",
      "       'choose_one_gold', 'keyword', 'location', 'text', 'tweetid', 'userid'],\n",
      "      dtype='object')\n",
      "Number of entries/tweets and number of the corresponding data columns. (10876, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>778252291</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 1:48</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obliteration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ashberxo @mind_mischief the removal of all tr...</td>\n",
       "      <td>6.276740e+17</td>\n",
       "      <td>321536657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8093</th>\n",
       "      <td>778252910</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 14:20</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rescued</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Val rescued the sister but alone died. In the ...</td>\n",
       "      <td>6.293600e+17</td>\n",
       "      <td>282079015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id  _golden  ...       tweetid       userid\n",
       "7474  778252291    False  ...  6.276740e+17  321536657.0\n",
       "8093  778252910    False  ...  6.293600e+17  282079015.0\n",
       "\n",
       "[2 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#    Example on how the data can be loaded here from the local file system.\n",
    "#    More options here: https://colab.research.google.com/notebook#fileId=/v2/external/notebooks/io.ipynb&scrollTo=vz-jH8T_Uk2c\n",
    "#    Downloaded the input csv.\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import pandas as pd\n",
    "with open('socialmedia-disaster-tweets-DFE.csv',\n",
    "          mode = 'r',\n",
    "          encoding = 'ascii',\n",
    "          errors = 'ignore'\n",
    "         ) as csvfile:\n",
    "  disasters_df = pd.read_csv(csvfile, header=0)\n",
    "print (\"Features:\\n\", disasters_df.keys())\n",
    "print (\"Number of entries/tweets and number of the corresponding data columns.\", disasters_df.shape)\n",
    "disasters_df.sample(2, random_state=432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIYbIuF70Tsf"
   },
   "source": [
    "Let's now check out what actually we have in the dataset. \n",
    "Typically if you create a dataset you know what is there, however, if you acquired it then we need to get familiar with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtQZOGMrD54F"
   },
   "source": [
    "Note: It is important to look into your data and particular examples to understand it.\n",
    "<br>However, normally you should sample a representative set of data to avoid non-intentional overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-brBM_o8Rj1"
   },
   "source": [
    "## Application-specific data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PDgpM9HzBqj"
   },
   "source": [
    "Considering the nature of the data set (crowd-annotated), let's create some filters to separate golden and annotated examples. In particular, let's ignore the examples where annotators did not have enough confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kAI9LQKjl65"
   },
   "source": [
    "#### Helper Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EHD-hjWgipE"
   },
   "outputs": [],
   "source": [
    "#    Golden examples\n",
    "golden = disasters_df['_unit_state'] == \"golden\"\n",
    "golden_positive = disasters_df['choose_one_gold'] == \"Relevant\"\n",
    "golden_negative = disasters_df['choose_one_gold'] == \"Not Relevant\"\n",
    "\n",
    "#    Annotated examples\n",
    "finalized = disasters_df['_unit_state'] == \"finalized\"\n",
    "confident = disasters_df['choose_one:confidence'] > 0.8\n",
    "finalized_positive = disasters_df['choose_one'] == \"Relevant\"\n",
    "finalized_negative = disasters_df['choose_one'] == \"Not Relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1563881215673,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "1akKmUf6gjHM",
    "outputId": "7ffdfd8a-14a5-4f1f-9833-c0c9c51aa05c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All golden examples: 87\n",
      "Positive golden examples: 57\n",
      "Annotated examples: 10789\n",
      "Annotated examples positive and confident: 2950\n",
      "Annotated examples negative and confident: 3445\n"
     ]
    }
   ],
   "source": [
    "print (\"All golden examples:\", disasters_df[golden].shape[0])\n",
    "print (\"Positive golden examples:\", disasters_df[golden & golden_positive].shape[0])\n",
    "print (\"Annotated examples:\", disasters_df[finalized].shape[0])\n",
    "print (\"Annotated examples positive and confident:\",\n",
    "       disasters_df[finalized & confident & finalized_positive].shape[0])\n",
    "print (\"Annotated examples negative and confident:\",\n",
    "       disasters_df[finalized & confident & finalized_negative].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1563881217528,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "Q0dtwCRByvUu",
    "outputId": "909d1577-ce96-4d11-a63c-6f090a8c7d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just happened a terrible car crash\n",
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Heard about #earthquake is different cities, stay safe everyone.\n",
      "there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\n",
      "Forest fire near La Ronge Sask. Canada\n",
      "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "13,000 people receive #wildfires evacuation orders in California \n",
      "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
      "#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\n"
     ]
    }
   ],
   "source": [
    "for t in disasters_df[golden & golden_positive].text[0:10]:\n",
    "  print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cIaB53GeRj3"
   },
   "outputs": [],
   "source": [
    "clean_confident_entries = disasters_df[\n",
    "    golden | (finalized & finalized_positive & confident) | \n",
    "    (finalized & finalized_negative & confident)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1563881220309,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "2bVWi33RAH0q",
    "outputId": "2798c6e5-69b6-47ff-a9d3-aa76180faed9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6482, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_confident_entries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgs6UJgYQ-UP"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLr3Xe1i6ehc"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EP-3TQrg6gsp"
   },
   "source": [
    "Usually when we deal with text, we deal with raw input represented as a string or lists of strings (sentences), etc.\n",
    "<br>\n",
    "First, those strings could be very long and quite unique to be of any use. \n",
    "<br>\n",
    "Second, your input could be rather noisy, contain outliers, have some mistakes or missing pieces.\n",
    "<br>\n",
    "All of this would be great to detect and possibly remove from your data.\n",
    "<br>\n",
    "Moreover, by preprocessing and cleaning the input, the size of the input could be significantly reduced.\n",
    "\n",
    "Some of the basic preprocessing steps we can perform on the text is sentence splitting and word tokenization.\n",
    "<br>Luckily for us, there are already some tools that work for most of the cases or already agreed in the teams.\n",
    "<br>Though you can implement and adjust the preprocessing manually, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAnOsG5Xy1wt"
   },
   "source": [
    "## Tokenization + Sentence Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rMxr1aU6iLT"
   },
   "source": [
    "Tokens are basically anything you encounter in the text: words (alpha), numbers (numerics), punctuation, various encodings, etc.\n",
    "<br>\n",
    "Token is anything without word boundaries - that could be different for different languages.\n",
    "<br>\n",
    "Typically, \" \" space and some punctuation is a good enough approximation of word boundaries, assuming latin-script input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36681,
     "status": "ok",
     "timestamp": 1563881116970,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "vZ9DTvwC4t2Z",
    "outputId": "155e644a-65dc-4881-b4a0-9d9078de15aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Collecting pyicu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/35/211ffb949c68e688ade7d40426de030a24eaec4b6c45330eeb9c0285f43a/PyICU-2.3.1.tar.gz (214kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 2.9MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyicu\n",
      "  Building wheel for pyicu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/45/7e/ccee9f1fe52787595e92641b5645cdf2cb40096749b39b4422\n",
      "Successfully built pyicu\n",
      "Installing collected packages: pyicu\n",
      "Successfully installed pyicu-2.3.1\n"
     ]
    }
   ],
   "source": [
    "#    Need to decide on tokenization - most of the time \" \" is a good guess.\n",
    "\n",
    "# Imports\n",
    "# Note: following nltk packages should be downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "!pip install pyicu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1563881224528,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "_H50IysDzANP",
    "outputId": "00144e50-5f54-4dd9-ef4d-333780c74945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'\", 's', 'clean', 'up', 'a', 'bit', 'the', 'dataset', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import (\n",
    "    sent_tokenize as splitter,\n",
    "    wordpunct_tokenize as tokenizer\n",
    ")\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import icu\n",
    "\n",
    "# Splits a string into sentences and words.\n",
    "def tokenize(text):\n",
    "  return [tokenizer(sentence) for sentence in splitter(text)]\n",
    "\n",
    "# In this exercise we do no!pip install pyicut care about the sentences (if any),\n",
    "# so let's flatten the list.\n",
    "def flatten(nested_list):\n",
    "  return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "# Let's see if it works ;)\n",
    "# Also would be great to write some TESTS!\n",
    "print (flatten(tokenize(\"Let's clean up a bit the dataset.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5z3WNEo_kDYO"
   },
   "outputs": [],
   "source": [
    "weird_text = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLphgENLij6P"
   },
   "outputs": [],
   "source": [
    "def iterate_breaks(text, break_iterator):\n",
    "    break_iterator.setText(text)\n",
    "    lastpos = 0\n",
    "    while True:\n",
    "        next_boundary = break_iterator.nextBoundary()\n",
    "        if next_boundary == -1: return\n",
    "        yield text[lastpos:next_boundary]\n",
    "        lastpos = next_boundary\n",
    "        \n",
    "icu_words = icu.BreakIterator.createWordInstance(icu.Locale('en_US'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajLr3fOX7uLa"
   },
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xfsie_9p94jw"
   },
   "source": [
    "**Note**: Look at the way this tokenization function handles words like \"Let's\".\n",
    "<br>Experiment to see what happens with some other punctuation symbols, and consider using another library\n",
    "<br>like ICU that provides both sentence and word break iterators.\n",
    "<br>\n",
    "To import `icu` use `!pip install pyicu,` and then you can create break iterators like `BreakIterator.createWordInstance`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34Tz05V2jjNI"
   },
   "source": [
    "#### *Exercise*: Compre and play with 2 different tokenizers\n",
    "\n",
    "NLTK VS ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1563881139124,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "MdjDeGlVkYGh",
    "outputId": "a3fb7b1d-5d2b-4a7d-dc3c-02d097148891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\n",
      "[This, is, a, cooool, #dummysmiley:, :-), :-P, <3, and, some, arrows, <, >, ->, <--]\n",
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n",
      "['This', ' ', 'is', ' ', 'a', ' ', 'cooool', ' ', '#', 'dummysmiley', ':', ' ', ':', '-', ')', ' ', ':', '-', 'P', ' ', '<', '3', ' ', 'and', ' ', 'some', ' ', 'arrows', ' ', '<', ' ', '>', ' ', '-', '>', ' ', '<', '-', '-']\n",
      "['This', 'is', 'a', 'cooool', '#dummysmiley:', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n",
      "['This', 'is', 'a', 'cooool', '#dummysmiley:', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "print(weird_text)\n",
    "print(flatten(tokenize(weird_text)))\n",
    "print(TweetTokenizer().tokenize(weird_text))\n",
    "print(list(iterate_breaks(weird_text, icu_words)))\n",
    "print(weird_text.split())\n",
    "print([word.text for word in tokenizer(weird_text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7fSuAhRkZX_"
   },
   "source": [
    "#### Clean up the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICUPzaL5d-5k"
   },
   "outputs": [],
   "source": [
    "#    Let's clean up the dataset a bit.\n",
    "#    Below are just some examples what can be done to clean up the data.\n",
    "#    Note: all this transformation should be done at your own risk ;), since\n",
    "#          they might introduce some bias in the data, and remove some important\n",
    "#          information.\n",
    "\n",
    "def tokenize_flatten_df(row, field):\n",
    "  return flatten(tokenize(row[field]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1588,
     "status": "ok",
     "timestamp": 1563881230573,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "56TxKQmY8I3F",
    "outputId": "4556c14e-a887-4395-9a4b-983ffced2bdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "clean_confident_entries['text_tokenized'] = clean_confident_entries.apply(lambda row: tokenize_flatten_df (row, 'text'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1563881233078,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "-vcMneiPGAlh",
    "outputId": "29f63d62-b15d-435b-9b00-c67d20f8ad33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Japan', 'Marks', '70th', 'Anniversary', 'of', 'Hiroshima', 'Atomic', 'Bombing', 'http', '://', 't', '.', 'co', '/', 'jzgxwRgFQg']\n",
      "['@', 'rinkydnk2', '@', 'ZaibatsuNews', '@', 'NeoProgressive1', 'When', 'push2Left', 'talk', \"='\", 'ecology', \"'&\", 'amp', \";'\", 'human', 'rts', \"'&\", 'amp', \";'\", 'democracy', \"'.\", 'War', 'Afghetc', \"='\", 'Left', \"'\", 'humanitarian', 'bombing']\n",
      "['Hiroshima', 'bombing', 'justified', ':', 'Majority', 'Americans', 'even', 'today', '-', 'Hindustan', 'Times', 'http', '://', 't', '.', 'co', '/', 'cC9z5asVZh']\n",
      "['Japan', 'marks', 'the', '70th', 'anniversary', 'of', 'the', 'atomic', 'bombing', 'of', 'Hiroshima', '.', 'http', '://', 't', '.', 'co', '/', 'YmKn1IwPvF', 'http', '://', 't', '.', 'co', '/', 'mMmJ8Bo9y3']\n",
      "[\"'\", 'Japan', 'Marks', '70th', 'Anniversary', 'of', 'Hiroshima', 'Atomic', 'Bombing', \"'\", 'by', 'THE', 'ASSOCIATED', 'PRESS', 'via', 'NYT', 'http', '://', 't', '.', 'co', '/', 'kKULqGB9e3']\n",
      "['Japan', 'marks', '70th', 'anniversary', 'of', 'Hiroshima', 'atomic', 'bombing', 'http', '://', 't', '.', 'co', '/', 'a2SS7pr4gW']\n",
      "['The', \"'\", 'sanitised', 'narrative', \"'\", 'of', 'Hiroshima', \"'\", 's', 'atomic', 'bombing', '.', '#', 'Hiroshima70', '#', 'japan', 'http', '://', 't', '.', 'co', '/', 'zsyj6sqYCn']\n",
      "['#', 'Japan', 'marks', '70th', 'anniversary', 'of', '#', 'Hiroshima', 'atomic', 'bombing', '(', 'from', '@', 'AP', ')', 'http', '://', 't', '.', 'co', '/', 'qREInWg0GS']\n",
      "['Today', 'is', 'the', 'day', 'Hiroshima', 'got', 'Atomic', 'bomb', '70', 'years', 'ago', '.', '-', 'The', \"'\", 'sanitised', 'narrative', \"'\", 'of', 'Hiroshima', \"'\", 's', 'atomic', 'bombing', 'http', '://', 't', '.', 'co', '/', 'GKpANz7vg0']\n",
      "['What', 'it', 'was', 'like', 'to', 'survive', 'the', 'atomic', 'bombing', 'of', 'Hiroshima', 'http', '://', 't', '.', 'co', '/', 'LGrOcbXPqo']\n"
     ]
    }
   ],
   "source": [
    "for line in clean_confident_entries['text_tokenized'][1000:1010]:\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH07XbXCF3lJ"
   },
   "source": [
    "Probably a bad idea. Need to remove urls first!\n",
    "\n",
    "Thus, below are the things we can do before tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHHabYi-61D6"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBz-0PIS8ElZ"
   },
   "source": [
    "Normalization usually refers to the unification of terms in the document.\n",
    "<br>\n",
    "\"Run\" and \"runs\" are probably referring to the same word and for particular applications it does not make sense to\n",
    "<br>treat them differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCDUh_i763Bs"
   },
   "source": [
    "### Text normalization or cleaning options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yc8M2N6P8JuB"
   },
   "source": [
    "Depending on the problem you are solving, your methods could be sensitive to the noise present in the data.\n",
    "<br>\n",
    "In order to reduce the input size and increase the importance of specific tokens in your documents,\n",
    "<br>\n",
    "some clean up could be useful. Cleaning the input usually includes the following:\n",
    "\n",
    "* Removal of the sensitive or ambiguous information (urls, names, numbers, hashtags, emojis, non-alpha-numeric, etc.)\n",
    "* Dealing with punctuation\n",
    "* Lowercasing and/or stemming/lemmatizing words\n",
    "* Removing tokens with low information gain (tokens with high document frequency or stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awUzoXouG2Zb"
   },
   "source": [
    "### Removing Twitter-specific characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZljmjtwFU_1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# remove urls\n",
    "def remove_urls(text):\n",
    "  return re.sub(r\"(https?\\://)\\S+\", \"\", text)\n",
    "\n",
    "# remove mentions (@name) completely\n",
    "def remove_mentions(text):\n",
    "  return re.sub(r\"@[^:| ]+:? ?\", \"\", text)\n",
    "\n",
    "# remove \"RT:\", if the tweet contains it.\n",
    "def remove_rt(text):\n",
    "  if text.lower().startswith(\"rt:\"):\n",
    "    return text[3:].strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1563881240317,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "HXFPNhEYQPU8",
    "outputId": "57f7105d-7657-4ffe-e1a0-4aa99caf7f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT: @julia: Calgary Police Flood Road Closures in Calgary. \n",
      "RT: Calgary Police Flood Road Closures in Calgary. http://t.co/RLN09WKe9g\n",
      "@julia Calgary Police Flood Road Closures in Calgary. http://t.co/RLN09WKe9g\n",
      "Calgary Police Flood Road Closures in Calgary.\n"
     ]
    }
   ],
   "source": [
    "print (remove_urls(\"RT: @julia: Calgary Police Flood Road Closures in Calgary. http://t.co/RLN09WKe9g\"))\n",
    "print (remove_mentions(\"RT: @julia Calgary Police Flood Road Closures in Calgary. http://t.co/RLN09WKe9g\"))\n",
    "print (remove_rt(\"RT: @julia Calgary Police Flood Road Closures in Calgary. http://t.co/RLN09WKe9g\"))\n",
    "print (remove_rt(\n",
    "          remove_mentions(\n",
    "              remove_urls(\n",
    "                  \"RT: @julia Calgary Police Flood Road Closures in Calgary. http://t.co/RLN09WKe9g\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1950,
     "status": "ok",
     "timestamp": 1563881243635,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "eERjnEw_UcoV",
    "outputId": "838926b0-9ea6-4e47-c889-b65effe34d52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def remove_urls_mentions_rt_df(row, field):\n",
    "  return remove_rt(remove_mentions(remove_urls(row[field])))\n",
    "\n",
    "clean_confident_entries['text_cleaned_from_url_mentions_rt'] = clean_confident_entries.apply(lambda row: remove_urls_mentions_rt_df (row, 'text'), axis=1)\n",
    "\n",
    "clean_confident_entries['text_tokenized'] = clean_confident_entries.apply(lambda row: tokenize_flatten_df (row, 'text_cleaned_from_url_mentions_rt'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xG42_YUnWaC"
   },
   "source": [
    "### *Exercise*: Hashtag removal\n",
    "\n",
    "Write function to replace hashtags '#word' with 'word' only in texts!\n",
    "\n",
    "Type your solution below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1563881244515,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "KhJA0iPWGj2a",
    "outputId": "fa3795c5-4101-449e-9e9d-0df9e9c51528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world how are you\n"
     ]
    }
   ],
   "source": [
    "def replace_hashtags_from_text(text):\n",
    "  return re.sub(r\"#+ ?\", '', text)\n",
    "\n",
    "print(replace_hashtags_from_text(text='Hello #world how are you'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkSA2q2TZr3F"
   },
   "source": [
    "### Removing punctuation, Numbers, and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zwTwQJHAZpih"
   },
   "source": [
    "Although some approaches could also handle and deal with punctuation, occasionally, it might be usefull to remove the punctuation.\n",
    "<br>\n",
    "Moreover, for the simple scenarious, punctuation might have very high document frequency and,\n",
    "<br> as a result would be removed during stopwords removal that we cover later.\n",
    "<br>\n",
    "Of course, if we are working with some complicated sequence models where punctuation is part of the result, it is crutial to leave it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DK_tUy9ORri"
   },
   "outputs": [],
   "source": [
    "# remove hashtags\n",
    "def replace_hashtags_from_list(tokens_list):\n",
    "  return [token for token in tokens_list if token != \"#\"]\n",
    "\n",
    "# remove digits\n",
    "def remove_digits(tokens_list):\n",
    "  return [token for token in tokens_list if not re.match(r\"[-+]?\\d+(\\.[0-9]*)?$\", token)]\n",
    "\n",
    "# remove all tokens that contains non alpha numeric, punctuation\n",
    "def remove_containing_non_alphanum(tokens_list):\n",
    "  return [token for token in tokens_list if token.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6v0N2C37QvW"
   },
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRd7pFQM7dj1"
   },
   "source": [
    "Important: Specific word cases could be important and have a separate meaning, e.g., \"the\" - article, \"THE\" - possible abbreviation.\n",
    "<br>\n",
    "In Twitter and other informal messages, capitalized words can also indicate emphasis ('It was THE DAY!').  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XM6wOL_77HxP"
   },
   "outputs": [],
   "source": [
    "# lowercase everything\n",
    "def lowercase_list(tokens_list):\n",
    "  return [token.lower() for token in tokens_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWy7Eq4a7SbJ"
   },
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxY7NR-N7mKd"
   },
   "source": [
    "Spot words are typically seens as high frequency (document-wise) noise, or function words\n",
    "<br>(words that does not convey a meaning but rather perform some function).\n",
    "* Created *stopwords lexicons* (nltk.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1563881263654,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "VgEbWe9z__1y",
    "outputId": "3c00034c-ba34-43dc-c5b3-e4889674a692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzVE6jKi7JYn"
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(tokens_list):\n",
    "  return [token for token in tokens_list if not token in stopwords.words(u'english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1563881266861,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "e9DZcK-xdlGA",
    "outputId": "cfe34771-66ca-4df4-95bd-aa5d03b396e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n"
     ]
    }
   ],
   "source": [
    "for token in stopwords.words(u'english')[0:20]:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1563881270544,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "4lU1vOZpWXhP",
    "outputId": "5f303a6e-2559-45fb-a89e-0589a5944ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calgary', 'police', 'flood', 'road', 'closures', 'calgary']\n"
     ]
    }
   ],
   "source": [
    "print (replace_hashtags_from_list(\n",
    "            remove_digits(\n",
    "                remove_containing_non_alphanum(\n",
    "                    lowercase_list(remove_stopwords(\n",
    "                        [\"Calgary\", \"#\", \"Police\", \",\", \"123\", \",\",\n",
    "                         \"?\", \"Flood\", \"#\", \"Road\", \"Closures\", \"in\",\n",
    "                         \"Calgary\", \".\", \"the\", \"13,000\"]))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mq7rJF41gLf"
   },
   "source": [
    "### Final full text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azBhIhmlBRQE"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15803,
     "status": "ok",
     "timestamp": 1563881289024,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "PWvgrWQTV2sL",
    "outputId": "1b46f037-1791-47dc-9be8-f07a2334ddf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6482/6482 [00:15<00:00, 427.64it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Iterates over the elements of the list with tokens and performs cleanup.\n",
    "def clean_tokens(row, field):\n",
    "  return replace_hashtags_from_list(\n",
    "            remove_digits(\n",
    "                remove_containing_non_alphanum(\n",
    "                    lowercase_list(remove_stopwords(row[field])))))\n",
    "\n",
    "clean_confident_entries['text_tokenized_cleaned'] = clean_confident_entries.progress_apply(lambda row: clean_tokens(row, 'text_tokenized'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvc5lROA72T3"
   },
   "source": [
    "### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4xuDf7wcFm2"
   },
   "source": [
    "In NLP, to deal with the variety of representation of the various words/tokens in language, **stemming** or **lemmatization** is used.\n",
    "<br>\n",
    "Stemming is typically faster, more naive word shortening. \n",
    "<br>\n",
    "Lemmatization take deeper approach to extracting lemmas of the words\n",
    "<br>(this includes identification of the part of speech, language specific dictionary with mapping between words and lemmas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqID4ugXcrCi"
   },
   "source": [
    "*Examples:* \n",
    "\n",
    "am, are, is $\\Rightarrow$ be\n",
    "\n",
    "car, cars, car's, cars' $\\Rightarrow$ car\n",
    "\n",
    "<img src=\"https://nlp.stanford.edu/IR-book/html/htmledition/img102.png\" width=50%>\n",
    "\n",
    "(c) [Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNyNkZ9GctKO"
   },
   "source": [
    "Let's now lemmatize/stem the words we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1563881297837,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "hjgGix00C0FQ",
    "outputId": "2ee328f3-2570-466c-8776-89abe1ca1066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3153,
     "status": "ok",
     "timestamp": 1563881301171,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "g__fqttwcEmC",
    "outputId": "6c434db2-85e8-4009-c863-37b03f19511c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_stemmer ['calgari', 'polic', 'polici', 'flood', 'road', 'closur', 'in', 'calgari']\n",
      "lancaster_stemmer ['calg', 'pol', 'policy', 'flood', 'road', 'clos', 'in', 'calg']\n",
      "snowball_stemmer ['calgari', 'polic', 'polici', 'flood', 'road', 'closur', 'in', 'calgari']\n",
      "wordnet_lemmatizer ['calgary', 'police', 'policy', 'flood', 'road', 'closure', 'in', 'calgary']\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = nltk.PorterStemmer()\n",
    "lancaster_stemmer = nltk.LancasterStemmer()\n",
    "snowball_stemmer = nltk.SnowballStemmer(u'english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatizer.stem = lemmatizer.lemmatize\n",
    "normalizers = [\n",
    "    ('porter_stemmer', porter_stemmer),\n",
    "    ('lancaster_stemmer', lancaster_stemmer),\n",
    "    ('snowball_stemmer', snowball_stemmer),\n",
    "    ('wordnet_lemmatizer', lemmatizer)\n",
    "]\n",
    "\n",
    "for stemmer_name, normalizer in normalizers:\n",
    "    print (stemmer_name, [normalizer.stem(token) for token in tokenizer(\"Calgary Police Policy Flood Road Closures in Calgary\".lower())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hL-NLeeQchMO"
   },
   "source": [
    "### *Exercise*: POS integration\n",
    "\n",
    "Try and do it similarly to the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1563881301786,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "irTvvBo3FmST",
    "outputId": "7b54c5c3-874a-4db0-dc5c-ad09f0e65e51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calgary police policy flood road closures in calgary'"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"Calgary Police Policy Flood Road Closures in Calgary\".lower(), 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAeg14TErLIr"
   },
   "source": [
    "## Text Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrvBAQkqajpO"
   },
   "source": [
    "### Part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqpENGS4asAU"
   },
   "source": [
    "In many languages, the word behaviour can be described by the word's part of speech (how the word would behave in a sequence of words).\n",
    "<br>\n",
    "POS - at least for english and very very simplified - nouns, verbs, adjectives, adverbs.\n",
    "<br>\n",
    "Check [nltk practical examples](https://www.nltk.org/book/ch05.html) on how to get the tags for your input.\n",
    "<br>\n",
    "Moreover, nltk has an interface to [Stanford POS tagger](https://nlp.stanford.edu/software/tagger.shtml).\n",
    "\n",
    "Actual tags go beyond just simplistic noun vs verb classification.\n",
    "<br>\n",
    "There are also proper nouns, count nouns, common nouns.\n",
    "<br>\n",
    "Moreover, the following tags are available - adjectives, adverbs, locative, degrees, prepositions, articles, pronouns,\n",
    "<br>\n",
    "phrasal vebs, auxilairies, modal verbs, etc. Also singular or plural tags are added.\n",
    "![List of POS](https://d2vlcm61l7u1fs.cloudfront.net/media%2Ffef%2Ffef719c5-d0fe-4b32-846b-66b7b540e268%2Fphptcdp2B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gtntxs4Ne0Mh"
   },
   "source": [
    "Let's try nltk POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1563881304897,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "roWr__NzdSI1",
    "outputId": "bc17b670-e429-4746-8648-2b0a4ac76a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('Time', 'NNP'), ('flies', 'NNS'), ('like', 'IN'), ('an', 'DT'), ('arrow', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(\"Time flies like an arrow\".split())\n",
    "print (tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzmYkd-IkYWB"
   },
   "source": [
    "### Parse trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsvChapLkgSM"
   },
   "source": [
    "* Constituency-based parse trees\n",
    "<br>\n",
    "Is a parse tree where the phrase structure grammars are preserved.\n",
    "<br>This tree is a binary tree that contains non-terminal nodes (phrases) and terminal nodes (actual words).\n",
    "<br>The standart structure of a sentence is $ S = NP\\ VP$, that is a sentence should contain a noun phrase and a verbal phrase.\n",
    "* Dependency-based parse trees\n",
    "<br>\n",
    "In these trees, all nodes are terminal where edges specify the actual dependencies between the words. \n",
    "<br>This means that there are less nodes in such trees (since we do not create non terminal nodes).\n",
    "<br>The constituency is also acknowledged in such graphs as any complete sub-tree.\n",
    "The tree could employ various dependencies: morphological, semantic, syntactic.\n",
    "<br>Syntactic functions of such tree could be useful if you are interested in extracting some particular phrases, or construction:\n",
    "<br>e.g., you can get information if something is an ATTRibute of an object, or if a noun is a COMPlement TO the object,\n",
    "<br> you can also discover what is a subject and what is an object in the sentence, etc.\n",
    "\n",
    "![Constituency and Dependency parse trees](https://upload.wikimedia.org/wikipedia/commons/0/0d/Wearetryingtounderstandthedifference_%282%29.jpg)\n",
    "\n",
    "(c) taken from [Wikipedia](https://en.wikipedia.org/wiki/Dependency_grammar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zvGjO8Ir2B3"
   },
   "source": [
    "### *Exercise*: Extract parse trees\n",
    "\n",
    "Try it out yourself during the break and let's discuss afterwards.\n",
    "\n",
    "In order to use Stanford parses, follow the instruction [here](https://stanfordnlp.github.io/CoreNLP/download.html).\n",
    "<br>Once, you have downloaded CoreNLP libs, unzipped it, specify the jar and models_jar paths in the code below and you should be good to go :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 689,
     "status": "error",
     "timestamp": 1563881307723,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "OtMkVNlKdTMP",
    "outputId": "3c80da4f-0ce1-462c-a982-95cae6329d9e"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-784712661f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m dependency_parser = StanfordDependencyParser(path_to_jar=\"\",\n\u001b[0;32m----> 3\u001b[0;31m                                              path_to_models_jar=\"\")\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time flies like an arrow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at "
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=\"\",\n",
    "                                             path_to_models_jar=\"\")\n",
    "\n",
    "result = dependency_parser.raw_parse('Time flies like an arrow')\n",
    "dependency_tree = result.next()\n",
    "list(dependency_tree.triples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bsf0Nijd34Nk"
   },
   "source": [
    "Or if we first run this code locally, you could run the StanfordNLP server and run the following commands: \n",
    "\n",
    "`java -mx1g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9010 -timeout 15000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 754,
     "status": "error",
     "timestamp": 1563881310503,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "cuX4dvoE4Eir",
    "outputId": "75024079-7340-4507-ec4f-23665d38bcf0"
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f3262bf4d68>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=9010): Max retries exceeded with url: /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%2C+%22ssplit.ssplit.eolonly%22%3A+%22true%22%2C+%22tokenize.whitespace%22%3A+%22false%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3262bf4d68>: Failed to establish a new connection: [Errno 111] Connection refused',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9b01d8046865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m next(\n\u001b[0;32m----> 5\u001b[0;31m      \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The quick brown fox sucks at jumping.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m      ).pretty_print()\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mraw_parse\u001b[0;34m(self, sentence, properties, *args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m             )\n\u001b[1;32m    230\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mraw_parse_sents\u001b[0;34m(self, sentences, verbose, properties, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mparsed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparsed_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mapi_call\u001b[0;34m(self, data, properties)\u001b[0m\n\u001b[1;32m    246\u001b[0m             },\n\u001b[1;32m    247\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         )\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \"\"\"\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=9010): Max retries exceeded with url: /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%2C+%22ssplit.ssplit.eolonly%22%3A+%22true%22%2C+%22tokenize.whitespace%22%3A+%22false%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3262bf4d68>: Failed to establish a new connection: [Errno 111] Connection refused',))"
     ]
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser(url='http://localhost:9010')\n",
    "\n",
    "next(\n",
    "     parser.raw_parse('The quick brown fox sucks at jumping.')\n",
    "     ).pretty_print()\n",
    "\n",
    "next(\n",
    "     parser.raw_parse('Time flies like an arrow.')\n",
    "     ).pretty_print()\n",
    "\n",
    "\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "dep_parser = CoreNLPDependencyParser(url='http://localhost:9010')\n",
    "\n",
    "parse, = dep_parser.raw_parse('Time flies like an arrow.')\n",
    "\n",
    "print(parse.to_conll(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXMVBOv7heyR"
   },
   "source": [
    "There is also a wrapper around the library. Check [here](https://github.com/Lynten/stanford-corenlp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56212,
     "status": "ok",
     "timestamp": 1563881369532,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "D5cM3dZ_hkIM",
    "outputId": "0ccc59aa-2e52-4fce-fdb7-566b2880146b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanfordnlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.16.4)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (41.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.6.16)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
      "Installing collected packages: stanfordnlp\n",
      "Successfully installed stanfordnlp-0.2.0\n",
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "y\n",
      "\n",
      "Default download directory: /root/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235M/235M [00:38<00:00, 4.99MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "!pip install stanfordnlp\n",
    "import stanfordnlp\n",
    "\n",
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1563881374601,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "J8RoUVXzrbIC",
    "outputId": "d969b8df-b590-4c69-b990-cdac82fff051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Token index=1;words=[<Word index=1;text=Barack;lemma=Barack;upos=PROPN;xpos=NNP;feats=Number=Sing;governor=4;dependency_relation=nsubj:pass>]>\n",
      "<Token index=2;words=[<Word index=2;text=Obama;lemma=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing;governor=1;dependency_relation=flat>]>\n",
      "<Token index=3;words=[<Word index=3;text=was;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin;governor=4;dependency_relation=aux:pass>]>\n",
      "<Token index=4;words=[<Word index=4;text=born;lemma=bear;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass;governor=0;dependency_relation=root>]>\n",
      "<Token index=5;words=[<Word index=5;text=in;lemma=in;upos=ADP;xpos=IN;feats=_;governor=6;dependency_relation=case>]>\n",
      "<Token index=6;words=[<Word index=6;text=Hawaii;lemma=Hawaii;upos=PROPN;xpos=NNP;feats=Number=Sing;governor=4;dependency_relation=obl>]>\n",
      "<Token index=7;words=[<Word index=7;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=4;dependency_relation=punct>]>\n",
      "None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "print (doc.sentences[0].print_tokens())\n",
    "print('-' * 100)\n",
    "print (doc.sentences[0].print_dependencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vmgdx2c46D14"
   },
   "source": [
    "# What to do with the clean data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gP7k67UJOujQ"
   },
   "source": [
    "# Documents representation\n",
    "\n",
    "Let's finally impose some vectorized representation on our documents so that the machine and math could easily operate over it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEVlAcrJf1a0"
   },
   "source": [
    "### Document representation as term counts (Bag-Of-Words, or BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpW-BkIdYGgh"
   },
   "outputs": [],
   "source": [
    "#    Now we need to convert our documents to the common representation.\n",
    "#    You can do it manually, just for fun, or we can already use some libs.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = []\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  corpus.append(\" \".join(row['text_tokenized_cleaned']))\n",
    "  \n",
    "document_term_matrix = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1563881380514,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "Wfmp1qlbPhe4",
    "outputId": "ace2a26c-b894-4f11-effd-45ad4b813862"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKUKV9lMYv8W"
   },
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E35O9XbkccTk"
   },
   "source": [
    "* We need to represent our data somehow\n",
    "  * Bag-of-words\n",
    "  <br>\n",
    "  The concept comes from the fact, that we could represent our input as a set of tokens or words without preserving any notion of order,\n",
    "  <br> just like items in the bag. In such cases, we assume that each document is simply a set of words and each\n",
    "  <br> word could appear several or zero times in a bag.\n",
    "  <br> In order to somehow presenve the order in the documents, n-grams (n consequitive words/tokens in the document) could be introduced.\n",
    "  <br> Though being rather powerful, number of features/dimentions grow exponentially.\n",
    "  * Document-term frequency\n",
    "  <br>\n",
    "  Similar to the BOW model though instead of 0s and 1s for each word that appear in the document,\n",
    "  <br> we would place TF or TFIDF score of the word in the document.\n",
    "  * Semantic Vector space representation\n",
    "  <br>\n",
    "  Previous representation is rather simple but very very high dimentional, as usually number of tokens in the \"bag\" could be \n",
    "  <br> around 1M (despite them being present or not in the document). \n",
    "  <br> As you could have seen in the ML course, we could reduce the dimentionality by somehow converting our \n",
    "  <br> 1M dimentional document representation to let's say 300. \n",
    "  <br> Such transformation can be both done for words (special case of the document with a single 1 on the word position and 0s elsewhere),\n",
    "  <br> combination of words, sentences, paragraphs, and whole documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S99D10LX324y"
   },
   "source": [
    "### Feature weighting and noise removal\n",
    "\n",
    "Apart from the general stopword removal that you might decide to do, several other techniques exist.\n",
    "<br>Those could be used for, first, cleaning the overall corpus from too frequent/too sparse tokens.\n",
    "<br>And, second, for potentially better representations of feature weights (rather thatn 0 and 1).\n",
    "\n",
    "Note: would work better for BOW representation, but might work worse for sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM3W3x7CWv8L"
   },
   "source": [
    "*Weighting tokens/features*:\n",
    "\n",
    "* *TF-IDF*\n",
    "<br>\n",
    "Term frequency * inverse document frequency. The intuition here is to preserve all words in the dataset in each document,\n",
    "<br>however, **correct** their frequencies with word specificity, i.e., the more specific word is (if the word appear in only several\n",
    "<br>documents it might be quite specific) the more we would want to promote word's score.\n",
    "<br>One way to compute this specificity is to inverse docuemtn frequency. This way, if the term is very frequent accross documents -\n",
    "<br>we would have to divide the term frequency with a high value, and vice versa.\n",
    "<br>Note: here still the more frequent the word is in a document, the more likely it is to be important, though it might be slightly degraded if it is not specific.\n",
    "\n",
    "* Filter words with the highest difference between *conditional probability* of a word within a document vs. a word within the whole corpus\n",
    "<br>\n",
    "For conditional probability, we do not care that much about the word frequency in a document, but rather how much its\n",
    "<br>probability changes in a given document sample with respect to the overall probability in the entire dataset.\n",
    "<br>\n",
    "As a result, conditional probability might score higher more specific (but less frequent) words.\n",
    "\n",
    "*Removing noise*:\n",
    "\n",
    "* Filter out ones with the *highest Document Frequency or part of the probability mass*\n",
    "<br>\n",
    "You might have heard, that natural language vocabularies exhibit frequency distribution similar to [Zipf law](https://en.wikipedia.org/wiki/Zipfs_law)\n",
    "<br>(subclass of the long-tail distribution where i_-frequent word would be twice as likely to appear in the corpus is _i+1_-frequent word).\n",
    "<br>Though zipf allow more relaxed behaviour, i.e., a lot of tokens have low frequency and few have high frequency.\n",
    "\n",
    "* *Background-foreground overlap idea*\n",
    "<br>\n",
    "The idea behind this methods is to compare frequency distributions of two independent datasets and diminish the intersection.\n",
    "<br>This allows to deemphasize the effect of the high frequency noise. In more details, we take frequency distribution of one dataset,\n",
    "<br>then same for some other totally independent (unrelated) dataset, extract top-n words in each distribution, and remove any items in the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "oLo9nIA1FRRR"
   },
   "outputs": [],
   "source": [
    "#@title Document Frequency Helper Functionality\n",
    "# Compute vector representation for each document in the collection.\n",
    "# Term frequency\n",
    "# TFIDF\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "class TermDocumentCounts:\n",
    "  def __init__(self):\n",
    "    # Counters of all the words in the corpus\n",
    "    self.total_word_counts = Counter()\n",
    "    self.total_number_of_words = 0\n",
    "    self.term_count_per_document = defaultdict(Counter)\n",
    "    self.number_of_words_per_document = defaultdict(int)\n",
    "    self.number_of_document = 0\n",
    "    self.df = defaultdict(int)\n",
    "    \n",
    "  def update(self, document_id, tokens):\n",
    "    self.number_of_document += 1\n",
    "    num_tokens = len(tokens)\n",
    "    self.total_word_counts.update(tokens)\n",
    "    self.total_number_of_words += num_tokens\n",
    "    self.term_count_per_document[document_id].update(tokens)\n",
    "    self.number_of_words_per_document[document_id] += num_tokens\n",
    "    for token in set(tokens):\n",
    "      self.df[token] += 1\n",
    "  \n",
    "  def most_common_word_in_document(self, document_id, top_n = None):\n",
    "    return self.term_count_per_document[document_id].most_common(top_n)\n",
    "  \n",
    "  def _compute_tfidf_for_word(self, document_id, word):\n",
    "        tf = self.term_count_per_document[document_id][word]\n",
    "        idf = math.log(self.number_of_document / self.df[word], 10)\n",
    "        return tf * idf\n",
    "  \n",
    "  # Returns the list of words ranked accoring to TFIDF score.\n",
    "  def ranked_document_words_tfidf(self, document_id, top_n=None):\n",
    "        tfidfs = [\n",
    "            (word, self._compute_tfidf_for_word(document_id, word))\n",
    "              for word in self.term_count_per_document[document_id].keys()\n",
    "        ]\n",
    "        tfidfs.sort(key=lambda x: x[1], reverse=True)\n",
    "        if not top_n:\n",
    "            top_n = len(tfidfs)\n",
    "        return tfidfs[:top_n]\n",
    "      \n",
    "  # Returns the list of words ranked accoring to max conditional probability\n",
    "  # change.\n",
    "  def ranked_document_words_conditional_probability(self, document_id,\n",
    "                                                    top_n = None):\n",
    "    word_posterior = [(word, self.compute_posterios(document_id, word)) \\\n",
    "              for word, count in \\\n",
    "                self.term_count_per_document[document_id].items()]\n",
    "    word_prior = {word: self.compute_priors(word) for word, _ in word_posterior}\n",
    "    conditional_probability = sorted(\n",
    "        [(word, (math.log((probability / word_prior[word]), 2))) \\\n",
    "            for word, probability in word_posterior],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    if not top_n:\n",
    "        top_n = len(conditional_probability)\n",
    "    return conditional_probability[:top_n]  \n",
    "      \n",
    "  # Computes posterior word distribution over given document.\n",
    "  def compute_posterios(self, document_id, word):\n",
    "    return self.term_count_per_document[document_id][word] \\\n",
    "              / self.number_of_words_per_document[document_id]\n",
    "  \n",
    "  # Computes prior word probability distribution\n",
    "  def compute_priors(self, word):\n",
    "    return self.total_word_counts[word] / self.total_number_of_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6o1DxxyTaGy"
   },
   "outputs": [],
   "source": [
    "corpus_counts = TermDocumentCounts()\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  corpus_counts.update(document_id, row['text_tokenized_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqmeuOhilqTg"
   },
   "source": [
    "What see what are words that are common in some of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1563732546036,
     "user": {
      "displayName": "Julia Proskurnia",
      "photoUrl": "https://lh4.googleusercontent.com/-Qke-0JLRMLo/AAAAAAAAAAI/AAAAAAAAIsM/X24zHBBNvWo/s64/photo.jpg",
      "userId": "11878887336903611948"
     },
     "user_tz": -120
    },
    "id": "8yBhuD44Wc8e",
    "outputId": "05351fd1-5e21-4975-a2b6-75d0b75703e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 with top 5 important words by tfidf or condiotional probability.\n",
      "['Just', 'happened', 'a', 'terrible', 'car', 'crash']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('terrible', 2.966610986681934) \t ('terrible', 10.79125593263882)\n",
      "('happened', 2.697765674389354) \t ('happened', 9.898171136555332)\n",
      "('just', 2.033557776312547) \t ('just', 7.691720259087907)\n",
      "('car', 1.8874297406343095) \t ('car', 7.122877423730027)\n",
      "('crash', 1.7905197276262528) \t ('crash', 6.778431892281237)\n",
      "\n",
      "Document 2 with top 5 important words by tfidf or condiotional probability.\n",
      "['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us', 'all']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('deeds', 3.811709026696191) \t ('deeds', 12.920538949583786)\n",
      "('forgive', 3.3345877719765284) \t ('forgive', 11.33557644886263)\n",
      "('allah', 3.1127390223601723) \t ('allah', 10.598610854696425)\n",
      "('our', 2.581260105317917) \t ('our', 8.833076108333447)\n",
      "('reason', 2.581260105317917) \t ('reason', 8.833076108333447)\n",
      "\n",
      "Document 1002 with top 5 important words.\n",
      "['Colorado', 'is', 'a', 'Spanish', 'word', '([', 'Latin', 'origin', ']', 'meaning', \"'\", 'reddish', \"'\", 'or', \"'\", 'colored', \"')\", 'all', 'you', 'dummies', 'are', 'pronouncing', 'it', 'wrong', '!!', '!']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('latin', 3.811709026696191) \t\t ('latin', 12.46110733094649)\n",
      "('reddish', 3.811709026696191) \t\t ('reddish', 12.46110733094649)\n",
      "('colored', 3.811709026696191) \t\t ('colored', 12.46110733094649)\n",
      "('dummies', 3.811709026696191) \t\t ('dummies', 12.46110733094649)\n",
      "('pronouncing', 3.811709026696191) \t\t ('pronouncing', 12.46110733094649)\n",
      "\n",
      "Document 1003 with top 5 important words.\n",
      "['Why', 'Some', 'Traffic', 'Is', 'Freezing', 'Cold', 'And', 'Some', 'Blazing', 'Hot', 'And', 'How', 'To', 'Heat', 'Up', 'Some', 'Of', 'Your', 'Traffic']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('some', 7.141035787611611) \t\t ('freezing', 11.672611436140203)\n",
      "('traffic', 4.793471357450746) \t\t ('some', 8.35068334125284)\n",
      "('and', 3.8967723331514703) \t\t ('cold', 8.213179817502905)\n",
      "('freezing', 3.811709026696191) \t\t ('traffic', 7.814630441012629)\n",
      "('cold', 2.770316341537966) \t\t ('your', 6.814630441012629)\n"
     ]
    }
   ],
   "source": [
    "for document_id in range(2):\n",
    "  print (\"\\nDocument\", document_id + 1,\n",
    "         \"with top 5 important words by tfidf or condiotional probability.\")\n",
    "  print (clean_confident_entries['text_tokenized'][document_id])\n",
    "  print (\"TFIDF\", \"\\t\\t\\t\\t\\t\", \"Condifional probability\")\n",
    "  for word_tfidf, word_conditional in zip(\n",
    "      corpus_counts.ranked_document_words_tfidf(document_id, 5),\n",
    "      corpus_counts.ranked_document_words_conditional_probability(document_id,\n",
    "                                                                  5)):\n",
    "    print (word_tfidf, \"\\t\", word_conditional)\n",
    "for document_id in range(1001,1003):\n",
    "  print (\"\\nDocument\", document_id + 1, \"with top 5 important words.\")\n",
    "  print (clean_confident_entries['text_tokenized'][document_id])\n",
    "  print (\"TFIDF\", \"\\t\\t\\t\\t\\t\", \"Condifional probability\")\n",
    "  for word_tfidf, word_conditional in zip(\n",
    "      corpus_counts.ranked_document_words_tfidf(document_id, 5),\n",
    "      corpus_counts.ranked_document_words_conditional_probability(document_id,\n",
    "                                                                  5)):\n",
    "    print (word_tfidf, \"\\t\\t\", word_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMi0VRsnJ3rm"
   },
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNVCn_sPcpdM"
   },
   "source": [
    "Before coding, check modules of [scikit-learn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction) and more examples [here](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). \n",
    "\n",
    "*Potential features*:\n",
    "\n",
    "* Words\n",
    "* N-grams\n",
    "* Character N-gram\n",
    "* Skip-gram\n",
    "* Part-of-Speech (POS)\n",
    "\n",
    "*Potential values*:\n",
    "\n",
    "* TF/Count Vectors ([CountVectorizer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html),  ([HashingVectorizer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)))\n",
    "* TF-IDF ([TFIDFVectorizer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) or [TFIDFTransformer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTI5AN2NJ0cl"
   },
   "source": [
    "## Preserving Word Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqoyNNLWco7P"
   },
   "source": [
    "Depending what is your particular problem, you might need to care about the order of words in your input data.\n",
    "\n",
    "* This could happen either on the data preprocessing step, where you would encode the order as part of the feature input,\\\n",
    "<br>e.g., n-grams, specific features that correspond to the positions of the words.\n",
    "<br>Further all those features are sent to the particular methods of your choice.\n",
    "* If you do not want and can't polute the input with those additional information, you might need to reply on the methods that\n",
    "<br>implicitly encode the order in the data as they read the input.\n",
    "  * Hiden Markov Models ([HMM on wiki](https://en.wikipedia.org/wiki/Hidden_Markov_model), [HMM Fundamentals](http://cs229.stanford.edu/section/cs229-hmm.pdf)), Conditional Random Fields ([CRF](https://en.wikipedia.org/wiki/Conditional_random_field)), Reccurrent neural networks, etc.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hut8vR9t8YWn",
    "AhDQe7Jv8YWu",
    "jrdDFQVD8YWv",
    "shr220FzedMt",
    "n-brBM_o8Rj1",
    "8kAI9LQKjl65",
    "34Tz05V2jjNI",
    "ZCDUh_i763Bs",
    "awUzoXouG2Zb"
   ],
   "name": "Copy of 2.Preprocessing.ipynb",
   "provenance": [
    {
     "file_id": "1ZHynBhz5sn-06T82c7K7C_-UyqLenP2m",
     "timestamp": 1563799786652
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
