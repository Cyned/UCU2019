{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.Introduction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["ouTN6I-5Z1S4","CLkSSuZYZucD","zB9kVYsbTigs","u4IKAenDXTMt","SsujCPuhXTKd","zswN0wSsZ7XM","w_0ZbR2ULt5a","9PMOSRMQaBAy","MDuZ1Bj95_Tg","mdNL0j7Ry7Ic"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AwOhmrKwEJZU","colab_type":"text"},"source":["#Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"utTKVK09Eg2g","colab_type":"text"},"source":["Natural Language Processing (NLP) can also be know as computational linguistics, text analysis, text mining, etc.\n","<br>\n","Based on its name it can be defined as follows (let's try it backwards):\n"]},{"cell_type":"markdown","metadata":{"id":"wGldRN05WwID","colab_type":"text"},"source":["## Natural Language\n"]},{"cell_type":"markdown","metadata":{"id":"Mw6U9illWy9x","colab_type":"text"},"source":["**There exist Formal and Natural languages.**\n","\n","*   Formal languages are by construction explicit and non-ambiguous.\n","*   Natural languages are in essence implicit and ambiguous\n","    *   \"Remove the stones from the cherries and put them in the pie.\"\n","    *   \"The hunter shot the tiger; his wife too.\"\n","    *   \"Time flies like an arrow.\"\n","    *   \"She was eating a fish with (bones, anger, some friends, a fork)\""]},{"cell_type":"markdown","metadata":{"id":"ctxxkhe5W_lQ","colab_type":"text"},"source":["Natural language is a method of human communication, either spoken or written, which typically\n","<br>\n","contains words that are structured in some conventional way."]},{"cell_type":"markdown","metadata":{"id":"6LNSReifW_va","colab_type":"text"},"source":["### <font color=\"grey\">Natural Language Functions</font>"]},{"cell_type":"markdown","metadata":{"id":"b9c7WAiGn0sd","colab_type":"text"},"source":["* Conciseness\n","  * The student gave *his* homework to the professor *who* told *him* that *it* could have been better.\n","  * The student gave the homework of the student to the professor. <br>The professor told the student that the homework of the student could have been better.\n","* Unlimited expressive power (Representation)\n","  * Logical expressions of any order.\n","    * Earth is curved  -  \"curved_earth = TRUE\"\n","    * All politicians lie.  -  For any x, politician(x) -> lier(x), etc\n","* Shared knowledge\n","  * I gave him a nice pen.\n","  * Scenario 1: - A \"Mont Blanc\"? - Yes, like that brand.\n","  * Scenario 2: - How large is it?  - To the moon and back.\n"]},{"cell_type":"markdown","metadata":{"id":"wEbnAcxnXDak","colab_type":"text"},"source":["### <font color=\"grey\">Why Is Natural language IMPLICIT and AMBIGUOUS?</font>"]},{"cell_type":"markdown","metadata":{"id":"JNGI3H6SXEo9","colab_type":"text"},"source":["Implicit -> allows conciseness (but potentially ambiguous)\n","<br>\n","Unlimited expressive power -> flexible interpretation rules (so meanings != word as it is written)\n","<br>\n","Why do we still understand each other?  -> A lot of shared knowledge!\n","\n","Moreover...\n","\n","*  Existing words (or linguistic units) change over time.\n","*  New words are added.\n","*  Language contains errors.\n","*  Linguistic units denote an idea or even several.\n","*  We have lots of common sense knowledge and understand hierarchical structures in language."]},{"cell_type":"markdown","metadata":{"id":"M00vdTBQW8DF","colab_type":"text"},"source":["## Natural Language Processing OR \"Machine! Do something with those texts!\"\""]},{"cell_type":"markdown","metadata":{"id":"fUIksym9n28h","colab_type":"text"},"source":["When a computer has to deal with the textual data, it actually faces the following:\n","\n","1.   Human language is very high dimensional and sparse.\n","<br>\n","Depending on the representation and the language, for a machine, natural language consists of a\n","<br>*huge number of words*, and even greater number of possible word-like features\n","<br>(parts of speech, grammatical variations of the language, various languages, etc.)\n","<br>\n","Moreover, for most of those, we simply *do not have enough data* to give to the machine or even worse,\n","<br>the same *words mean multiple things*.\n","2.   Human language is redundant and ambiguous.\n","<br>\n","As we have discussed earlier, human language is unnecessarily redundant.\n","<br>There are an enormous number of ways to describe the same thing."]},{"cell_type":"markdown","metadata":{"id":"MBIuy8m8XQyp","colab_type":"text"},"source":["---\n","# Applications and Examples\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"lOAOlao3ZiGF","colab_type":"text"},"source":["## Text normalization"]},{"cell_type":"markdown","metadata":{"id":"X48Tq2hCSqiN","colab_type":"text"},"source":["* Spell checking\n","* Grammar assistants (Grammarly)\n","* Formatting (more or less formal, etc.)\n","* Recapitalization\n","* Styling\n","* Autocompletion (SmartCompose in Gmail)"]},{"cell_type":"markdown","metadata":{"id":"Y5Wb7wUfXS4N","colab_type":"text"},"source":["## Document classification"]},{"cell_type":"markdown","metadata":{"id":"j4aX6q-PS4Yx","colab_type":"text"},"source":["Some examples of the document classification that you probably use every day.\n","\n","* Spam filtering\n","  * Fake review detection\n","  * Plagiarism detection\n","* Authorship attribution\n","* Sentiment analysis\n","  * Opinion mining (Stance detection)\n","  * Early warning/indicator systems<br>(e.g., \"Nestle processed food should advertise the products differently\")\n","  * Potential simple decision-making variations a.k.a. - \"shall I buy this product?\"\n","\n","Some **interesting examples** of classification systems:\n","* Gmail Spam detection\n","* Priority Inbox [Gmail link]() [Yahoo link]()"]},{"cell_type":"markdown","metadata":{"id":"k0fRZz-vXS6l","colab_type":"text"},"source":["## Automatic text summarization"]},{"cell_type":"markdown","metadata":{"id":"1DpU0EsEZe0G","colab_type":"text"},"source":["[Summarization](http://amzn.to/2giFqN1) is the process of distilling the most important information from a source.\n","[Why](http://amzn.to/2fhUPNt)?\n","* Reduce reading time\n","* Selection of the core ideas of the documents get faster\n","* More effective indexing\n","* Potentially less biased than human summaries\n","* Useful for QA systems\n","* Reduces the time to automatically process input data as well as reduces the storage needed to keep the data.\n","\n","Methods:\n","* *Extractive [Methods](https://arxiv.org/abs/1707.02268)* - methods that involve selection of phrases and <br> sentences from the input document\n","* *Abstractive Methods* - methods that generate entirely new phrases and sentences that capture the meaning <br> of the original source.\n","\n","Papers to check out:\n","<br>\n","[Get to the point: Summarization with pointer-generator networks](https://arxiv.org/abs/1704.04368) [Code](https://github.com/abisee/pointer-generator)\n","<br>\n","[A neural attention model for abstractive sentence summarization](https://arxiv.org/abs/1509.00685)\n","<br>\n","[Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond](https://arxiv.org/abs/1602.06023)"]},{"cell_type":"markdown","metadata":{"id":"xrsiG0MBXS9O","colab_type":"text"},"source":["### Topic modeling and Clustering"]},{"cell_type":"markdown","metadata":{"id":"8cMdttdlZcyy","colab_type":"text"},"source":["Another way to think about summarization is document representation through topics (significantly reduces the size and still conveys the general direction of the document) or through document agglomerations that represent some topic (by means of clustering or topic modelling in each document)."]},{"cell_type":"markdown","metadata":{"id":"hSvOWplXZiD0","colab_type":"text"},"source":["## Natural language generation"]},{"cell_type":"markdown","metadata":{"id":"gBXPPHaZZno3","colab_type":"text"},"source":["NLG is a process of automatically transforming data into written text that is preferable grammatically, syntactically and semantically correct.\n","\n","NLG can be used both for helping people to write what they might want to write as well as summarize and shorten given documents.\n","\n","One of the google example of generation is Google's SmartCompose and SmartReply efforts in Gmail."]},{"cell_type":"markdown","metadata":{"id":"1M4ANcYQbQAb","colab_type":"text"},"source":["### Caption generation\n","\n","Image caption generation with visual attention: [paper](https://arxiv.org/abs/1502.03044)."]},{"cell_type":"markdown","metadata":{"id":"zeSdlU4rajRY","colab_type":"text"},"source":["### Handwriting generation\n","\n","[Alex Grave demo](http://www.cs.toronto.edu/~graves/handwriting.html) and the [paper](https://arxiv.org/abs/1308.0850) describing the idea behind it."]},{"cell_type":"markdown","metadata":{"id":"2vICPxzfZp4A","colab_type":"text"},"source":["## Natural Language Understanding (NLU)"]},{"cell_type":"markdown","metadata":{"id":"ej0VEJkZZ4pe","colab_type":"text"},"source":["One of the main motivations behind better NLU is the creation of chat and speech enabled bots that can\n","<br>\n","interact with and react to human-generated language. Currently many big companies have efforts in\n","<br>\n","those directions: Alexa, Siri, Google Assistant, Cortana, etc."]},{"cell_type":"markdown","metadata":{"id":"kRrYaEN5ZuVH","colab_type":"text"},"source":["### Similar keyword search and Query expansion"]},{"cell_type":"markdown","metadata":{"id":"74NqAIH9Z5LO","colab_type":"text"},"source":["A common problem of information retrieval in the context of query understanding. Typically, this involves:\n","* finding synonyms of words,\n","* identifying and using part-of, is-a relationshipds between keywords\n","* performing morphological transformations,\n","* correcting spelling mistakes,\n","* reweighting the terms in the query."]},{"cell_type":"markdown","metadata":{"id":"R7wi_fH0hmZo","colab_type":"text"},"source":["### Entities and Knowledge bases"]},{"cell_type":"markdown","metadata":{"id":"uqdEimmkhj7a","colab_type":"text"},"source":["Identifying named entities (and just entities) is a common problem for text <br> understanding. It allows to link the text back to its semantics, <br> by disambiguating what exactly is mentioned.\n","\n","Common subtasks:\n","* [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition).\n","* Entity linking / disambiguation\n","* Coreference resolution\n","\n","Information about entities and their relations (e.g. isA / hierarchical relation) is stored in some sort of databases. \n","\n","Examples: \n","* [Google Knowledge Graph](http://go/kg) is an internal database of entities and facts.\n","* [WordNet](https://en.wikipedia.org/wiki/WordNet) is a lexical database for <br>\n","English, merging words into synsets with particular meaning and providing <br> semantic relations between them.\n","\n","\n","[More information on entities and knowledge bases is below](#scrollTo=WrN-j9kLdo0j).\n"]},{"cell_type":"markdown","metadata":{"id":"NKkvtTsGZuXh","colab_type":"text"},"source":["### Entity extraction"]},{"cell_type":"markdown","metadata":{"id":"CiX-kR2GZ5pz","colab_type":"text"},"source":["**[Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)** - is typically a problem of\n","<br>\n","detection entities of type LOCation, ORGanization, or PERson. Note: each of those entities might span several\n","<br>\n","words/tokens in a given text. Typically, NER is solved as follows: linguistic grammar-based, statistical models or\n","<br>\n","machine learning. Typically Conditional Random Fields are used for the tasks.\n","<br>\n","Examples:\n","  * Kaggle competition [dataset](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus).\n","  * [DataTurk](https://dataturks.com/projects/Mohan/Best%20Buy%20E-commerce%20NER%20dataset) - <br> search queries with entities.\n","  * [Stanford NER](https://nlp.stanford.edu/software/CRF-NER.shtml)\n","  * [Spacy](https://en.wikipedia.org/wiki/SpaCy)\n","  * Apache [OpenNLP](http://opennlp.apache.org/index.html)"]},{"cell_type":"markdown","metadata":{"id":"sNYDBwtNZuZx","colab_type":"text"},"source":["### Entity disambiguation"]},{"cell_type":"markdown","metadata":{"id":"zsxQ_3CQZ6Mn","colab_type":"text"},"source":["Entity disambiguation is a problem of matching the identity of the entity mentioned in the text to actual entity.\n","<br>\n","Typically, supervised learning is used to solve this problem where anchor texts are leveraged in the training data.\n","<br>\n","Further, several explorations were made with [clean unambiguous training data](http://www.aclweb.org/anthology/C10-1145),\n","<br>or with [topically related texts that potentially have similar types](https://www.cc.gatech.edu/~zha/CSE8801/query-annotation/p457-kulkarni.pdf) of entities in them, etc.\n","<br>\n","For example:\n","<br>\n","> Donald **Trump** and **Trump** card.\n","<br>\n",">**Paris** Hilton and **Paris** city."]},{"cell_type":"markdown","metadata":{"id":"ouTN6I-5Z1S4","colab_type":"text"},"source":["### Entity Linking/Matching"]},{"cell_type":"markdown","metadata":{"id":"3sRCjK9bZ7Ny","colab_type":"text"},"source":["Another important problem with regards to the entities is coreference resolution or entity linking or entity matching.\n","<br>\n","Coreference denotes a presence of several expressions in a text that refer to the same person/thing.\n","<br>\n","For example:\n","<br>\n","> **The food** was salty so guests did not enjoy **it**.\n","<br>\n","> **The student** was absent for 3 months. **Such a person** won't pass any exam."]},{"cell_type":"markdown","metadata":{"id":"CLkSSuZYZucD","colab_type":"text"},"source":["### Relation extraction"]},{"cell_type":"markdown","metadata":{"id":"0cjuzC7hZ6eW","colab_type":"text"},"source":["Entities could have several various relationships between each other that are important to extract. For example:\n","\n","* Types of relationships:\n","\n","  * *is-a* - the relationship between two entities in which one entity inherits from the other\n","\n","  * *Hypernyms* - a word with a broad meaning constituting a category into which words with more specific meanings fall;\n","  \n","  * *Hyponyms* - is a word or phrase whose semantic field[1] is included within that of another word. <br>Hyponyms denotes a subset of the hypernym.\n","  \n","  * *Meronymy* denotes a constituent part of, or a member of something. Meronym is a part of a whole!\n","\n","  * *Synsets* - A set of one or more synonyms that are interchangeable in some context without changing the <br> truth value of the proposition in which they are embedded;\n","\n","  * *Metonymy* - the substitution of the name of an attribute or adjunct for that of the thing meant, <br> for example suit for business executive.\n","  \n","  * *Anything*: who is married to who, what causes what, etc.\n","  \n","* Relation extraction\n","\n","  * Methodologies: [Regex](http://www.aclweb.org/anthology/D08-1003), [Rule based](http://iswc2012.semanticweb.org/sites/default/files/76490257.pdf), [Wikipedia categories](http://pages.cs.wisc.edu/~anhai/papers/kcs-sigmod13.pdf), [Distant supervision](https://web.stanford.edu/~jurafsky/mintz.pdf), [Bayesian networks](http://aclweb.org/anthology/D17-1192), [Factor graphs](https://cs.stanford.edu/people/czhang/zhang.thesis.pdf)."]},{"cell_type":"markdown","metadata":{"id":"zB9kVYsbTigs","colab_type":"text"},"source":["### Knowledge Bases/Knowledge Graphs"]},{"cell_type":"markdown","metadata":{"id":"vvXusNGhTnl8","colab_type":"text"},"source":["* [WordNet](https://en.wikipedia.org/wiki/WordNet)\n","<br>\n","WordNet is a lexical database for English. It groups English words into synonyms (synsets), provides their short\n","<br>\n","descriptions and usages. Moreover, it contains several relations between the enties of synsets.\n","\n","* [OmegaWiki](http://www.omegawiki.org/Meta:Main_Page), [BabelNet](https://babelnet.org/)\n","<br>\n","OmegaWiki aims at creating dictionaries of all words of all languages. BabelNet is a multilingual encyclopedic dictionary.\n","\n","* <a href=\"https://en.wikipedia.org/wiki/Taxonomy_(general)\">**Taxonomy**</a>\n","<br>\n","Taxonomy refers to the hierarchical categorization where relatively well-defined classes are nested under broader categories.\n","\n","* [Folksonomy](https://en.wikipedia.org/wiki/Folksonomy)\n","<br>\n","Folksonomy is a relatively new system where users apply tags to online items.\n","<br>\n","As opposed to Taxonomy, Folksonomy does not derive a hierarchical structure betwen the tags but rather only assigns them.\n","\n","* <a href=\"https://en.wikipedia.org/wiki/Ontology_(information_science)\">**Ontology**</a>\n","<br>\n","Ontology is a representation, naming, definitions, categories, properties, relations of the concepts/entities\n","<br>\n","for several or all domains.\n","\n","* [DBPedia](https://en.wikipedia.org/wiki/DBpedia)\n","<br>\n","DBPedia aims at extracting structured content from the Wikipedia. It describes about 4.5M entiuties,\n","<br>\n","with about 1.5M persons, 700K places, 240K organizations, etc.\n","\n","* <a href=\"https://en.wikipedia.org/wiki/YAGO_(database)\">YAGO</a>\n","<br>\n","YAGO is an open sourced knowlege base that was developed in Max Planck Institute.\n","<br>\n","This knowledge base contains over 10M entities and about 120M facts about those entities.\n","<br>\n","YAGO extracts information from Wikipedia boxes, WordNet and [GeoNames](https://en.wikipedia.org/wiki/GeoNames).\n","\n","* [**Knowledge Bases**](https://en.wikipedia.org/wiki/Knowledge_base) and [Knowledge Graphs](https://en.wikipedia.org/wiki/Knowledge_Graph)\n","<br>\n","KB or KG are technology to store complex structured and unstructured information.\n","<br>\n","One of the main example of the Knowledge Graph is [Google Knowledge Graph](https://en.wikipedia.org/wiki/Knowledge_Graph) that was in part\n","<br>\n","powered by [Freebase](https://en.wikipedia.org/wiki/Freebase) (Freebase is a large collaborative knowledge base that contain the\n","<br>\n","data composed mainly by its community members.).\n","<br>\n","Another example: Knowledge Graphs with [DeepDive](https://meta.wikimedia.org/wiki/Research:Wikipedia_Knowledge_Graph_with_DeepDive).\n","\n","* <a href=\"https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)\">Commonsense knowledge</a>\n","<br>\n","Common sense knowledge consists of facts about everyday life, e.g., The sky is blue, a lemon is yellow and sour.\n","<br>\n","A large corpus of this data called [Open Mind Common Sense](https://en.wikipedia.org/wiki/Open_Mind_Common_Sense) (OMCS) was created by MIT."]},{"cell_type":"markdown","metadata":{"id":"u4IKAenDXTMt","colab_type":"text"},"source":["## Question and Answer (Q&A) systems"]},{"cell_type":"markdown","metadata":{"id":"CRUCOVtWXTO2","colab_type":"text"},"source":["Q&A systems are supposed to return the answer to a query expressed in natural language or admit that the answer is not known.\n","Q&A systems usually benefits from a lot of NLP research areas: names entity recognition, taxonomies, ontologies, some relevant data for a domain, query rewriting, query similarity or matching to the existing material.\n","\n","Some prominent examples:\n","* Siri\n","* Google Now/ Google Assistant\n","* Cortana\n","* Amazon Alexa\n","* etc."]},{"cell_type":"markdown","metadata":{"id":"SsujCPuhXTKd","colab_type":"text"},"source":["## Decision making systems"]},{"cell_type":"markdown","metadata":{"id":"CdizEkefZiBj","colab_type":"text"},"source":["Decision making systems help aggregate unstructured text data to make decisions.\n","<br>For example, one can measure the strength of positive reaction towards a local place (restaurant or hotel)\n","<br>in order to decide whether to visit it, or one may want to answer questions like\n","<br>\"How many people support vaccination?\" and why.  Natural Language Processing tools help in such\n","<br>aggregation of lexical corpora of raw texts.\n","\n","Some examples:\n","* Detecting and analyzing relevant news for trading on financial stock market ([paper](http://www.aaai.org/ocs/index.php/ICWSM/ICWSM10/paper/download/1529/1904)).\n","* Finding patterns in user's survey free-format answers, e.g. discovering topics of responses ([paper](https://scholar.harvard.edu/files/dtingley/files/topicmodelsopenendedexperiments.pdf)).\n","* Extraction of relevant snippets from user reviews, e.g. 'long battery'.\n"]},{"cell_type":"markdown","metadata":{"id":"zswN0wSsZ7XM","colab_type":"text"},"source":["## Machine translation i18n"]},{"cell_type":"markdown","metadata":{"id":"fse-GmYxZ7aV","colab_type":"text"},"source":["Machine translation refers to the class of algorithms translating input text in a source language into a target language.\n","\n","Historically, statistical translation of n-grams/phrases was used. Now deep learning-based translation achieves\n","<br>better results and allows better capturing of the meaning in the long texts\n","<br>([paper](https://arxiv.org/pdf/1409.3215.pdf): Sequence to Sequence Learning with Neural Networks).\n","<br>Such methods require a large collection of parallel (aligned) text corpora between languages.\n","<br>The novel advances include using universal embedding representations, requiring parallel corpora\n","<br>only for some pairs ([paper](http\n","s://www.aclweb.org/anthology/Q/Q17/Q17-1024.pdf): Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation),\n","<br>and even training of translation system without any parallel corpora ([paper](https://arxiv.org/abs/1710.11041): Unsupervised Neural Machine Translation).\n","\n","Demo: [Google Translate](https://translate.google.com/), Available as a Cloud API at https://cloud.google.com/translate/.\n","\n","Open-source APIs: \n","* Classic: [Moses](http://www.statmt.org/moses/)\n","* Newer Neural MT: [Open NMT](http://opennmt.net/)\n"]},{"cell_type":"markdown","metadata":{"id":"fG9_yBXuaDOQ","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"w_0ZbR2ULt5a","colab_type":"text"},"source":["## Speech recognition"]},{"cell_type":"markdown","metadata":{"id":"GDKN_ZM1aFNy","colab_type":"text"},"source":["Speech recognition (or Speech-to-Text) is an essential step for enabling Natural Language Interfaces.\n","\n","Nice already existing applications:\n","* Voice control of devices: Google Assistant, Apple's Siri, Amazon's Alexa\n","* Automatic transcription of speech for dictation: hands-free input ([search by voice](http://www.google.com/mobile/voice-search/))\n","* Automatic transcription of videos (generating subtitles): [Auto-captions on youtube](https://www.youtube.com/watch?v=kTvHIDKLFqc)\n","\n","Available tools:\n","* Google Speech-to-Text API is open for developers: https://cloud.google.com/speech-to-text/ (public link).\n","* List of open-source toolkits: https://www.kdnuggets.com/2017/03/open-source-toolkits-speech-recognition.html \n","\n","Deep neural networks enabled significant advances in this domain.\n","\n","Some papers about recent models:\n","* End-to-end speech recognition with RNN: [DeepMind paper](http://proceedings.mlr.press/v32/graves14.pdf), 2014.\n","* Attention-based models for speech recognition: [paper](https://dl.acm.org/citation.cfm?id=2969304), 2015\n","* Tracking state-of-the-art in Speech Recognition: https://github.com/syhw/wer_are_we"]},{"cell_type":"markdown","metadata":{"id":"9PMOSRMQaBAy","colab_type":"text"},"source":["## Speech synthesis"]},{"cell_type":"markdown","metadata":{"id":"XbWHe81NaEse","colab_type":"text"},"source":["[WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/): speech synthesis"]},{"cell_type":"markdown","metadata":{"id":"MDuZ1Bj95_Tg","colab_type":"text"},"source":["# Typical NLP-heavy problems"]},{"cell_type":"markdown","metadata":{"id":"WwfFp05VZRiK","colab_type":"text"},"source":["* *Classification*: Classifying documents into particular categories (giving labels).\n","* *Regression*: Predict numerical values\n","* *Clustering*: Separating documents into non- overlapping subsets.\n","* *Ranking*: For a given input, rank documents according to some metric. \n","* *Association rule mining*: Infer likely associations patterns in data.\n","* *Structured output*: Bounding boxes, parse trees, etc.\n","* *Sequence-to-sequence*: For a given input/source text, generate output annotations or another text."]},{"cell_type":"markdown","metadata":{"id":"E6LV3tKgEi-g","colab_type":"text"},"source":["To address many of the challenges above, several important things has to be clarified beforehand:\n","\n","1. Set the research/exploration goal (e.g., how heavy will be the earthquake on a given day).\n","2. Make a hypothesis ( e.g., strength of the earthquake is an informative signal).\n","3. Collect the data (e.g., collect historical strength of the quakes on eachday).\n","4. Test the hypothesis (e.g., train a model using the data)\n","5. Analyze the results (e.g., are results better than existing systems).\n","6. Reach a conclusion (e.g., the model should be used or not because of A, B, C).\n","7. Refine hypothesis and repeat. (e.g., time of the year could be a useful signal)."]},{"cell_type":"markdown","metadata":{"id":"Kau8uIhCLB19","colab_type":"text"},"source":["Below is the table of the approaches that can be applied to the problems/applications above.\n","\n","| Type                   | Input   | Clarifications|\n","|------------------------|---------|---------------|\n","|   Rule-based           |Explicit linguistic patterns, <br>lexicons, etc.| Such an approach always have predefined behaviour and usually can't generalize usually. |\n","|   Supervised           | Training examples: typically <br>tuples of the form <br>(features, label) | Here input features are usually some vectorized, transformed, normalized input representation.|\n","|   Semi-supervised or <br> Pseudo-relevance <br>feedback   | Same as in supervised base, <br>but also results of the prediction <br>(e.g. with greatest confidence) <br>are used as input pairs.   | Once we trained the system on the input (feature, label), we label unknown examples <br>with the model. If the confidence of the label is high (on that unseen example), <br>we add those examples to the input set and retrain the model. |\n","|   Distant supervision  | Same as for supervised learning, <br>but (feature, label) pairs does not <br> come from the annotation, but from <br>some heuristic-based annotation.   | Rather than annotating thousands of examples (documents, sentences, words, etc.), <br>we take a few examples of the class and try to generalize and match those <br>in the domain-specific corpus. Such systems first generate or extract examples that are <br> very similar or slight modifications of the labelled input examples. For example, if you need to <br> find all the sentences about Obama's marriage, you would search for all sentences that match <br> Michelle and Barack Obama. Furthermore, those examples could be used to find any sentence <br> about a marriage. Another example could be for a given list of movies, match all the sentences that have <br> movie names. Clearly, such heuristics result in noisy input sets.|\n","|   Unsupervised         | Unlabeled features   | The system is expected to find some dependencies and patterns without clearly stating <br>which patterns we are looking for. E.g., clustering of documents, topics extraction <br>from the documents, etc. |\n","|   Hybrid               | Varies for method <br>combinations   | Combines several approaches that are mentioned earlier for various purposes. |"]},{"cell_type":"markdown","metadata":{"id":"ctAAN8LC577n","colab_type":"text"},"source":["# Evaluation Metrics"]},{"cell_type":"markdown","metadata":{"id":"ZiVg-jd2yEvl","colab_type":"text"},"source":["Depending on the problem you solve or the  data (balanced number of classes or not) different metrics\n","<br>might be more suitable.\n","\n","* **Accuracy**\n","<br>\n","$ Accuracy = \\frac{n_{correct}}{N}$, where $N$ is a total number of example that we were analysing,\n","<br>$n_{corrent}$ is the number of examples that we have guessed the label.\n","\n","*a.k.a. Classification*\n","\n","* **Precision**\n","<br>\n","In case of classification, and in particular, if the classes are unbalanced, Precision should be a better measure to check.\n","<br>\n","$ Precision = \\frac{n_{correct\\ class\\ prediction}}{n_{class\\ predictions}} = \\frac{TP}{TP + FP}$,\n","where $TP, FP, FN, TN$ are explained [here](https://en.wikipedia.org/wiki/False_positives_and_false_negatives).\n","\n","* **Precision@K**\n","<br>\n","Once your task is not simply to classify some examples, but e.g. rank them, a popular metric is P@K.\n","<br>Here you use K (typically, 1,3,5, 10, etc.) and compute precision for those K results in your ranking.\n","<br>For example, if you have a query for which you need to find similar documents,\n","<br>P@K would be computed for the top K documents that are returned for a query as described above for those K elements.\n","\n","* **Recall**\n","<br>\n","Another very important concept in NLP, is Recall - which basically tell us how many of the class examples\n","<br>or positive examples (maybe documents), our system had managed to extract.\n","<br>\n","$ Precision = \\frac{n_{correct\\ class\\ prediction}}{n_{positive\\ class\\ examples}} = \\frac{TP}{TP + FN}$ $\n","\n","* **F1**\n","<br>\n","[F1](https://en.wikipedia.org/wiki/F1_score) is the harmonic mean of the two above metrics.\n","<br>Usually used to compare different approaches when you are not optimizing for P and R in particular\n","<br>but rather overall performance.\n","\n","*a.k.a. Clustering* \n","\n","* [**Silhouette coefficient, Modularity, etc**](https://en.wikipedia.org/wiki/Cluster_analysis).\n","<br>\n","Once we move from the classification problems, and focus on clustering of the documents, many things can be measured depending\n","<br>if you have labels or not.\n","<br>\n","The case where we do not have labels: \n","  * For already proposed clustering of the input, *modularity* would measure the how well nodes are assigned to the clusters.\n","  <br> In particular, we would estimate how our current assignment is different from the assumed random graph.\n","  * Silhouette coefficient estimates how average distance between objects in the same clusters differs\n","  <br>from the average distance of those objects to the other clusters.\n","  * [Davies–Bouldin index](https://en.wikipedia.org/wiki/Davies-Bouldin_index) measures the difference between inter- and intra-cluster similarity.\n","\n","*a.k.a. Language models*\n","\n","* **Perplexity**\n","<br>\n","Once we consider text generation tasks, where we typically do not have labelled examples, or multiple correct answers\n","<br>are possible, *perplexity* can be used to evaluate your model. tl;dr - Perplexity estimates how surprised the model\n","<br>is upon receiving an input, e.g., how the model is surprised that the next word after a current one\n","<br>\"eat\" is \"me\", or \"meat\" or whatever. Typically, the lower the perplexity the more information about\n","<br>the input the model has (no surprises). Another interpretation is that we compare our probability\n","<br>distribution to the fair die."]},{"cell_type":"markdown","metadata":{"id":"mdNL0j7Ry7Ic","colab_type":"text"},"source":["\n","# Where text data comes from"]},{"cell_type":"markdown","metadata":{"id":"i_VIzhe_yynz","colab_type":"text"},"source":["Like anywhere in Data Science, it is important to first understand your data! Note: Of course, if you have it!\n","\n","In case you do not have the data:\n","\n","*Raw text data (Unlabeled/Non-annotated):*\n","* Pay for it :)\n","* Crawl it from the Web.\n","<br>\n","Examples: [Scrappy](https://scrapy.org/), [wiki/google crawler](http://www.netinstructions.com/how-to-make-a-web-crawler-in-under-50-lines-of-python-code/)\n","<br>\n","Of course, you might need to play with various IP addresses, throttling rates etc.\n","* Twitter\n","<br>\n","You can get both historical or livestream data. For any of the two, you can get 1% of the stream for free.\n","  * 1% historical tweets from [archive.org](https://archive.org/details/twitterstream?sort=-date).\n","  * Similarly, you can get 1% of full twitter stream as tweets appear via Twitter API.\n","  <br>You can also specify keywords (up to 2K) that would match tweets in real time - as a result you get up to\n","  <br>1% total stream of messages. Note: if you have a not very popular query, you might get all the tweets about it,\n","  <br>but of course, no guarantees.\n","* News media\n","<br>\n","[News Archive](https://archive.org),\n","[GDELT](https://www.gdeltproject.org)\n","* Wikipedia\n","<br>\n","[Wikipedia dumps](https://dumps.wikimedia.org/)\n","\n","*Annotated data: *\n","* Annotate and/or even generate your data using CrowdSourcing\n","  <br>\n","  [Crowdflower](https://www.figure-eight.com/), [MechanicalTurk](https://www.mturk.com/), etc.\n","* Annotate using auxiliary lexical resources\n","<br>\n","[LIWC](http://liwc.wpengine.com/) a tool that analyses your textual input on the presence of various\n","<br>\"shades\" - informal speech; syntactic structures; affect, social words; conginitive, perpetual,\n","<br>biological processes; relativity; personal concerns, etc.\n","\n","*Finally, you have the data!\n","... it is still not the final truth! *\n","\n","Work of [A. Olteanu](http://www.aolteanu.com/) well describes various [biases and pitfalls](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2886526) when it comes to the data analysis.\n","* Biases\n","* Not representative\n","* Noisy data\n","* Incomplete data\n","* Incorrect data\n","* Missing data, etc.\n","\n","So, first, get to know your data!\n","\n","Note: In this class, we will be working with the data that fits into the memory.\n","<br>However, once it does not, you should adapt other methods to scale your pipelines - Flume, Spark, hdfs, etc. -\n","<br>which are out of the scope of this class.\n"]},{"cell_type":"markdown","metadata":{"id":"AApQIpaXdZKP","colab_type":"text"},"source":["---"]}]}