{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDuZ1Bj95_Tg"
   },
   "source": [
    "# Typical NLP-heavy problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwfFp05VZRiK"
   },
   "source": [
    "* *Classification*: Classifying documents into particular categories (giving labels).\n",
    "* *Regression*: Predict numerical values\n",
    "* *Clustering*: Separating documents into non- overlapping subsets.\n",
    "* *Ranking*: For a given input, rank documents according to some metric. \n",
    "* *Association rule mining*: Infer likely associations patterns in data.\n",
    "* *Structured output*: Bounding boxes, parse trees, etc.\n",
    "* *Sequence-to-sequence*: For a given input/source text, generate output annotations or another text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6LV3tKgEi-g"
   },
   "source": [
    "To address many of the challenges above, several important things has to be clarified beforehand:\n",
    "\n",
    "1. Set the research/exploration goal (e.g., how heavy will be the earthquake on a given day).\n",
    "2. Make a hypothesis ( e.g., strength of the earthquake is an informative signal).\n",
    "3. Collect the data (e.g., collect historical strength of the quakes on eachday).\n",
    "4. Test the hypothesis (e.g., train a model using the data)\n",
    "5. Analyze the results (e.g., are results better than existing systems).\n",
    "6. Reach a conclusion (e.g., the model should be used or not because of A, B, C).\n",
    "7. Refine hypothesis and repeat. (e.g., time of the year could be a useful signal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kau8uIhCLB19"
   },
   "source": [
    "Below is the table of the approaches that can be applied to the problems/applications above.\n",
    "\n",
    "| Type                   | Input   | Clarifications|\n",
    "|------------------------|---------|---------------|\n",
    "|   Rule-based           |Explicit linguistic patterns, <br>lexicons, etc.| Such an approach always have predefined behaviour and usually can't generalize usually. |\n",
    "|   Supervised           | Training examples: typically <br>tuples of the form <br>(features, label) | Here input features are usually some vectorized, transformed, normalized input representation.|\n",
    "|   Semi-supervised or <br> Pseudo-relevance <br>feedback   | Same as in supervised base, <br>but also results of the prediction <br>(e.g. with greatest confidence) <br>are used as input pairs.   | Once we trained the system on the input (feature, label), we label unknown examples <br>with the model. If the confidence of the label is high (on that unseen example), <br>we add those examples to the input set and retrain the model. |\n",
    "|   Distant supervision  | Same as for supervised learning, <br>but (feature, label) pairs does not <br> come from the annotation, but from <br>some heuristic-based annotation.   | Rather than annotating thousands of examples (documents, sentences, words, etc.), <br>we take a few examples of the class and try to generalize and match those <br>in the domain-specific corpus. Such systems first generate or extract examples that are <br> very similar or slight modifications of the labelled input examples. For example, if you need to <br> find all the sentences about Obama's marriage, you would search for all sentences that match <br> Michelle and Barack Obama. Furthermore, those examples could be used to find any sentence <br> about a marriage. Another example could be for a given list of movies, match all the sentences that have <br> movie names. Clearly, such heuristics result in noisy input sets.|\n",
    "|   Unsupervised         | Unlabeled features   | The system is expected to find some dependencies and patterns without clearly stating <br>which patterns we are looking for. E.g., clustering of documents, topics extraction <br>from the documents, etc. |\n",
    "|   Hybrid               | Varies for method <br>combinations   | Combines several approaches that are mentioned earlier for various purposes. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctAAN8LC577n"
   },
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZiVg-jd2yEvl"
   },
   "source": [
    "Depending on the problem you solve or the  data (balanced number of classes or not) different metrics\n",
    "<br>might be more suitable.\n",
    "\n",
    "* **Accuracy**\n",
    "<br>\n",
    "$ Accuracy = \\frac{n_{correct}}{N}$, where $N$ is a total number of example that we were analysing,\n",
    "<br>$n_{corrent}$ is the number of examples that we have guessed the label.\n",
    "\n",
    "*a.k.a. Classification*\n",
    "\n",
    "* **Precision**\n",
    "<br>\n",
    "In case of classification, and in particular, if the classes are unbalanced, Precision should be a better measure to check.\n",
    "<br>\n",
    "$ Precision = \\frac{n_{correct\\ class\\ prediction}}{n_{class\\ predictions}} = \\frac{TP}{TP + FP}$,\n",
    "where $TP, FP, FN, TN$ are explained [here](https://en.wikipedia.org/wiki/False_positives_and_false_negatives).\n",
    "\n",
    "* **Precision@K**\n",
    "<br>\n",
    "Once your task is not simply to classify some examples, but e.g. rank them, a popular metric is P@K.\n",
    "<br>Here you use K (typically, 1,3,5, 10, etc.) and compute precision for those K results in your ranking.\n",
    "<br>For example, if you have a query for which you need to find similar documents,\n",
    "<br>P@K would be computed for the top K documents that are returned for a query as described above for those K elements.\n",
    "\n",
    "* **Recall**\n",
    "<br>\n",
    "Another very important concept in NLP, is Recall - which basically tell us how many of the class examples\n",
    "<br>or positive examples (maybe documents), our system had managed to extract.\n",
    "<br>\n",
    "$ Precision = \\frac{n_{correct\\ class\\ prediction}}{n_{positive\\ class\\ examples}} = \\frac{TP}{TP + FN}$ $\n",
    "\n",
    "* **F1**\n",
    "<br>\n",
    "[F1](https://en.wikipedia.org/wiki/F1_score) is the harmonic mean of the two above metrics.\n",
    "<br>Usually used to compare different approaches when you are not optimizing for P and R in particular\n",
    "<br>but rather overall performance.\n",
    "\n",
    "*a.k.a. Clustering* \n",
    "\n",
    "* [**Silhouette coefficient, Modularity, etc**](https://en.wikipedia.org/wiki/Cluster_analysis).\n",
    "<br>\n",
    "Once we move from the classification problems, and focus on clustering of the documents, many things can be measured depending\n",
    "<br>if you have labels or not.\n",
    "<br>\n",
    "The case where we do not have labels: \n",
    "  * For already proposed clustering of the input, *modularity* would measure the well nodes are assigned to the clusters.\n",
    "  <br> In particular, we would estimate how our current assignment is different from the assumed random graph.\n",
    "  * Silhouette coefficient estimates how average distance between objects in the same clusters differs\n",
    "  <br>from the average distance of those objects to the other clusters.\n",
    "  * [Daviesâ€“Bouldin index](https://en.wikipedia.org/wiki/Davies-Bouldin_index) measures the difference between inter- and intra-cluster similarity.\n",
    "\n",
    "*a.k.a. Language models*\n",
    "\n",
    "* **Perplexity**\n",
    "<br>\n",
    "Once we consider text generation tasks, where we typically do not have labelled examples, or multiple correct answers\n",
    "<br>are possible, *perplexity* can be used to evaluate your model. tl;dr - Perplexity estimates how surprised the model\n",
    "<br>is upon receiving an input, e.g., how the model is surprised that the next word after a current one\n",
    "<br>\"eat\" is \"me\", or \"meat\" or whatever. Typically, the lower the perplexity the more information about\n",
    "<br>the input the model has (no surprises). Another interpretation is that we compare our probability\n",
    "<br>distribution to the fair die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6xZCAdT2Jlg"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 158320,
     "status": "ok",
     "timestamp": 1563881779883,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "3LyC7367GWCi",
    "outputId": "7daf81e8-f58a-41e0-c141-a2f7264951f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-14bbd877-eca3-460e-8aa7-9b4d0cdaecea\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-14bbd877-eca3-460e-8aa7-9b4d0cdaecea\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving socialmedia-disaster-tweets-DFE.csv to socialmedia-disaster-tweets-DFE (1).csv\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[('Just', 'RB'), ('happened', 'VBD'), ('a', 'DT'), ('terrible', 'JJ'), ('car', 'NN'), ('crash', 'NN')]\n",
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/3d/89b27573f56abcd1b8c9598b240f53c45a3c79aa0924a24588e99716043b/gensim-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.2MB 1.2MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.189)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.189)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Installing collected packages: gensim\n",
      "  Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "Successfully installed gensim-3.8.0\n",
      "--2019-07-23 11:34:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-07-23 11:34:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2019-07-23 11:34:36--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: â€˜glove.6B.zipâ€™\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  35.5MB/s    in 33s     \n",
      "\n",
      "2019-07-23 11:35:09 (25.1 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "#@title Data preparation and preprocessing { display-mode: \"form\" }\n",
    "# You can download these data from the following link and store somewhere locally:\n",
    "\n",
    "csv_file = 'https://www.figure-eight.com/wp-content/uploads/2016/03/socialmedia-disaster-tweets-DFE.csv'\n",
    "\n",
    "#    Example on how the data cab be loaded here from the local file system. \n",
    "#    More options here: https://colab.research.google.com/notebook#fileId=/v2/external/notebooks/io.ipynb&scrollTo=vz-jH8T_Uk2c\n",
    "#    Downloaded the input csv.\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import pandas as pd\n",
    "with open('socialmedia-disaster-tweets-DFE.csv',\n",
    "          mode = 'r',\n",
    "          encoding = 'ascii',\n",
    "          errors = 'ignore'\n",
    "         ) as csvfile:\n",
    "  disasters_df = pd.read_csv(csvfile, header=0)\n",
    "\n",
    "#    Golden examples\n",
    "golden = disasters_df['_unit_state'] == \"golden\"\n",
    "golden_positive = disasters_df['choose_one_gold'] == \"Relevant\"\n",
    "golden_negative = disasters_df['choose_one_gold'] == \"Not Relevant\"\n",
    "\n",
    "#    Annotated examples\n",
    "finalized = disasters_df['_unit_state'] == \"finalized\"\n",
    "confident = disasters_df['choose_one:confidence'] > 0.8\n",
    "finalized_positive = disasters_df['choose_one'] == \"Relevant\"\n",
    "finalized_negative = disasters_df['choose_one'] == \"Not Relevant\"\n",
    "\n",
    "clean_confident_entries = disasters_df[\n",
    "    golden | (finalized & finalized_positive & confident) | \n",
    "    (finalized & finalized_negative & confident)]\n",
    "#    Need to decide on tokenization - most of the time \" \" is a good guess.\n",
    "\n",
    "# Imports\n",
    "# Note: following nltk packages should be downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import (\n",
    "    sent_tokenize as splitter,\n",
    "    wordpunct_tokenize as tokenizer\n",
    ")\n",
    "\n",
    "# Splits a string into sentences and words.\n",
    "def tokenize(text):\n",
    "  return [tokenizer(sentence) for sentence in splitter(text)]\n",
    "\n",
    "# In this exercise we do not care about the sentences (if any),\n",
    "# so let's flatten the list.\n",
    "def flatten(nested_list):\n",
    "  return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "def tokenize_flatten_df(row, field):\n",
    "  return flatten(tokenize(row[field]))\n",
    "\n",
    "import re\n",
    "\n",
    "# remove urls\n",
    "def remove_urls(text):\n",
    "  return re.sub(r\"(https?\\://)\\S+\", \"\", text)\n",
    "\n",
    "# remove mentions (@name) completely\n",
    "def remove_mentions(text):\n",
    "  return re.sub(r\"@[^:| ]+:? ?\", \"\", text)\n",
    "\n",
    "# remove \"RT:\", if the tweet contains it.\n",
    "def remove_rt(text):\n",
    "  if text.lower().startswith(\"rt:\"):\n",
    "    return text[3:].strip()\n",
    "  return text\n",
    "def remove_urls_mentions_rt_df(row, field):\n",
    "  return remove_rt(remove_mentions(remove_urls(row[field])))\n",
    "\n",
    "clean_confident_entries['text_cleaned_from_url_mentions_rt'] = \\\n",
    "    clean_confident_entries.apply(\n",
    "        lambda row: remove_urls_mentions_rt_df (row, 'text'),\n",
    "        axis=1)\n",
    "\n",
    "clean_confident_entries['text_tokenized'] = \\\n",
    "    clean_confident_entries.apply(\n",
    "        lambda row:\n",
    "            tokenize_flatten_df (row, 'text_cleaned_from_url_mentions_rt'),\n",
    "        axis=1)\n",
    "def replace_hashtags_from_text(text):\n",
    "  return re.sub(r\"#+ ?\", \"\", text)\n",
    "# remove hashtags\n",
    "def replace_hashtags_from_list(tokens_list):\n",
    "  return [token for token in tokens_list if token != \"#\"]\n",
    "\n",
    "# remove digits\n",
    "def remove_digits(tokens_list):\n",
    "  return [token for token in tokens_list \n",
    "                if not re.match(r\"[-+]?\\d+(\\.[0-9]*)?$\", token)]\n",
    "\n",
    "# remove all tokens that contains non alpha numeric, punctuation\n",
    "def remove_containing_non_alphanum(tokens_list):\n",
    "  return [token for token in tokens_list if token.isalpha()]\n",
    "# lowercase everything\n",
    "def lowercase_list(tokens_list):\n",
    "  return [token.lower() for token in tokens_list]\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# remove stopwords\n",
    "def remove_stopwords(tokens_list):\n",
    "  return [token for token in tokens_list\n",
    "                if not token in stopwords.words(u'english')]\n",
    "# Iterates over the elements of the list with tokens and performs cleanup.\n",
    "def clean_tokens(row, field):\n",
    "  return replace_hashtags_from_list(\n",
    "            remove_digits(\n",
    "                remove_containing_non_alphanum(\n",
    "                    lowercase_list(remove_stopwords(row[field])))))\n",
    "\n",
    "clean_confident_entries['text_tokenized_cleaned'] = \\\n",
    "    clean_confident_entries.apply(\n",
    "        lambda row:\n",
    "            clean_tokens (row, 'text_tokenized'),\n",
    "        axis=1)\n",
    "nltk.download('wordnet')\n",
    "porter_stemmer = nltk.PorterStemmer()\n",
    "lancaster_stemmer = nltk.LancasterStemmer()\n",
    "snowball_stemmer = nltk.SnowballStemmer(u'english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatizer.stem = lemmatizer.lemmatize\n",
    "normalizers = [\n",
    "    ('porter_stemmer', porter_stemmer),\n",
    "    ('lancaster_stemmer', lancaster_stemmer),\n",
    "    ('snowball_stemmer', snowball_stemmer),\n",
    "    ('wordnet_lemmatizer', lemmatizer)\n",
    "]\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(clean_confident_entries['text_tokenized'][0])\n",
    "print (tagged)\n",
    "\n",
    "!pip install --upgrade gensim\n",
    "\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "#    Note: it might take several minutes. Be patient!\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove2word2vec(glove_input_file='glove.6B.50d.txt',\n",
    "               word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\")\n",
    "\n",
    "# Compute vector representation for each document in the collection.\n",
    "# Term frequency\n",
    "# TFIDF\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "class TermDocumentCounts:\n",
    "  def __init__(self):\n",
    "    # Counters of all the words in the corpus\n",
    "    self.total_word_counts = Counter()\n",
    "    self.total_number_of_words = 0\n",
    "    self.term_count_per_document = defaultdict(Counter)\n",
    "    self.number_of_words_per_document = defaultdict(int)\n",
    "    self.number_of_document = 0\n",
    "    self.df = defaultdict(int)\n",
    "    \n",
    "  def update(self, document_id, tokens):\n",
    "    self.number_of_document += 1\n",
    "    num_tokens = len(tokens)\n",
    "    self.total_word_counts.update(tokens)\n",
    "    self.total_number_of_words += num_tokens\n",
    "    self.term_count_per_document[document_id].update(tokens)\n",
    "    self.number_of_words_per_document[document_id] += num_tokens\n",
    "    for token in set(tokens):\n",
    "      self.df[token] += 1\n",
    "  \n",
    "  def most_common_word_in_document(self, document_id, top_n = None):\n",
    "    return self.term_count_per_document[document_id].most_common(top_n)\n",
    "  \n",
    "  def _compute_tfidf_for_word(self, document_id, word):\n",
    "        tf = self.term_count_per_document[document_id][word]\n",
    "        idf = math.log(self.number_of_document / self.df[word], 10)\n",
    "        return tf * idf\n",
    "  \n",
    "  # Returns the list of words ranked accoring to TFIDF score.\n",
    "  def ranked_document_words_tfidf(self, document_id, top_n=None):\n",
    "        tfidfs = [\n",
    "            (word, self._compute_tfidf_for_word(document_id, word))\n",
    "              for word in self.term_count_per_document[document_id].keys()\n",
    "        ]\n",
    "        tfidfs.sort(key=lambda x: x[1], reverse=True)\n",
    "        if not top_n:\n",
    "            top_n = len(tfidfs)\n",
    "        return tfidfs[:top_n]\n",
    "      \n",
    "  # Returns the list of words ranked accoring to max conditional probability\n",
    "  # change.\n",
    "  def ranked_document_words_conditional_probability(self, document_id,\n",
    "                                                    top_n = None):\n",
    "    word_posterior = [(word, self.compute_posterios(document_id, word)) \\\n",
    "              for word, count in \\\n",
    "                self.term_count_per_document[document_id].items()]\n",
    "    word_prior = {word: self.compute_priors(word) for word, _ in word_posterior}\n",
    "    conditional_probability = sorted(\n",
    "        [(word, (math.log((probability / word_prior[word]), 2))) \\\n",
    "            for word, probability in word_posterior],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    if not top_n:\n",
    "        top_n = len(conditional_probability)\n",
    "    return conditional_probability[:top_n]  \n",
    "      \n",
    "  # Computes posterior word distribution over given document.\n",
    "  def compute_posterios(self, document_id, word):\n",
    "    return self.term_count_per_document[document_id][word] \\\n",
    "              / self.number_of_words_per_document[document_id]\n",
    "  \n",
    "  # Computes prior word probability distribution\n",
    "  def compute_priors(self, word):\n",
    "    return self.total_word_counts[word] / self.total_number_of_words\n",
    "    \n",
    "corpus_counts = TermDocumentCounts()\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  corpus_counts.update(document_id, row['text_tokenized_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gP7k67UJOujQ"
   },
   "source": [
    "# Documents representation\n",
    "\n",
    "Let's finally impose some vectorized representation on our documents so that the machine and math could easily operate over it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEVlAcrJf1a0"
   },
   "source": [
    "### Document representation as term counts (Bag-Of-Words, or BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpW-BkIdYGgh"
   },
   "outputs": [],
   "source": [
    "#    Now we need to convert our documents to the common representation.\n",
    "#    You can do it manually, just for fun, or we can already use some libs.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = []\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  corpus.append(\" \".join(row['text_tokenized_cleaned']))\n",
    "  \n",
    "document_term_matrix = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1563882773150,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "Wfmp1qlbPhe4",
    "outputId": "d7a1d885-37ea-4fa9-9faa-d52f4067f0fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1563882829892,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "htQBnkUkrWyX",
    "outputId": "49079801-051f-497e-fded-77a1e8719d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ziuw',\n",
       " 'zombie',\n",
       " 'zone',\n",
       " 'zones',\n",
       " 'zoom',\n",
       " 'zouma',\n",
       " 'zrnf',\n",
       " 'zss',\n",
       " 'zuma',\n",
       " 'zumiez']"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKUKV9lMYv8W"
   },
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E35O9XbkccTk"
   },
   "source": [
    "* We need to represent our data somehow\n",
    "  * Bag-of-words\n",
    "  <br>\n",
    "  The concept comes from the fact, that we could represent our input as a set of tokens or words without preserving any notion of order,\n",
    "  <br> just like items in the bag. In such cases, we assume that each document is simply a set of words and each\n",
    "  <br> word could appear several or zero times in a bag.\n",
    "  <br> In order to somehow presenve the order in the documents, n-grams (n consequitive words/tokens in the document) could be introduced.\n",
    "  <br> Though being rather powerful, number of features/dimentions grow exponentially.\n",
    "  * Document-term frequency\n",
    "  <br>\n",
    "  Similar to the BOW model though instead of 0s and 1s for each word that appear in the document,\n",
    "  <br> we would place TF or TFIDF score of the word in the document.\n",
    "  * Semantic Vector space representation\n",
    "  <br>\n",
    "  Previous representation is rather simple but very very high dimentional, as usually number of tokens in the \"bag\" could be \n",
    "  <br> around 1M (despite them being present or not in the document). \n",
    "  <br> As you could have seen in the ML course, we could reduce the dimentionality by somehow converting our \n",
    "  <br> 1M dimentional document representation to let's say 300. \n",
    "  <br> Such transformation can be both done for words (special case of the document with a single 1 on the word position and 0s elsewhere),\n",
    "  <br> combination of words, sentences, paragraphs, and whole documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S99D10LX324y"
   },
   "source": [
    "### Feature weighting and noise removal\n",
    "\n",
    "Apart from the general stopword removal that you might decide to do, several other techniques exist.\n",
    "<br>Those could be used for, first, cleaning the overall corpus from too frequent/too sparse tokens.\n",
    "<br>And, second, for potentially better representations of feature weights (rather thatn 0 and 1).\n",
    "\n",
    "Note: would work better for BOW representation, but might work worse for sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM3W3x7CWv8L"
   },
   "source": [
    "*Weighting tokens/features*:\n",
    "\n",
    "* *TF-IDF*\n",
    "<br>\n",
    "Term frequency * inverse document frequency. The intuition here is to preserve all words in the dataset in each document,\n",
    "<br>however, **correct** their frequencies with word specificity, i.e., the more specific word is (if the word appear in only several\n",
    "<br>documents it might be quite specific) the more we would want to promote word's score.\n",
    "<br>One way to compute this specificity is to inverse docuemtn frequency. This way, if the term is very frequent accross documents -\n",
    "<br>we would have to divide the term frequency with a high value, and vice versa.\n",
    "<br>Note: here still the more frequent the word is in a document, the more likely it is to be important, though it might be slightly degraded if it is not specific.\n",
    "\n",
    "* Filter words with the highest difference between *conditional probability* of a word within a document vs. a word within the whole corpus\n",
    "<br>\n",
    "For conditional probability, we do not care that much about the word frequency in a document, but rather how much its\n",
    "<br>probability changes in a given document sample with respect to the overall probability in the entire dataset.\n",
    "<br>\n",
    "As a result, conditional probability might score higher more specific (but less frequent) words.\n",
    "\n",
    "*Removing noise*:\n",
    "\n",
    "* Filter out ones with the *highest Document Frequency or part of the probability mass*\n",
    "<br>\n",
    "You might have heard, that natural language vocabularies exhibit frequency distribution similar to [Zipf law](https://en.wikipedia.org/wiki/Zipfs_law)\n",
    "<br>(subclass of the long-tail distribution where i_-frequent word would be twice as likely to appear in the corpus is _i+1_-frequent word).\n",
    "<br>Though zipf allow more relaxed behaviour, i.e., a lot of tokens have low frequency and few have high frequency.\n",
    "\n",
    "* *Background-foreground overlap idea*\n",
    "<br>\n",
    "The idea behind this methods is to compare frequency distributions of two independent datasets and diminish the intersection.\n",
    "<br>This allows to deemphasize the effect of the high frequency noise. In more details, we take frequency distribution of one dataset,\n",
    "<br>then same for some other totally independent (unrelated) dataset, extract top-n words in each distribution, and remove any items in the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "oLo9nIA1FRRR"
   },
   "outputs": [],
   "source": [
    "#@title Document Frequency Helper Functionality\n",
    "# Compute vector representation for each document in the collection.\n",
    "# Term frequency\n",
    "# TFIDF\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "class TermDocumentCounts:\n",
    "  def __init__(self):\n",
    "    # Counters of all the words in the corpus\n",
    "    self.total_word_counts = Counter()\n",
    "    self.total_number_of_words = 0\n",
    "    self.term_count_per_document = defaultdict(Counter)\n",
    "    self.number_of_words_per_document = defaultdict(int)\n",
    "    self.number_of_document = 0\n",
    "    self.df = defaultdict(int)\n",
    "    \n",
    "  def update(self, document_id, tokens):\n",
    "    self.number_of_document += 1\n",
    "    num_tokens = len(tokens)\n",
    "    self.total_word_counts.update(tokens)\n",
    "    self.total_number_of_words += num_tokens\n",
    "    self.term_count_per_document[document_id].update(tokens)\n",
    "    self.number_of_words_per_document[document_id] += num_tokens\n",
    "    for token in set(tokens):\n",
    "      self.df[token] += 1\n",
    "  \n",
    "  def most_common_word_in_document(self, document_id, top_n = None):\n",
    "    return self.term_count_per_document[document_id].most_common(top_n)\n",
    "  \n",
    "  def _compute_tfidf_for_word(self, document_id, word):\n",
    "        tf = self.term_count_per_document[document_id][word]\n",
    "        idf = math.log(self.number_of_document / self.df[word], 10)\n",
    "        return tf * idf\n",
    "  \n",
    "  # Returns the list of words ranked accoring to TFIDF score.\n",
    "  def ranked_document_words_tfidf(self, document_id, top_n=None):\n",
    "        tfidfs = [\n",
    "            (word, self._compute_tfidf_for_word(document_id, word))\n",
    "              for word in self.term_count_per_document[document_id].keys()\n",
    "        ]\n",
    "        tfidfs.sort(key=lambda x: x[1], reverse=True)\n",
    "        if not top_n:\n",
    "            top_n = len(tfidfs)\n",
    "        return tfidfs[:top_n]\n",
    "      \n",
    "  # Returns the list of words ranked accoring to max conditional probability\n",
    "  # change.\n",
    "  def ranked_document_words_conditional_probability(self, document_id,\n",
    "                                                    top_n = None):\n",
    "    word_posterior = [(word, self.compute_posterios(document_id, word)) \\\n",
    "              for word, count in \\\n",
    "                self.term_count_per_document[document_id].items()]\n",
    "    word_prior = {word: self.compute_priors(word) for word, _ in word_posterior}\n",
    "    conditional_probability = sorted(\n",
    "        [(word, (math.log((probability / word_prior[word]), 2))) \\\n",
    "            for word, probability in word_posterior],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    if not top_n:\n",
    "        top_n = len(conditional_probability)\n",
    "    return conditional_probability[:top_n]  \n",
    "      \n",
    "  # Computes posterior word distribution over given document.\n",
    "  def compute_posterios(self, document_id, word):\n",
    "    return self.term_count_per_document[document_id][word] \\\n",
    "              / self.number_of_words_per_document[document_id]\n",
    "  \n",
    "  # Computes prior word probability distribution\n",
    "  def compute_priors(self, word):\n",
    "    return self.total_word_counts[word] / self.total_number_of_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6o1DxxyTaGy"
   },
   "outputs": [],
   "source": [
    "corpus_counts = TermDocumentCounts()\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  corpus_counts.update(document_id, row['text_tokenized_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqmeuOhilqTg"
   },
   "source": [
    "What see what are words that are common in some of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1563883060948,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "8yBhuD44Wc8e",
    "outputId": "ac818ba2-a5dc-40c1-801c-ef829d95b80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 with top 5 important words by tfidf or condiotional probability.\n",
      "['Just', 'happened', 'a', 'terrible', 'car', 'crash']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('terrible', 2.966610986681934) \t ('terrible', 10.79125593263882)\n",
      "('happened', 2.697765674389354) \t ('happened', 9.898171136555332)\n",
      "('just', 2.033557776312547) \t ('just', 7.691720259087907)\n",
      "('car', 1.8874297406343095) \t ('car', 7.122877423730027)\n",
      "('crash', 1.7905197276262528) \t ('crash', 6.778431892281237)\n",
      "\n",
      "Document 2 with top 5 important words by tfidf or condiotional probability.\n",
      "['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us', 'all']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('deeds', 3.811709026696191) \t ('deeds', 12.920538949583786)\n",
      "('forgive', 3.3345877719765284) \t ('forgive', 11.33557644886263)\n",
      "('allah', 3.1127390223601723) \t ('allah', 10.598610854696425)\n",
      "('our', 2.581260105317917) \t ('our', 8.833076108333447)\n",
      "('reason', 2.581260105317917) \t ('reason', 8.833076108333447)\n",
      "\n",
      "Document 1002 with top 5 important words.\n",
      "['Colorado', 'is', 'a', 'Spanish', 'word', '([', 'Latin', 'origin', ']', 'meaning', \"'\", 'reddish', \"'\", 'or', \"'\", 'colored', \"')\", 'all', 'you', 'dummies', 'are', 'pronouncing', 'it', 'wrong', '!!', '!']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('latin', 3.811709026696191) \t\t ('latin', 12.46110733094649)\n",
      "('reddish', 3.811709026696191) \t\t ('reddish', 12.46110733094649)\n",
      "('colored', 3.811709026696191) \t\t ('colored', 12.46110733094649)\n",
      "('dummies', 3.811709026696191) \t\t ('dummies', 12.46110733094649)\n",
      "('pronouncing', 3.811709026696191) \t\t ('pronouncing', 12.46110733094649)\n",
      "\n",
      "Document 1003 with top 5 important words.\n",
      "['Why', 'Some', 'Traffic', 'Is', 'Freezing', 'Cold', 'And', 'Some', 'Blazing', 'Hot', 'And', 'How', 'To', 'Heat', 'Up', 'Some', 'Of', 'Your', 'Traffic']\n",
      "TFIDF \t\t\t\t\t Condifional probability\n",
      "('some', 7.141035787611611) \t\t ('freezing', 11.672611436140203)\n",
      "('traffic', 4.793471357450746) \t\t ('some', 8.35068334125284)\n",
      "('and', 3.8967723331514703) \t\t ('cold', 8.213179817502905)\n",
      "('freezing', 3.811709026696191) \t\t ('traffic', 7.814630441012629)\n",
      "('cold', 2.770316341537966) \t\t ('your', 6.814630441012629)\n"
     ]
    }
   ],
   "source": [
    "for document_id in range(2):\n",
    "  print (\"\\nDocument\", document_id + 1,\n",
    "         \"with top 5 important words by tfidf or condiotional probability.\")\n",
    "  print (clean_confident_entries['text_tokenized'][document_id])\n",
    "  print (\"TFIDF\", \"\\t\\t\\t\\t\\t\", \"Condifional probability\")\n",
    "  for word_tfidf, word_conditional in zip(\n",
    "      corpus_counts.ranked_document_words_tfidf(document_id, 5),\n",
    "      corpus_counts.ranked_document_words_conditional_probability(document_id,\n",
    "                                                                  5)):\n",
    "    print (word_tfidf, \"\\t\", word_conditional)\n",
    "for document_id in range(1001,1003):\n",
    "  print (\"\\nDocument\", document_id + 1, \"with top 5 important words.\")\n",
    "  print (clean_confident_entries['text_tokenized'][document_id])\n",
    "  print (\"TFIDF\", \"\\t\\t\\t\\t\\t\", \"Condifional probability\")\n",
    "  for word_tfidf, word_conditional in zip(\n",
    "      corpus_counts.ranked_document_words_tfidf(document_id, 5),\n",
    "      corpus_counts.ranked_document_words_conditional_probability(document_id,\n",
    "                                                                  5)):\n",
    "    print (word_tfidf, \"\\t\\t\", word_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMi0VRsnJ3rm"
   },
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNVCn_sPcpdM"
   },
   "source": [
    "Before coding, check modules of [scikit-learn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction) and more examples [here](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). \n",
    "\n",
    "*Potential features*:\n",
    "\n",
    "* Words\n",
    "* N-grams\n",
    "* Character N-gram\n",
    "* Skip-gram\n",
    "* Part-of-Speech (POS)\n",
    "\n",
    "*Potential values*:\n",
    "\n",
    "* TF/Count Vectors ([CountVectorizer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html),  ([HashingVectorizer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)))\n",
    "* TF-IDF ([TFIDFVectorizer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) or [TFIDFTransformer scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTI5AN2NJ0cl"
   },
   "source": [
    "## Preserving Word Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqoyNNLWco7P"
   },
   "source": [
    "Depending what is your particular problem, you might need to care about the order of words in your input data.\n",
    "\n",
    "* This could happen either on the data preprocessing step, where you would encode the order as part of the feature input,\\\n",
    "<br>e.g., n-grams, specific features that correspond to the positions of the words.\n",
    "<br>Further all those features are sent to the particular methods of your choice.\n",
    "* If you do not want and can't polute the input with those additional information, you might need to reply on the methods that\n",
    "<br>implicitly encode the order in the data as they read the input.\n",
    "  * Hiden Markov Models ([HMM on wiki](https://en.wikipedia.org/wiki/Hidden_Markov_model), [HMM Fundamentals](http://cs229.stanford.edu/section/cs229-hmm.pdf)), Conditional Random Fields ([CRF](https://en.wikipedia.org/wiki/Conditional_random_field)), Reccurrent neural networks, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnCAcjjCgpWC"
   },
   "source": [
    "---\n",
    "\n",
    "# Document Classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yIQpdKm9n7Q"
   },
   "source": [
    "# Simple Classification by Lexical Rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd5QQyjrAjxJ"
   },
   "source": [
    " $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ $\\Rightarrow_{YES}$  Relevant\n",
    "\n",
    "Text Document $\\ \\ \\ \\Rightarrow \\ \\ \\ $ Are there any relevant terms present?\n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ $\\Rightarrow_{NO}$ Rot_Relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PrjZyLUhArH"
   },
   "source": [
    "## Building lexicons: PMI and Relatedness (Strength of Association)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJv1oVmedf1x"
   },
   "source": [
    "One classic way to document classification is to classify texts based on the presence of terms from  either existing or heuristically built lexicons.\n",
    "\n",
    "Pointwise Mutual Information (or PMI) can be a great estimator of the informativeness or relevance weight of words. It was originally introduced to measure how strongly two words are associated, that is how often they go together among their occurrences.\n",
    "\n",
    "In the context of classification, PMI measures the relatedness between a term $t$ and a certain class $c$. It is defined as follows:\n",
    "\n",
    "$PMI(t,c) = \\log_2\\frac{P(t, c)}{P(t)P(c)} = \\log_2\\frac{P(t|c)}{P(t)}$, where $P(t,c)$ is a joint probability of $t$ and $c$, and $P(t)$, $P(c)$ are the marginal probabilities of $t$ and $c$.\n",
    "\n",
    "Another potential reason to explore the most important or relevant keywords in the dataset is to determine whether there are some dependencies and correlations in the data. Thus, confirming potential benefit of the machine intelligence :)\n",
    "\n",
    "Note: simple correlation measure could be also checked prior to PMI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aR9zftfOMbru"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_terms_pmi(inclass_corpus_counts, total_corpus_counts):\n",
    "  PMI_dict = defaultdict(float)\n",
    "  for word, inclass_count in inclass_corpus_counts.total_word_counts.items():\n",
    "    inclass_term_probability = float(inclass_count / inclass_corpus_counts.total_number_of_words)\n",
    "    marginal_term_probability = \\\n",
    "      float(total_corpus_counts.total_word_counts[word] / \\\n",
    "            total_corpus_counts.total_number_of_words)\n",
    "    PMI_dict[word] = math.log(float(inclass_term_probability / \\\n",
    "                              marginal_term_probability), 2)\n",
    "  return PMI_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuWDwTACOKwy"
   },
   "outputs": [],
   "source": [
    "# We compute PMI of all words in the corpus to the positive 'relevant' class.\n",
    "positive_corpus_counts = TermDocumentCounts()\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  if row['choose_one'] == \"Relevant\" or \\\n",
    "        row['choose_one_gold'] == \"Relevant\":\n",
    "      positive_corpus_counts.update(document_id, row['text_tokenized_cleaned'])\n",
    "      \n",
    "PMI_positive = compute_terms_pmi(positive_corpus_counts, corpus_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 504,
     "status": "ok",
     "timestamp": 1563884219976,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "QeA0kcKVpx8g",
    "outputId": "8d5b925c-0bcb-4fa3-eb4a-0cafd8f2f632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deeds', 1.02804356582442)\n",
      "('geese', 1.02804356582442)\n",
      "('fleeing', 1.02804356582442)\n",
      "('ronge', 1.02804356582442)\n",
      "('sask', 1.02804356582442)\n",
      "('residents', 1.02804356582442)\n",
      "('shelter', 1.02804356582442)\n",
      "('notified', 1.02804356582442)\n",
      "('officers', 1.02804356582442)\n",
      "('orders', 1.02804356582442)\n"
     ]
    }
   ],
   "source": [
    "for related_words in sorted(PMI_positive.items(), key=lambda x:x[1], reverse=True)[:10]:\n",
    "  print (related_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1563884830193,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "tnPkHOj_PJR4",
    "outputId": "39b9f6bd-f2bd-4762-9f15-e897be7dc172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: orders\n",
      "Appeared in positive class: 8 times\n",
      "Appeared overall: 8 times\n",
      "PMI to positive class: 1.02804356582442\n"
     ]
    }
   ],
   "source": [
    "# Print occurrences for particular word:\n",
    "word = 'orders'\n",
    "print('Term:', word)\n",
    "print('Appeared in positive class:', positive_corpus_counts.total_word_counts[word], 'times')\n",
    "print('Appeared overall:', corpus_counts.total_word_counts[word], 'times')\n",
    "print('PMI to positive class:', PMI_positive[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHBt-LSdjsye"
   },
   "source": [
    "However, the above measure does not tell us anything if a term with a high PMI for a given class has also a very high score for a \"not-class\". A better metric would be to compute a difference between a term's PMIs to related texts and non-related texts. This approach was used for [building lexicons of relevant words for data collection](http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/download/8091/8138/) and for [bulding sentiment lexicons](https://arxiv.org/pdf/cs/0212032.pdf). \n",
    "\n",
    "<br>\n",
    "The score (called Relatedness or Strength of Association) will be:\n",
    "<br>\n",
    "$$relatedness_{PMI}(t) = PMI(t, related) - PMI(t, not\\ related) = $$ $$\\log_2\\frac{p(t|related)}{p(t|not\\ related)}$$ where\n",
    "<br>\n",
    "$$p(t|related)= \\frac{count(t\\ in\\ related)}{count(t\\ in\\ related) + count(not\\ t\\ in\\ related)}$$\n",
    "<br>\n",
    "$$p(t|not\\ related)= \\frac{count(t\\ in\\ not\\ related)}{count(t\\ in\\ not\\ related) + count(not\\ t\\ in\\ not\\ related)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnvSiFsSSbjJ"
   },
   "outputs": [],
   "source": [
    "# We compute PMI of all words in the corpus to the negative 'not relevant' class.\n",
    "negative_corpus_counts = TermDocumentCounts()\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  if row['choose_one'] == \"Not Relevant\" or \\\n",
    "        row['choose_one_gold'] == \"Not Relevant\":\n",
    "      negative_corpus_counts.update(document_id, row['text_tokenized_cleaned'])\n",
    "      \n",
    "PMI_negative = compute_terms_pmi(negative_corpus_counts, corpus_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bIL7f6gqxVv"
   },
   "outputs": [],
   "source": [
    "# Computing PMI relatedness score for all terms in the corpus.\n",
    "# Terms occurring only in one class will have zero score for another class.\n",
    "PMI_relatedness = defaultdict(float)\n",
    "for word in corpus_counts.df.keys():\n",
    "  pmi_word_pos = PMI_positive[word] if word in PMI_positive else 0.0\n",
    "  pmi_word_neg = PMI_negative[word] if word in PMI_negative else 0.0\n",
    "  PMI_relatedness[word] = pmi_word_pos - pmi_word_neg\n",
    "  \n",
    "# Note: If you'd like, you can try to implement count-based version as described in the\n",
    "# formulas above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1563884844023,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "gRx2M_TCr5G2",
    "outputId": "49412e15-007d-433c-8066-ba1d71a4fd39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('suicide', 7.285146784187203)\n",
      "('bombing', 6.4313675250382465)\n",
      "('homes', 6.265781459320271)\n",
      "('families', 5.641290594412478)\n",
      "('spill', 5.641290594412478)\n",
      "('warning', 5.61091694536896)\n",
      "('outbreak', 5.579890049748334)\n",
      "('california', 5.51575971232862)\n",
      "('wildfire', 5.4313675250382465)\n",
      "('anniversary', 5.413880098309407)\n"
     ]
    }
   ],
   "source": [
    "for related_words in sorted(PMI_relatedness.items(), key=lambda x:x[1], reverse=True)[:10]:\n",
    "  print (related_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1563884872899,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "kbMGb1c1TIUf",
    "outputId": "a0ba6b11-d7f9-4d5a-9bbf-85d5823af421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('screamed', -4.943671906308678)\n",
      "('blew', -4.897868216695554)\n",
      "('fan', -4.801652901436251)\n",
      "('panic', -4.751026828366283)\n",
      "('crushed', -4.698559408472147)\n",
      "('things', -4.64411162444977)\n",
      "('pick', -4.467233862365691)\n",
      "('quiz', -4.335989329087439)\n",
      "('finally', -4.265600001196041)\n",
      "('hat', -4.191599419752264)\n"
     ]
    }
   ],
   "source": [
    "for nonrelated_words in sorted(PMI_relatedness.items(), key=lambda x:x[1], reverse=False)[:10]:\n",
    "  print (nonrelated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1563884888555,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "SS3CT8KaTNLr",
    "outputId": "8743c615-ed6e-46d8-b070-4c19a03018b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: canada\n",
      "Appeared in positive class: 10 times\n",
      "Appeared in negative class: 4 times\n",
      "Appeared overall: 14 times\n",
      "PMI to positive class: 0.5426167386541784\n",
      "PMI to negative class: -0.835639449924506\n",
      "PMI relatedness score : 1.3782561885786844\n"
     ]
    }
   ],
   "source": [
    "# Print occurrences for particular word:\n",
    "word = 'canada'\n",
    "print('Term:', word)\n",
    "print('Appeared in positive class:', positive_corpus_counts.total_word_counts[word], 'times')\n",
    "print('Appeared in negative class:', negative_corpus_counts.total_word_counts[word], 'times')\n",
    "print('Appeared overall:', corpus_counts.total_word_counts[word], 'times')\n",
    "print('PMI to positive class:', PMI_positive[word])\n",
    "print('PMI to negative class:', PMI_negative[word])\n",
    "print('PMI relatedness score :', PMI_relatedness[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1563884977487,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "B7e1sE6lTZnZ",
    "outputId": "0ada799c-2ccb-4c8f-cb96-71476ba68e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
      "Relevant Relevant\n",
      "['idgaf', 'tough', 'canada', 'north', 'philly', 'meek', 'acting', 'like', 'bitch', 'amp', 'drake', 'body', 'bagging', 'ass', 'tracks']\n",
      "Not Relevant nan\n",
      "['the', 'cryptic', 'words', 'guided', 'pilots', 'hiroshima', 'bombing', 'mission', 'canada']\n",
      "Relevant nan\n",
      "['a', 'decade', 'long', 'billion', 'dollar', 'deluge', 'messages', 'government', 'canada', 'harperslegacy']\n",
      "Not Relevant nan\n",
      "['dead', 'injured', 'displaced', 'syria', 'visit', 'us', 'canada']\n",
      "Relevant nan\n"
     ]
    }
   ],
   "source": [
    "# Examples of texts with a particular word.\n",
    "word = 'canada'\n",
    "limit_to_print = 5\n",
    "current_print = 0\n",
    "\n",
    "for document_id, row in clean_confident_entries.iterrows():\n",
    "  if word in row['text_tokenized_cleaned']:\n",
    "    print(row['text_tokenized_cleaned'])\n",
    "    print(row['choose_one'], row['choose_one_gold'])\n",
    "    current_print += 1\n",
    "    if (current_print >= limit_to_print):\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1563885033057,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "lQ4rDFN2UXBk",
    "outputId": "ddd222fd-fe99-4dfe-8f07-1e5859360579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('another', 0.003860673797185999)\n",
      "('say', 0.017853945876685998)\n",
      "('movie', -0.031134747559017736)\n",
      "('man', -0.03678131070015989)\n",
      "('order', -0.050587110225190114)\n",
      "('live', -0.05470321869742224)\n",
      "('days', 0.056328093691321565)\n",
      "('london', 0.056328093691321565)\n",
      "('consider', 0.056328093691321565)\n",
      "('park', 0.056328093691321565)\n"
     ]
    }
   ],
   "source": [
    "# Explore terms with PMI relatedness around 0 (no strong relatedness to any).\n",
    "# They are candidates for removal from the lexicon.\n",
    "for weak_words in sorted(PMI_relatedness.items(), key=lambda x:abs(x[1]), reverse=False)[:10]:\n",
    "  print (weak_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGy6KB0tFtFD"
   },
   "source": [
    "Note: The same techniques can be used to assign term feature weights and for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8jmzvt_0IOuM"
   },
   "source": [
    "Note: PMI is biased towards inflating scores for low-occuring terms, it requires some smoothing of occurrences (e.g. by simply adding 1 to all counts: [Add-1 or Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)) and ideally removal of such low-occuring terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc5PfB3nh21k"
   },
   "source": [
    "# Document Classification with Machine-Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S21l8P2fh5qY"
   },
   "source": [
    "## General description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ta0HSFzwjDZQ"
   },
   "source": [
    "Classification is a prediction problem where by a given (labelled) set of data, e.g., data representation (features) and each entry class.\n",
    "<br>\n",
    "There can be two or multiple classes: \n",
    "* (1) positive/negative - binary classification;\n",
    "* (2) multiple classes - multi-class classification problem.\n",
    "\n",
    "The latter can be represented via several binary classifications.\n",
    "\n",
    "Let's try to get some indepth understanding of the two important classification methods that are used in practice, easy and fast.\n",
    "\n",
    "### Notations\n",
    "\n",
    "Let's denote $\\dot X$ -  our input set (observation set), and $\\dot y$ - our output set.\n",
    "\n",
    "Note: It is possible that no assumptions are made on the $\\dot X$ and $\\dot y$, however,\n",
    "<br>\n",
    "in this class we assume that $\\dot y \\in \\{c_1, \\dots c_k\\}$ is a finite set and it contains \n",
    "<br>\n",
    "$k$ classes (labels).\n",
    "<br>\n",
    "In our use case, $\\dot X$ will be a set of documents (as we have seen in the previous exercises), and\n",
    "<br>\n",
    "$\\dot y$ is a set of document categories. The classification goal is to assign a category to each document.\n",
    "\n",
    "More notations:\n",
    "\n",
    "* $X$ is a random variable taking values in the same space as entries of $\\dot X$.\n",
    "* $y$ is a random variable taking values in the same space as entries of $\\dot y$.\n",
    "\n",
    "* $P(y)$ denotes a probability associated with the event $Y = \\dot y$, and similarly for joint and conditional probabilities.\n",
    "\n",
    "Probability recap:\n",
    "\n",
    "$P(x, y) = P(y|x) P(x) = P(x|y) P(y)$\n",
    "\n",
    "### Model types\n",
    "\n",
    "**Generative** (joint) classifiers place probabilities over both observed data and the hidden structure\n",
    "<br>(a.k.a., generate the observed data from the hidden structure):\n",
    "<br>\n",
    "Example models: n-gram language models, Naive Bayes, Hidden Markov Models, Probabilistic context-free grammats, etc.\n",
    "<br>\n",
    "These models give joint probabilities $P(X,y)$ and try to maximize joint likelihood.\n",
    "\n",
    "**Discriminative** (conditional) models take the data as given, and put a probability over hidden structure given the data.\n",
    "<br>\n",
    "Example models: Logistic regression, conditional loglinear or maximum entropy models, conditional random fields, SVM, perceptron, etc.\n",
    "<br>\n",
    "These models give conditional probabilities $P(y|X)$ and try to maximize conditional likelihood.\n",
    "<br>Here, the observed data is taken as is and the output class is modelled only using conditional probability.\n",
    "\n",
    "More details about these model types [here](https://web.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWCgpFuBhfHK"
   },
   "source": [
    "## <font color='gray'>Generative Classifiers: </font> [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zvc-Q9Z2YGY9"
   },
   "source": [
    "If we knew the true distribution $P(X, y)$, the best possible classifier (Bayes optimal) would be the one that predicts according to:\n",
    "<br>\n",
    "$ \\hat y =  arg\\,max_{y \\in Y} P(y|x)$\n",
    "<br>\n",
    "$ = arg\\,max_{y \\in Y} \\frac{P(x, y)}{P(x)} $\n",
    "<br>\n",
    "$ \\hat = arg\\,max_{y \\in Y} P(x, y) $\n",
    "<br>\n",
    "$  = arg\\,max_{y \\in Y} P(y) P(x|y) $\n",
    "\n",
    "Thus, we need to estimate the probability distributions $P(y)$ and $P(X|y)$ - class priors and conditional, respectively.\n",
    "\n",
    "Assumption is that the data is generated according to following generative scenario (independently for each $m = 1, \\dots M$).\n",
    "\n",
    "1. A class $y_m \\sim P(y)$ is drawn from the class prior distribution\n",
    "2. An input $X_m \\sim P(X|y = y_m)$ is drawn from the corresponding class conditional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2ixTbyaYHdA"
   },
   "source": [
    "### Training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URrt7qdsYNEN"
   },
   "source": [
    "To train the generative model, we need to estimate probabilities $P(y)$ and $P(X|y)$ using given dataset.\n",
    "\n",
    "Once the training is done, we take new input and we need to make predictions according to:\n",
    "<br>\n",
    "$ \\hat y = arg\\,max_{y \\in Y} \\hat P(y) \\hat P(x|y)$ using the estimated probabilities.\n",
    "\n",
    "This step is called inference or decoding.\n",
    "\n",
    "Two problems remain:\n",
    "\n",
    "1. How we shoud define the distributions $\\hat P(y)$ and $\\hat P(X|y)$? (any independence assumptions etc.)\n",
    "2. How to estimate the parameters of the distributions from the training data?\n",
    "\n",
    "*Former*, is dependent on the application. Quite often, there is a natural decomposition of the input variables $X$ in $J$ components\n",
    "<br>(like a document is decomposed to the words/features etc).\n",
    "<br>\n",
    "The **Naive Bayes** method makes the following assumptions: $X_1, \\dots, X_J$ are conditionally independent given the class.\n",
    "<br>Or\n",
    "$$P(X|y) = \\prod_{j=1}^{J} P (X_j | y)$$\n",
    "\n",
    "As a result, such an assumption, greatly reduces the number of parameters to be estimated (degrees of freedom, dimentionalities) -\n",
    "<br>from $O(\\exp(J))$ to $O(J)$.\n",
    "<br>Thus, computation become more efficient for large $J$ and the risk of overfitting also descreases.\n",
    "\n",
    "*Latter* problem, can be solved using **maximum likelihood estimation**, which aims to maximize the probability of the training sample,\n",
    "<br>assuming that each point was generated **independently** from selected distribution. Let's denote such probability as follows:\n",
    "$$ P(Sample) = \\prod_{m=1}^{M} P(x^m, y^m)$$\n",
    "$$ = \\prod_{m=1}^{M} P(y^m) \\prod_{j=1}^{J} P(x_j^m|y^m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_87XglopYOWB"
   },
   "source": [
    "### Example: Multinomial Naive Bayes for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMS82guV646M"
   },
   "source": [
    "Let's say we need to classifiy documents: $X$ is the set of possible documents, and $y = \\{i_1, \\dots, y_k\\}$ is a set of clases for those documents.\n",
    "<br>Let $V = \\{w_1, \\dots, w_J\\}$ be the vocabulary, i.e., set of words that appear in the documents.\n",
    "\n",
    "As one of the document representation that we have seen, BOW can be applied here (order of words is ignored and thus each document is\n",
    "<br>a collection of the words with their frequencies).\n",
    "\n",
    "Interesting, such a representation - BOW - is equivalent to the Naive Bayes assumption with the multinomial model.\n",
    "<br>\n",
    "What does it mean, is that, we associate to each class a multinomial distribution, that ignores the word ordering,\n",
    "<br>but takes into consideration the frequencies with which each word appears in a document.\n",
    "\n",
    "In such scenario, each document is assumed to be generated as follows. \n",
    "1. Class y is generated according to $P(y)$.\n",
    "2. $X$ is generated by sequentially picking words from $V$ (vocabulary) with replacement.\n",
    "<br>\n",
    "Each word is picked with probability $P(w_j|y)$.\n",
    "\n",
    "For example, the probability to generate a document $x = w_{j1} \\dots w_{jL}$ is\n",
    "$$ P(x|y) = \\prod_{l=1}^{L} P(w_{jl}|y) =  \\prod_{j=1}^{J} P(w_{j}|y)^{n_j(x)}$$\n",
    ", where L is the length of the docuemnt; $n_j(x)$ is the number of occurrences of word $w_j$ in document $x$.\n",
    "\n",
    "Considering the independence assumption, we only need to estimate the following parameters:\n",
    "<br>\n",
    "$\\hat P(y_1), \\dots, \\hat P(y_K)$, and $\\hat P(w_j | y_k) $ for $j=1, \\dots, J$ and $k = 1, \\dots, K$.\n",
    "\n",
    "Let's denote $J_k$ indices of the documents that are labeled with the class $k$.\n",
    "\n",
    "Thus, *maximum likelihood estimation* prior and conditionals are the following:\n",
    "<br>\n",
    "$ \\hat P (y_k) = \\frac{|J_K|}{M}$ - relative frequencies of the classes\n",
    "<br>\n",
    "$ \\hat P (w_j | y_k) = \\frac{ \\sum_{m \\in J_k} n_j(x_m)}{ \\sum_{i=1}^{J} \\sum_{m \\in J_k} n_i(x_m) }$ - relative frequencies of the words accross documents with that class.\n",
    "\n",
    "Important, what will happen if during the prediction we encounter unseen words?\n",
    "<br>\n",
    "We can solve it using add-one smoothing technique.\n",
    "<br>\n",
    "$ \\hat P (w_j | y_k) = \\frac{ 1 + \\sum_{m \\in J_k} n_j(x_m)}{ V + \\sum_{i=1}^{J} \\sum_{m \\in J_k} n_i(x_m) }$, where $V$ is the number of distinct words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "NhFAztCQiIxs"
   },
   "outputs": [],
   "source": [
    "#@title Multinomial Naive Bayes Implementation\n",
    "import numpy as np\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "        self.likelihood = 0\n",
    "        self.prior = 0\n",
    "        self.smooth = True\n",
    "        self.smooth_param = 1\n",
    "\n",
    "    def train(self, x, y):\n",
    "        # n_docs = no. of documents\n",
    "        # n_words = no. of unique words\n",
    "        n_docs, n_words = x.shape\n",
    "\n",
    "        # classes = a list of possible classes\n",
    "        classes = np.unique(y)\n",
    "        # n_classes = no. of classes\n",
    "        n_classes = np.unique(y).shape[0]\n",
    "\n",
    "        # initialization of the prior and likelihood variables\n",
    "        prior = np.zeros(n_classes)\n",
    "        likelihood = np.zeros((n_words, n_classes))\n",
    "\n",
    "        # We need to compute the values of the prior and likelihood parameters\n",
    "        # and place them in the variables called \"prior\" and \"likelihood\".\n",
    "        # Examples:\n",
    "        # prior[0] is the prior probability of a document being of class 0\n",
    "        # likelihood[4, 0] is the likelihood of the fifth feature/word being\n",
    "        # active, given that the document is of class 0\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            # docs_in_class = indices of documents in class i\n",
    "            docs_in_class = np.nonzero(y == classes[i])\n",
    "            # rior = fraction of documents with this class\n",
    "            prior[i] = 1.0 * len(docs_in_class) / n_docs\n",
    "\n",
    "            # word_count_in_class = count of word occurrences in documents\n",
    "            # of class i\n",
    "            word_count_in_class = np.sum(x[docs_in_class, :][0], axis=0)\n",
    "            # total_words_in_class = total number of words in\n",
    "            # documents of class i\n",
    "            total_words_in_class = word_count_in_class.sum()\n",
    "            if not self.smooth:\n",
    "                # likelihood = count of occurrences of a word in a class\n",
    "                likelihood[:, i] = word_count_in_class / total_words_in_class\n",
    "            else:\n",
    "                likelihood[:, i] = \\\n",
    "                    (word_count_in_class + self.smooth_param) \\\n",
    "                    / (total_words_in_class + self.smooth_param*n_words)\n",
    "\n",
    "        # +1 account for the parameters of the prior estimation\n",
    "        params = np.zeros((n_words+1, n_classes))\n",
    "        for i in range(n_classes):\n",
    "            params[0, i] = np.log(prior[i])\n",
    "            params[1:, i] = np.nan_to_num(np.log(likelihood[:, i]))\n",
    "        self.likelihood = likelihood\n",
    "        self.prior = prior\n",
    "        self.trained = True\n",
    "        return params\n",
    "      \n",
    "    def get_label(self, x, w):\n",
    "        # Computes the label for each data point\n",
    "        scores = np.dot(x, w)\n",
    "        return np.argmax(scores, axis=1).transpose()\n",
    "\n",
    "    def test(self, x, w):\n",
    "        # Classifies the points based on a weight vector.\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model not trained. Cannot test\")\n",
    "            return 0\n",
    "        x = self.add_intercept_term(x)\n",
    "        return self.get_label(x, w)\n",
    "\n",
    "    def add_intercept_term(self, x):\n",
    "        # Adds a column of ones to estimate the intercept term for separation\n",
    "        # boundary\n",
    "        n_docs, n_features = x.shape\n",
    "        intercept = np.ones([n_docs, 1])\n",
    "        x = np.hstack((intercept, x))\n",
    "        return x\n",
    "\n",
    "    def evaluate(self, truth, predicted):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(truth)):\n",
    "            if truth[i] == predicted[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8esm8NmvhlRl"
   },
   "source": [
    "##  <font color='gray'> Discriminative classifiers: </font> [Logistic Regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) (Maximum Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KO4WsS8Z673Y"
   },
   "source": [
    "Classifiers that does not explicitly model $P(y)$ and $P(X|y)$ are called discriminative.\n",
    "<br>\n",
    "They are basically any function that maps objects $x \\in X$ into classes $y \\in Y$.\n",
    "\n",
    "In such classifiers, input $x \\in X$ is subject to a set of descriptions or measurements, which are called features.\n",
    "<br>A feature is simply a real number that describes the value of some property of input.\n",
    "\n",
    "*Feature mapping* is a mapping from the input $X$ to the *feature vector representation of x.*\n",
    "\n",
    "Feature can be anything, like \"a sentence contains word Trump\", etc.\n",
    "\n",
    "*Join feature mapping* is a collection of join input-output measurements, e.g., \"a sentence contains word Trump and topic is election\".\n",
    "\n",
    "Linear classifiers are very popular in NLP applications. The classification decision in such models are made based on the rule:\n",
    "<br>\n",
    "$$ \\hat y = arg\\,max\\, \\lambda \\cdot f(x, y) $$\n",
    "where $\\lambda$ - weight vector,\n",
    "$f(x, y)$ - feature vector,\n",
    "<br>\n",
    "$\\lambda \\cdot f(x, y) = \\sum_{d=1}^{D} \\lambda_d f_d(x,y)$ - inner product between $\\lambda$ and $f(x,y)$.\n",
    "\n",
    "As a result, $f_d(x,y)$ has a weight $\\lambda_d$, and for each class $y \\in Y$,\n",
    "<br>a score is computed as linear combination of all the weighted features.\n",
    "\n",
    "If we decompose weight vector $\\lambda = (\\lambda_1, \\dots, \\lambda_K)$, we can have\n",
    "<br>$\\lambda \\cdot f(x, y) == \\lambda_k \\cdot f(x)$ - a.k.a., each class gets its own weight vector and we only need\n",
    "<br>a feature vectors that only looks at the input.\n",
    "\n",
    "Prediction rule will be the following for such models:\n",
    "$$ \\hat y = arg\\,max\\,\\lambda_k \\cdot f(x) $$\n",
    "\n",
    "Despite the representation details, exponential models (log-linear, maxent, logistic, Gibbs):\n",
    "* Make a probability model from the linear combination of weight and features.\n",
    "$$ P(y| X, \\lambda)  = \\frac{\\exp \\sum_d \\lambda_d f_d(x, y)}{\\sum_{k}\\exp \\sum_d \\lambda_d f_d(x, y_k)}$$\n",
    "Interesting, is that the gradient of the logarithm of the denominator - equals the feature expectation.\n",
    "* Weights are the parameters of the probability model, combined via a soft max function.\n",
    "\n",
    "Now, we estimate conditional log-likelihood:\n",
    "$$ L(\\lambda; Data) = \\frac{1}{N} \\log P_{\\lambda} (y^1, \\dots, y^N | x^1, \\dots, x^N) $$\n",
    "$$ = \\frac{1}{N} \\log \\prod_{n=1}^{N} P_{\\lambda} (y^n | x^n) $$\n",
    "$$ = \\frac{1}{N} \\sum_{n=1}^{N} \\log P_{\\lambda} (y^n | x^n) $$\n",
    "\n",
    "So $\\lambda$ can be found as $arg\\,max \\, L(\\lambda; Data)$, plus we can add some regularization here.\n",
    "\n",
    "We won't have a closed form solution for the problem, thus, we follow the following standard procedure.\n",
    "<br>Since the function is convex (-log), local optimum equals to the global one, thus, we differentiate it by our parameter and\n",
    "<br>using some numerical method to find the parameters that optimizes our function.\n",
    "\n",
    "We can use batch gradient method to optimize our problem.\n",
    "$\\lambda^{t+1} = \\lambda^t - \\mu_t \\nabla_{\\lambda} (-\\log L(\\lambda; Data))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "zx8ShVOsowf1"
   },
   "outputs": [],
   "source": [
    "#@title Perceptrone code\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, nr_epochs=10, learning_rate=1, averaged=True):\n",
    "        self.trained = False\n",
    "        self.nr_epochs = nr_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.averaged = averaged\n",
    "        self.params_per_round = []\n",
    "\n",
    "    def train(self, x, y, seed=1):\n",
    "        self.params_per_round = []\n",
    "        x_orig = x[:, :]\n",
    "        x = self.add_intercept_term(x)\n",
    "        nr_x, nr_f = x.shape\n",
    "        nr_c = np.unique(y).shape[0]\n",
    "        w = np.zeros((nr_f, nr_c))\n",
    "        for epoch_nr in range(self.nr_epochs):\n",
    "\n",
    "            # use seed to generate permutation\n",
    "            np.random.seed(seed)\n",
    "            perm = np.random.permutation(nr_x)\n",
    "\n",
    "            # change the seed so next epoch we don't get the same permutation\n",
    "            seed += 1\n",
    "\n",
    "            for nr in range(nr_x):\n",
    "                # print \"iter %i\" %( epoch_nr * nr_x + nr)\n",
    "                inst = perm[nr]\n",
    "                y_hat = self.get_label(x[inst:inst+1, :], w)\n",
    "\n",
    "                if y[inst:inst+1] != y_hat:\n",
    "                    # Increase features of th e truth\n",
    "                    w[:, y[inst:inst+1]] += \\\n",
    "                        self.learning_rate * x[inst:inst+1, :].transpose()\n",
    "\n",
    "                    # Decrease features of the prediction\n",
    "                    w[:, y_hat] += \\\n",
    "                        -1 * self.learning_rate * x[inst:inst+1, :].transpose()\n",
    "\n",
    "            self.params_per_round.append(w.copy())\n",
    "            self.trained = True\n",
    "            y_pred = self.test(x_orig, w)\n",
    "            acc = self.evaluate(y, y_pred)\n",
    "            self.trained = False\n",
    "            print(\"Rounds: %i Accuracy: %f\" % (epoch_nr, acc))\n",
    "        self.trained = True\n",
    "\n",
    "        if self.averaged:\n",
    "            new_w = 0\n",
    "            for old_w in self.params_per_round:\n",
    "                new_w += old_w\n",
    "            new_w /= len(self.params_per_round)\n",
    "            return new_w\n",
    "        return w\n",
    "\n",
    "    def get_label(self, x, w):\n",
    "        # Computes the label for each data point\n",
    "        scores = np.dot(x, w)\n",
    "        return np.argmax(scores, axis=1).transpose()\n",
    "\n",
    "    def test(self, x, w):\n",
    "        # Classifies the points based on a weight vector.\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model not trained. Cannot test\")\n",
    "            return 0\n",
    "        x = self.add_intercept_term(x)\n",
    "        return self.get_label(x, w)\n",
    "\n",
    "    def add_intercept_term(self, x):\n",
    "        # Adds a column of ones to estimate the intercept term for separation\n",
    "        # boundary\n",
    "        nr_x, nr_f = x.shape\n",
    "        intercept = np.ones([nr_x, 1])\n",
    "        x = np.hstack((intercept, x))\n",
    "        return x\n",
    "\n",
    "    def evaluate(self, truth, predicted):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(truth)):\n",
    "            if truth[i] == predicted[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "VWXgFRpZpLN-"
   },
   "outputs": [],
   "source": [
    "#@title Maximum Entropy Classifier (or Logistic Regression) Implementation\n",
    "import numpy as np\n",
    "import scipy.optimize.lbfgsb as opt2\n",
    "from tensorflow.contrib.solvers.python.ops import util\n",
    "\n",
    "class MaxEntBatch:\n",
    "\n",
    "    def __init__(self, regularizer=1):\n",
    "        self.trained = False\n",
    "        self.parameters = 0\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = self.add_intercept_term(x)\n",
    "        n_docs, n_features = x.shape\n",
    "        classes = np.unique(y)\n",
    "        n_classes = classes.shape[0]\n",
    "        # Add the bias feature\n",
    "        init_parameters = np.zeros((n_features, n_classes), dtype=float)\n",
    "        empirical_counts = np.zeros((n_features, n_classes))\n",
    "        classes_idx = []\n",
    "        for c_i, c in enumerate(classes):\n",
    "            idx = np.nonzero(y == c)\n",
    "            classes_idx.append(idx)\n",
    "            empirical_counts[:, c_i] = np.sum(x[idx, :], axis = 0)[0]\n",
    "        params = self.minimize_lbfgs(init_parameters, x, y, self.regularizer,\n",
    "                                     empirical_counts, classes_idx, n_docs,\n",
    "                                     n_features, n_classes)\n",
    "        self.trained = True\n",
    "        return params\n",
    "\n",
    "    def minimize_lbfgs(self, parameters, x, y, sigma, empirical_counts,\n",
    "                       classes_idx, n_docs, n_features, n_classes):\n",
    "        parameters_flattened = parameters.reshape([n_features*n_classes],\n",
    "                                                  order=\"F\")\n",
    "        result, _, d = opt2.fmin_l_bfgs_b(self.get_objective,\n",
    "                                          parameters_flattened,\n",
    "                                          args=[x, y, sigma, empirical_counts,\n",
    "                                                classes_idx, n_docs, n_features,\n",
    "                                                n_classes])\n",
    "        return result.reshape([n_features, n_classes], order=\"F\")\n",
    "\n",
    "    # Obj =  -sum_(x,y) p(y|x) + sigma*||w||_2^2\n",
    "    # Obj = \\sum_(x,y) -w*f(x,y)+log(\\sum_(y') exp(w*f(x,y')))+sigma*||w||_2^2\n",
    "    def get_objective(self, parameters_flattened, x, y, sigma, empirical_counts,\n",
    "                      classes_idx, n_docs, n_features, n_classes):\n",
    "        parameters = parameters_flattened.reshape([n_features, n_classes],\n",
    "                                                  order=\"F\")\n",
    "        # f(x,y).w\n",
    "        scores = np.dot(x, parameters)\n",
    "        # exp(f(x,y).w)\n",
    "        exp_scores = np.exp(scores)\n",
    "        # sum_y exp(f(x,y).w)\n",
    "        z = exp_scores.sum(axis=1).reshape([n_docs, 1], order=\"F\")\n",
    "        # log sum_y exp(f(x,y).w)\n",
    "        logz = np.log(z)\n",
    "        sum_scores = 0\n",
    "        for i, classes in enumerate(classes_idx):\n",
    "            sum_scores += np.sum(scores[classes, i])\n",
    "\n",
    "        objective = - sum_scores / n_docs \\\n",
    "                    + np.sum(logz) / n_docs \\\n",
    "                    + 0.5 * sigma * util.l2norm_squared(parameters)\n",
    "        probs = exp_scores / z\n",
    "        exp_feat = np.dot(x.transpose(), probs)\n",
    "        grad = - empirical_counts / n_docs \\\n",
    "               + exp_feat / n_docs \\\n",
    "               + parameters * sigma \n",
    "        print(\"Objective = {0}\".format(objective))\n",
    "        return objective, grad.reshape([n_features * n_classes], order=\"F\")\n",
    "      \n",
    "    def get_label(self, x, w):\n",
    "        # Computes the label for each data point\n",
    "        scores = np.dot(x, w)\n",
    "        return np.argmax(scores, axis=1).transpose()\n",
    "\n",
    "    def test(self, x, w):\n",
    "        # Classifies the points based on a weight vector.\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model not trained. Cannot test\")\n",
    "            return 0\n",
    "        x = self.add_intercept_term(x)\n",
    "        return self.get_label(x, w)\n",
    "\n",
    "    def add_intercept_term(self, x):\n",
    "        # Adds a column of ones to estimate the intercept term for separation\n",
    "        # boundary\n",
    "        nr_x, nr_f = x.shape\n",
    "        intercept = np.ones([nr_x, 1])\n",
    "        x = np.hstack((intercept, x))\n",
    "        return x\n",
    "\n",
    "    def evaluate(self, truth, predicted):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(truth)):\n",
    "            if truth[i] == predicted[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BV5y4isKEfW1"
   },
   "source": [
    "## Running various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZ6m-NQPfe0n"
   },
   "outputs": [],
   "source": [
    "# Generating corpus of texts with corresponding golden labels.\n",
    "import numpy as np\n",
    "\n",
    "corpus = []\n",
    "labels = np.zeros(clean_confident_entries.shape[0], dtype=int)\n",
    "for i, (document_id, row) in enumerate(clean_confident_entries.iterrows()):\n",
    "  corpus.append(\" \".join(row['text_tokenized_cleaned']))\n",
    "  if (row['choose_one'].lower() == \"relevant\" or \\\n",
    "        row['choose_one_gold'] == \"Relevant\"):\n",
    "    labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyeS03Txv32i"
   },
   "outputs": [],
   "source": [
    "# We split the data into train/tet to avoid overfitting. Another strategy would be to do cross-validation, as below.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus_train, corpus_test, labels_train, labels_test = train_test_split(\n",
    "   corpus, labels, test_size=0.33, random_state=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOF7yi9EgHeD"
   },
   "outputs": [],
   "source": [
    "#@title Get feature representation of documents\n",
    "#    You can do it manually, just for fun, or we can already use some libs.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# We build document-term matrix for each dataset:\n",
    "document_term_matrix_train = vectorizer.fit_transform(corpus_train).toarray()\n",
    "document_term_matrix_test = vectorizer.transform(corpus_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsLmwnM_rVjp"
   },
   "outputs": [],
   "source": [
    "# Configuring evaluation function.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_classifier(classifier, X_train, X_test, y_train, y_test):\n",
    "  classifier.fit(X_train, y_train)\n",
    "  predicted_y_test = classifier.predict(X_test)\n",
    "  print(\"Accuracy:\", accuracy_score(y_test, predicted_y_test))\n",
    "  report = classification_report(y_test, predicted_y_test)\n",
    "  print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1563885982406,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "kLwc9JNxl3kB",
    "outputId": "75f89b0d-99a0-4716-ff9a-9b0c9ec1474d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8775700934579439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89      1154\n",
      "           1       0.89      0.84      0.86       986\n",
      "\n",
      "    accuracy                           0.88      2140\n",
      "   macro avg       0.88      0.88      0.88      2140\n",
      "weighted avg       0.88      0.88      0.88      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Run evaluation with MultinomialNB:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "evaluate_classifier(MultinomialNB(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4406,
     "status": "ok",
     "timestamp": 1563885992424,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "tAMX3zZ5oeA1",
    "outputId": "406d9529-09cb-419f-b51d-7bebfd185c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866822429906542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      1154\n",
      "           1       0.86      0.84      0.85       986\n",
      "\n",
      "    accuracy                           0.87      2140\n",
      "   macro avg       0.87      0.87      0.87      2140\n",
      "weighted avg       0.87      0.87      0.87      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Run evaluation with Perceptron classifier:\n",
    "from sklearn.linear_model import Perceptron\n",
    "evaluate_classifier(Perceptron(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1333,
     "status": "ok",
     "timestamp": 1563885996849,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "F8Pr11jyo5G8",
    "outputId": "b269210b-e800-47c8-fd30-ae3d293d429b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8855140186915887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90      1154\n",
      "           1       0.92      0.82      0.87       986\n",
      "\n",
      "    accuracy                           0.89      2140\n",
      "   macro avg       0.89      0.88      0.88      2140\n",
      "weighted avg       0.89      0.89      0.88      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Run evaluation with Logistic regression classifier:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "evaluate_classifier(LogisticRegression(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2338,
     "status": "ok",
     "timestamp": 1563886006872,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "HPQvCifNt9MF",
    "outputId": "c2c9c4d0-9f95-43a1-c96f-2c3a8a970152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8710280373831776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88      1154\n",
      "           1       0.89      0.82      0.85       986\n",
      "\n",
      "    accuracy                           0.87      2140\n",
      "   macro avg       0.87      0.87      0.87      2140\n",
      "weighted avg       0.87      0.87      0.87      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Run evaluation with Linear SVM classifier:\n",
    "from sklearn.svm import LinearSVC\n",
    "evaluate_classifier(LinearSVC(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BX7B2LW2oVsA"
   },
   "source": [
    "Note: You can change the meta-parameters of classifiers, e.g. avoid using Prior for MNB, penalty for regulazation in LogRegression, etc.\n",
    "<br>The optimal meta-parameters are usually optimized on a separate tuning dataset using cross-validation\n",
    "<br>(see [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) in scikit-learn or [Vizier](http://go/vizier) for Google internal optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6FKVcc3O6nf"
   },
   "source": [
    "### Trying different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6604,
     "status": "ok",
     "timestamp": 1563886017980,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "5Q71IW4Cue8b",
    "outputId": "71009a7b-c4ee-4077-f8fc-40bfbdd327f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8864485981308411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90      1154\n",
      "           1       0.93      0.81      0.87       986\n",
      "\n",
      "    accuracy                           0.89      2140\n",
      "   macro avg       0.89      0.88      0.88      2140\n",
      "weighted avg       0.89      0.89      0.89      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the results with n-grams.\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "document_ngram_matrix_train = ngram_vectorizer.fit_transform(corpus_train).toarray()\n",
    "document_ngram_matrix_test = ngram_vectorizer.transform(corpus_test).toarray()\n",
    "\n",
    "evaluate_classifier(LogisticRegression(), \n",
    "                    document_ngram_matrix_train, document_ngram_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1563886026562,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "08tmPdxKd-9h",
    "outputId": "0ba2bdc2-ec19-447f-b8af-2b35a84dd6e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.891588785046729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90      1154\n",
      "           1       0.94      0.82      0.87       986\n",
      "\n",
      "    accuracy                           0.89      2140\n",
      "   macro avg       0.90      0.89      0.89      2140\n",
      "weighted avg       0.90      0.89      0.89      2140\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Check what happens when only features with high PMI are used (feature selection).\n",
    "# Note: This evaluation setup is not fair as PMI relatedness scores were obtained over the full data (including test data). \n",
    "PMI_selected_words = [word for word, score in PMI_relatedness.items() if abs(score) >= 1.0]\n",
    "filtered_vectorizer = CountVectorizer(vocabulary = PMI_selected_words)\n",
    "\n",
    "document_filtered_term_matrix_train = filtered_vectorizer.fit_transform(corpus_train).toarray()\n",
    "document_filtered_term_matrix_test = filtered_vectorizer.transform(corpus_test).toarray()\n",
    "\n",
    "evaluate_classifier(LogisticRegression(), \n",
    "                    document_filtered_term_matrix_train, document_filtered_term_matrix_test, \n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FztH9XpHj2Hl"
   },
   "source": [
    "# Another classification problem: Sentiment Analysis\n",
    "\n",
    "One common classification problem is to understand whether users-generated texts (e.g. reviews or general feedback) are rather positive or negative.\n",
    "<br>The annotated data usually come from the reviews' websites, where users explicitly provide ratings along with the reviews.\n",
    "<br>The area of [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is large and involves understanding of\n",
    "<br>various expressions of opinions, emotions, and attidudues. \n",
    "\n",
    "In this class, let's see how would you predict polarity of movie reviews. We will focus on re-using already existing methods,\n",
    "<br>showing you how the classification would be actually done, without wasting time to reimplement what was already\n",
    "<br>implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJXXKnhlM6fK"
   },
   "source": [
    "## Exercise: Exploration of different classification approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvStn-yIzJPI"
   },
   "source": [
    "*Exercise*: Go over the cells here. Make sure you understand what is happening. Check the results of various classification methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRzww7KH0jZp"
   },
   "source": [
    "### Data and function imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1246,
     "status": "ok",
     "timestamp": 1563888995003,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "gnzWzAIMLQsL",
    "outputId": "d597cdb8-7967-4d2b-a089-7a4624b36efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPEfF6FuKtka"
   },
   "outputs": [],
   "source": [
    "#    Imports\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SciEG4QjJWJ3"
   },
   "outputs": [],
   "source": [
    "#    Extract lists of ids of the reviews that are labeled as negative and positive\n",
    "negative_ids = movie_reviews.fileids('neg')\n",
    "positive_ids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1697,
     "status": "ok",
     "timestamp": 1563889001357,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "Nori9MvhMI5S",
    "outputId": "15c2852e-983c-4c76-91f5-16ef34f2ec57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Some examples of the document ids with the negative reviews.\n",
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt']\n"
     ]
    }
   ],
   "source": [
    "print (\"---> Some examples of the document ids with the negative reviews.\")\n",
    "print (negative_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71ChhW8FKxim"
   },
   "outputs": [],
   "source": [
    "#    Let's prepare the list of texts and their classes as a training examples\n",
    "#    Note: the texts are already preprocessed and split to tokens.\n",
    "negative_reviews = [\n",
    "    \" \".join(movie_reviews.words(fileids=[f])) \n",
    "    for f in negative_ids\n",
    "]\n",
    "positive_reviews = [\n",
    "    \" \".join(movie_reviews.words(fileids=[f])) \n",
    "    for f in positive_ids\n",
    "]\n",
    "\n",
    "texts = negative_reviews + positive_reviews\n",
    "labels = [0] * len(negative_reviews) + [1] * len(positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1563889078867,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "bmKmQOweJbSd",
    "outputId": "98e791fd-7cc5-4a33-aa17-575f16de25ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . they get into an accident . one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . what ' s the deal ? watch the movie and \" sorta \" find out . . . critique : a mind - fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn ' t snag this one correctly . they seem to have taken this pretty neat concept , but executed it terribly . so what are the problems with the movie ? well , its main problem is that it ' s simply too jumbled . it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what ' s going on . there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . now i personally don ' t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film ' s biggest problem . it ' s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . and do they make things entertaining , thrilling or even engaging , in the meantime ? not really . the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half - way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn ' t the make the film all that more entertaining . i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! okay , we get it . . . there are people chasing her and we don ' t know who they are . do we really need to see it over and over again ? how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? apparently , the studio took this film away from its director and chopped it up themselves , and it shows . there might ' ve been a pretty decent teen mind - fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character ' s unraveling . overall , the film doesn ' t stick because it doesn ' t entertain , it ' s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . oh , and by the way , this is not a horror or teen slasher flick . . . it ' s just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . it also wrapped production two years ago and has been sitting on the shelves ever since . whatever . . . skip it ! where ' s joblo coming from ? a nightmare of elm street 3 ( 7 / 10 ) - blair witch 2 ( 7 / 10 ) - the crow ( 9 / 10 ) - the crow : salvation ( 4 / 10 ) - lost highway ( 10 / 10 ) - memento ( 10 / 10 ) - the others ( 9 / 10 ) - stir of echoes ( 8 / 10 )\n"
     ]
    }
   ],
   "source": [
    "#    See example text.\n",
    "print (texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hB7WabSt0n32"
   },
   "source": [
    "### Running basic classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGX5HFLXMMl_"
   },
   "outputs": [],
   "source": [
    "#    Bulding the pipeline to transform texts and classify them.\n",
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "  return Pipeline(\n",
    "      [('vectorizer', vectorizer),\n",
    "       ('transformer', transformer),\n",
    "       ('classifier', classifier)]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25884,
     "status": "ok",
     "timestamp": 1563889213541,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "zmcFKuPHM2os",
    "outputId": "e3f70020-9928-443a-f576-abe26b53f9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "0.8125\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "0.8210000000000001\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "0.8545\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
      "0.8575000000000002\n"
     ]
    }
   ],
   "source": [
    "#    Let's estimate the quality of classification from various methods\n",
    "for classifier in [MultinomialNB, LogisticRegression, LinearSVC, SGDClassifier]:\n",
    "  print (classifier)\n",
    "  print (cross_val_score(\n",
    "            text_classifier(\n",
    "                CountVectorizer(),\n",
    "                TfidfTransformer(),\n",
    "                classifier()), \n",
    "            texts,\n",
    "            labels,\n",
    "            cv = 5)\n",
    "         .mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2809,
     "status": "ok",
     "timestamp": 1563889333464,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "hyb-MXXRPAkb",
    "outputId": "06b4f91e-aa11-4de3-9a56-3f00051ccbee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#    Now let's use the classifier trained on the whole data for the prediction\n",
    "#    on some random texts.\n",
    "\n",
    "classification_pipeline = Pipeline(\n",
    "                            [(\"vectorizer\", TfidfVectorizer()),\n",
    "                             (\"classifier\", LinearSVC())]\n",
    "                          )\n",
    "\n",
    "classification_pipeline.fit(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3326,
     "status": "ok",
     "timestamp": 1563889339617,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "lk1WfAwhPlFw",
    "outputId": "efe47984-d955-4d24-8969-17eac508f3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Label 1 is for positive prediction, 0 - for negative one.\n",
    "print (classification_pipeline.predict([\n",
    "    \"Amazing film! I will advice it to all my friends. Genious\",\n",
    "    \"Awful film! The man who advised me to watch it is really crazy idiot.\",\n",
    "    \"I did not find this movie so great!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgLa9KfoP0Ia"
   },
   "source": [
    "### Reducing dimentionality of feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2298,
     "status": "ok",
     "timestamp": 1563889378972,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "W2kuSSCztl8Z",
    "outputId": "63109839-b738-40e8-cf75-3c4cc0d3c020"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39659"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing number of all present terms:\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(texts)\n",
    "len(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTI86AnOJ0_P"
   },
   "source": [
    "We can see that the matrix is quite large and sparse. We need to [reduce dimentionality](https://en.wikipedia.org/wiki/Dimensionality_reduction) (or in other words smoothen or select the features to reduce their number). Read here about common [dimentionality reduction methods](https://www.kdnuggets.com/2015/05/7-methods-data-dimensionality-reduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Zx0y0nGP7S_"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "v = CountVectorizer()\n",
    "mx = v.fit_transform(texts)\n",
    "mf = TruncatedSVD(10)\n",
    "u = mf.fit_transform(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25977,
     "status": "ok",
     "timestamp": 1563889415389,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "Ntp40MYHQEF1",
    "outputId": "2b7db28d-e0d7-4304-ee69-c995ef565be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.decomposition.truncated_svd.TruncatedSVD'>\n",
      "0.538983594372816\n",
      "<class 'sklearn.decomposition.nmf.NMF'>\n",
      "0.6430082777388167\n"
     ]
    }
   ],
   "source": [
    "for transform in [TruncatedSVD, NMF]:\n",
    "    print (transform)\n",
    "    print (cross_val_score(\n",
    "              text_classifier(\n",
    "                  CountVectorizer(),\n",
    "                  transform(n_components=10),\n",
    "                  LinearSVC()),\n",
    "              texts,\n",
    "              labels)\n",
    "           .mean())\n",
    "#    The results look way lower than with the simple methods we tried earlier.\n",
    "#    What if we set n_components to something highers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 120639,
     "status": "ok",
     "timestamp": 1563889739112,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "AGyMXJ89QxXu",
    "outputId": "0887a3b7-be01-4b90-b505-ad7d3f9d0363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8120171069272866\n"
     ]
    }
   ],
   "source": [
    "#    Important: will take about 3 minutes :)\n",
    "print (cross_val_score(\n",
    "          text_classifier(\n",
    "              CountVectorizer(),\n",
    "              TruncatedSVD(n_components=1000),\n",
    "              LinearSVC()),\n",
    "          texts,\n",
    "          labels)\n",
    "       .mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrKoShoLRjU3"
   },
   "source": [
    "### Trying ensemble trees on top of reduced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kc6uTcoHRcwX"
   },
   "outputs": [],
   "source": [
    "#    Again some more imports\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16796,
     "status": "ok",
     "timestamp": 1563889833185,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "IqllTzYzRvPn",
    "outputId": "0a50d443-189b-47d7-9fcb-404bb03b65f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71951742161323\n"
     ]
    }
   ],
   "source": [
    "print (cross_val_score(\n",
    "    Pipeline([\n",
    "            (\"vectorizer\", CountVectorizer()),\n",
    "            (\"transformer\", TruncatedSVD(100)),\n",
    "            (\"classifier\", RandomForestClassifier(100))\n",
    "        ]),\n",
    "    texts,\n",
    "    labels\n",
    "    ).mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 204213,
     "status": "ok",
     "timestamp": 1563890026241,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "O7TDeGadRvaf",
    "outputId": "4f59340f-3a40-4c3c-9185-81321c18fc1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7130094166022309\n"
     ]
    }
   ],
   "source": [
    "#    We can try more components and more trees :) \n",
    "#    Note: will run about 5 minutes.\n",
    "print (cross_val_score(\n",
    "         text_classifier(\n",
    "             CountVectorizer(),\n",
    "             TruncatedSVD(n_components=1000),\n",
    "             RandomForestClassifier(1000)),\n",
    "         texts,\n",
    "         labels)\n",
    "      .mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCt-eCyULukW"
   },
   "source": [
    "### Trying with other initial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 212157,
     "status": "ok",
     "timestamp": 1563890131077,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "iSUCyFFrL5vW",
    "outputId": "0a24f90e-8d8d-41c7-9db7-7da964567f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8405201608794424\n"
     ]
    }
   ],
   "source": [
    "#    Let's now see what we will get if we use TFIDF (TfidfVectorizer)\n",
    "#    instead of words frequencies (CountVectorizer).\n",
    "#    Note: will take about 3 minutes.\n",
    "print (cross_val_score(\n",
    "          text_classifier(\n",
    "              TfidfVectorizer(),\n",
    "              TruncatedSVD(n_components=1000),\n",
    "              LinearSVC()),\n",
    "          texts,\n",
    "          labels)\n",
    "       .mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 403696,
     "status": "ok",
     "timestamp": 1563890324223,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "DG4ccpO1Rvi2",
    "outputId": "6f2f1ac7-57ee-4ad0-ff52-2dfa14296394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5789936643230057\n"
     ]
    }
   ],
   "source": [
    "#    Note: will run about 5 minutes.\n",
    "print (cross_val_score(\n",
    "         text_classifier(\n",
    "             TfidfVectorizer(),\n",
    "             TruncatedSVD(n_components=1000),\n",
    "             RandomForestClassifier(1000)),\n",
    "         texts,\n",
    "         labels)\n",
    "      .mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNCOH7GBSx4Q"
   },
   "source": [
    "### Combining TFIDF and SVD features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grzr14Pf1BOP"
   },
   "source": [
    "We combine TF-IDF features and the reduced-dimentionality representation by SVD. In contrast with above transformation, here both sparse (TF-IDF of terms) and dense (SVD representation) features will be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "rP3Wk8x7SwCz"
   },
   "outputs": [],
   "source": [
    "#    Import Feature union\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "estimators = [('tfidf', TfidfTransformer()), ('svd', TruncatedSVD(100))]\n",
    "combined = FeatureUnion(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23201,
     "status": "ok",
     "timestamp": 1563890383468,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "raCRkEn5TABl",
    "outputId": "6a7979db-08ad-4f34-f808-6aa5bdc5daea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7870145594696493\n"
     ]
    }
   ],
   "source": [
    "print (cross_val_score(\n",
    "          Pipeline([\n",
    "              (\"vectorizer\", CountVectorizer()),\n",
    "              (\"transformer\", combined),\n",
    "              (\"classifier\", LinearSVC())\n",
    "              ]),\n",
    "          texts,\n",
    "         labels)\n",
    "      .mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zd5Qc4DDdj1t"
   },
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9EJ_j8yAkT7"
   },
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BscTaIKQm5wr"
   },
   "source": [
    "[Word embeddings](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795) is a set of language modeling and feature learning techniques in\n",
    "<br>NLP where words/phrases/sentences/paragraphs/documents are mapped to vectors of real numbers (usually of less dimentilnality).\n",
    "\n",
    "The idea about this form of dimentionality reduction is to capture semantic/morphological/contextual/hierarchical/etc\n",
    "<br>information as possible from the original text. While training the models to find the embeddings, several directions could be taken:\n",
    "<br>(1) preserving the [morphological structure](https://arxiv.org/pdf/1607.04606.pdf) (subword information, etc.);\n",
    "<br>(2) [word context](https://arxiv.org/pdf/1411.2738.pdf) representation;\n",
    "<br>(3) [global corpus statistics](https://nlp.stanford.edu/pubs/glove.pdf);\n",
    "<br>(4) [word hierarchy](https://arxiv.org/pdf/1705.08039.pdf) as in WordNet;\n",
    "<br>(5) [relationship between documents](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html) and the terms they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaqVQtNIA0cO"
   },
   "source": [
    "* **CBOW**\n",
    "\n",
    "An approach where we train a model to predict one word given another word (given a set of words in the context).\n",
    "<br>$ p(w_i | w_j) $. To do so, we train a neural network with the following structure:\n",
    "\n",
    "input (one-hot encoded vectors for the words, V x 1; where V is the size of the vocabulary) -> \n",
    "\n",
    "hidden layer (weight matrix W, V x N; where N is the size of the hidden layer) ->\n",
    "\n",
    "output (weight matrix W', N x V) + softmax\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*6OSkLBIxnXOwloUozZljjw.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLQziTlBA4AX"
   },
   "source": [
    "* **Skip-gram**\n",
    "\n",
    "Opposite to the previous version, we try to predict the context words by a target word in the input\n",
    "<br>$ \\sum_{all\\ words\\ i} \\sum_{j - k}^{j + k} p( w_{j}| w_i) $,\n",
    "<br>where $k$ is the window of context we are trying to predic with a given target workd $w_i$.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*pRV0o3uuY7n5NdHhAaAzlw.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnEifnCIA6_x"
   },
   "source": [
    "* **[Glove](https://nlp.stanford.edu/pubs/glove.pdf)**\n",
    "\n",
    "This approach tries to capture the meaning of one word embedding with the structure of the whole observed corpus.\n",
    "<br>This model is trained on the global co-occurrence counts and uses the word statistics.\n",
    "<br>More details and explations are either in the paper or in [this tutorial](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XR__LKQtA7NE"
   },
   "source": [
    "* **FastText**\n",
    "\n",
    "FastText is basically skip-gram model with negative sampling (trying to max the probability of the actual context words and\n",
    "<br>minimize the probability of random words taken from the dictionary) but also we can add the internal structure of words\n",
    "<br>by splitting them into the bag of characters n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X57L-n1xAzcB"
   },
   "source": [
    "Fast and easy to start with direction, if you need to train the embeddings on your own corpus:\n",
    "<br>Embeddings can be trained on your provided corpus, e.g. using [FastText](https://github.com/facebookresearch/fastText).\n",
    "([Wikipedia dumps](https://dumps.wikimedia.org/) could be of use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjm-CQAxm3mV"
   },
   "source": [
    "### Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkYNGi2VAyf4"
   },
   "source": [
    "#### Open-source, external embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpWWsF52ytge"
   },
   "source": [
    "[Fasttext models](https://fasttext.cc/docs/en/english-vectors.html):\n",
    "* [crawl-300d-2M.vec.zip](https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip): 2 million word vectors trained on Common Crawl (600B tokens).\n",
    "* [wiki-news-300d-1M.vec.zip](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip): 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
    "* [wiki-news-300d-1M-subword.vec.zip](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M-subword.vec.zip): 1 million word vectors trained with subword infomation on Wikipedia 2017,\n",
    "<br>UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
    "* [Wiki word vectors](https://fasttext.cc/docs/en/pretrained-vectors.html), dim=300: [wiki.en.zip](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip): bin+text model\n",
    "\n",
    "[Google Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "* Pretrained word/phrase vectors:\n",
    "  * [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n",
    "  * [GoogleNews-vectors-negative300-SLIM.bin.gz](https://github.com/eyaler/word2vec-slim/blob/master/GoogleNews-vectors-negative300-SLIM.bin.gz): slim version with app. 300k words\n",
    "* Pretrained entity vectors:\n",
    "  * [freebase-vectors-skipgram1000.bin.gz](https://docs.google.com/file/d/0B7XkCwpI5KDYaDBDQm1tZGNDRHc/edit?usp=sharing): Entity vectors trained on 100B words from various news articles\n",
    "  * [freebase-vectors-skipgram1000-en.bin.gz](https://docs.google.com/file/d/0B7XkCwpI5KDYeFdmcVltWkhtbmM/edit?usp=sharing): Entity vectors trained on 100B words from various news articles,\n",
    "  <br>using the deprecated /en/ naming (more easily readable); the vectors are sorted by frequency\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/): Global Vectors for Word Representation\n",
    "* [glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip): Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download).\n",
    "<br>Here's an example in action. TODO: Add a link.\n",
    "* [glove.840B.300d.zip](http://nlp.stanford.edu/data/glove.840B.300d.zip): Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ifIvKLElABi"
   },
   "source": [
    "Let's load some pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "cxMhjhbu0DAB"
   },
   "outputs": [],
   "source": [
    "#@title [Fix it if you can later] Some potential help to load the model from drive\n",
    "\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from google.colab import auth\n",
    "from io import BytesIO\n",
    "from oauth2client.client import GoogleCredentials\n",
    "!pip install pydrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "import shutil\n",
    "import smart_open\n",
    "from tempfile import NamedTemporaryFile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def authentication():\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "  return drive\n",
    "\n",
    "def read_zipfile(input_file_id='glove.6B.zip'):\n",
    "  drive = authentication()\n",
    "\n",
    "  file_id = drive.ListFile({'q':\"title='\"+input_file_id+\"'\"}).GetList()[0]['id']\n",
    "  f = drive.CreateFile({'id': file_id})\n",
    "  zf_string = f.GetContentString(encoding='cp862')\n",
    "  zf_bytes = BytesIO(zf_string.encode('cp862'))\n",
    "  return ZipFile(zf_bytes, 'r')\n",
    "\n",
    "def read_files_zipfile(zf):\n",
    "  _files = []\n",
    "  _file = {}\n",
    "  for file_name in zf.namelist():\n",
    "    _file['name'] = file_name\n",
    "    _file['data'] = zf.read(file_name)\n",
    "    _files.append(_file)\n",
    "    _file = {}\n",
    "  return _files\n",
    "\n",
    "def get_lines(zipfile, file_name):\n",
    "  with zipfile.open(file_name, 'r') as f:\n",
    "    num_lines = sum(1 for line in f) - 1\n",
    "  with zipfile.open(file_name, 'r') as f:\n",
    "    num_dims = len(f.readline().split()) - 1\n",
    "  return num_lines, num_dims\n",
    "\n",
    "def prepend_slow(zipfile, file_name, line):\n",
    "  tmp_file_name = None\n",
    "  with zf.open(file_name, 'r') as fin:\n",
    "    with NamedTemporaryFile(delete=False) as tmp_file:\n",
    "      tmp_file.write(gensim_first_line + b'\\n')\n",
    "      for gensim_first_line in fin:\n",
    "        tmp_file.write(gensim_first_line)\n",
    "      tmp_file_name = tmp_file.name\n",
    "\n",
    "  if tmp_file_name is not None:\n",
    "    return tmp_file_name\n",
    "  \n",
    "def drive_upload_content_file(drive, file_name, tmp_file_path, parent_id, verbose=True):\n",
    "  metadata = {'title': file_name}\n",
    "  if parent_id:\n",
    "    metadata['parents'] = [{'kind': 'drive#fileLink', 'id': parent_id}]\n",
    "  uploaded = drive.CreateFile(metadata)\n",
    "  uploaded.SetContentFile(tmp_file_path)\n",
    "  uploaded.Upload()\n",
    "  if verbose:\n",
    "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
    "\n",
    "def glove_to_word2vec(zipfile, file_name, **kwargs):\n",
    "  num_lines, num_dims = get_lines(zipfile, file_name)\n",
    "  gensim_first_line = '{} {}'.format(num_lines, num_dims)\n",
    "  gensim_first_line = gensim_first_line.encode()\n",
    "  tmp_file_name = prepend_slow(zipfile, file_name, gensim_first_line)\n",
    "  print (tmp_file_name)\n",
    "  \n",
    "  verbose = kwargs.get('verbose', True)\n",
    "  parent_id = kwargs.get('parent_id', None)\n",
    "  \n",
    "  if tmp_file_name is not None:\n",
    "    drive = authentication()\n",
    "    drive_upload_content_file(drive, file_name, tmp_file_name, parent_id, verbose)\n",
    "    \n",
    "# zf = read_zipfile('glove.6B.zip')\n",
    "# print ([f['name'] for f in read_files_zipfile(zf)])\n",
    "\n",
    "# glove_to_word2vec(zf, 'glove.6B/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pgsa_fuTAsMz"
   },
   "source": [
    "#### Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9809,
     "status": "ok",
     "timestamp": 1563891701101,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "gt9ewyNvVb5G",
    "outputId": "e62e870f-b324-4200-d3cf-453a8db3351b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.189)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.189)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 149383,
     "status": "ok",
     "timestamp": 1563891846883,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "3c06X9JdsHCw",
    "outputId": "03182130-3df9-4aee-922d-7ef07ca48708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-23 14:21:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-07-23 14:21:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2019-07-23 14:21:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: â€˜glove.6B.zip.1â€™\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  21.2MB/s    in 73s     \n",
      "\n",
      "2019-07-23 14:22:55 (11.3 MB/s) - â€˜glove.6B.zip.1â€™ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: glove.6B.50d.txt        \n",
      "\n",
      "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [{ENTER}]\n",
      "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
      "error:  invalid response [a]\n",
      "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "\n",
      "  inflating: glove.6B.300d.txt       n\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "#    Note: it might take several minutes. Be patient!\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove2word2vec(glove_input_file='glove.6B.50d.txt',\n",
    "               word2vec_output_file=\"gensim_glove_vectors.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwSYdt_jCdU0"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 149882,
     "status": "ok",
     "timestamp": 1563891870311,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "wkkilBkJB7Aa",
    "outputId": "ddd5fd6c-cd82-466a-ce77-4c08422e064b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caravaggio', 0.7772032618522644),\n",
       " ('michelangelo', 0.7727645039558411),\n",
       " ('painting', 0.7709366083145142),\n",
       " ('rembrandt', 0.745723307132721),\n",
       " ('titian', 0.742861807346344),\n",
       " ('gogh', 0.7370840311050415),\n",
       " ('picasso', 0.7252718806266785),\n",
       " ('painter', 0.7237308025360107),\n",
       " ('altarpiece', 0.7143588662147522),\n",
       " ('monet', 0.7027225494384766)]"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model[\"madonna\"] - model[\"singer\"] + model[\"painter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14178,
     "status": "ok",
     "timestamp": 1563891870719,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "uvH2RLrdDGVy",
    "outputId": "795ae2db-8046-4d4a-b694-abbc0e1908df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('koizumi', 0.8049983978271484),\n",
       " ('hashimoto', 0.7850980758666992),\n",
       " ('putin', 0.7768493294715881),\n",
       " ('obama', 0.7716479301452637),\n",
       " ('barack', 0.747847318649292),\n",
       " ('bush', 0.7318081259727478),\n",
       " ('junichiro', 0.7254975438117981),\n",
       " ('clinton', 0.7190637588500977),\n",
       " ('ryutaro', 0.7096714973449707),\n",
       " ('stimulus', 0.7046528458595276)]"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model[\"obama\"] - model[\"usa\"] + model[\"japan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11987,
     "status": "ok",
     "timestamp": 1563891870721,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "dUpp0hzOqU0N",
    "outputId": "9143572c-ee65-4964-a9b5-704e3321f0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " earthquake \n",
      "---\n",
      "('quake', 0.9659477472305298)\n",
      "('temblor', 0.8302474021911621)\n",
      "('tsunami', 0.825542688369751)\n",
      "('tremor', 0.822434663772583)\n",
      "('magnitude', 0.7913191318511963)\n",
      "('disaster', 0.7809879183769226)\n",
      "('earthquakes', 0.7694811820983887)\n",
      "('epicenter', 0.7664315700531006)\n",
      "('jolted', 0.7571674585342407)\n",
      "('aftershock', 0.7534422278404236)\n",
      "---\n",
      " happy \n",
      "---\n",
      "(\"'m\", 0.9142324328422546)\n",
      "('everyone', 0.8976402878761292)\n",
      "('everybody', 0.8965489864349365)\n",
      "('really', 0.8839760422706604)\n",
      "('me', 0.8784631490707397)\n",
      "('definitely', 0.8762789368629456)\n",
      "('maybe', 0.875670313835144)\n",
      "(\"'d\", 0.8718012571334839)\n",
      "('feel', 0.8707678318023682)\n",
      "('i', 0.8707453608512878)\n",
      "---\n",
      " i \n",
      "---\n",
      "(\"'d\", 0.9566037654876709)\n",
      "('me', 0.9546188116073608)\n",
      "('maybe', 0.930670440196991)\n",
      "('know', 0.927360475063324)\n",
      "(\"n't\", 0.9259788393974304)\n",
      "('you', 0.9237291812896729)\n",
      "(\"'m\", 0.9199258089065552)\n",
      "('else', 0.9198518395423889)\n",
      "('never', 0.9197366237640381)\n",
      "('really', 0.9190417528152466)\n",
      "---\n",
      " queen \n",
      "---\n",
      "('princess', 0.851516604423523)\n",
      "('lady', 0.805060863494873)\n",
      "('elizabeth', 0.787304162979126)\n",
      "('king', 0.7839042544364929)\n",
      "('prince', 0.7821860909461975)\n",
      "('coronation', 0.7692778706550598)\n",
      "('consort', 0.7626097202301025)\n",
      "('royal', 0.7442864775657654)\n",
      "('crown', 0.7382649779319763)\n",
      "('victoria', 0.7285771369934082)\n"
     ]
    }
   ],
   "source": [
    "#@title Find closest words to the input word\n",
    "input_words = ['earthquake', 'happy', 'i', 'queen']\n",
    "\n",
    "for word in input_words:\n",
    "  print (\"---\\n\", word, \"\\n---\")\n",
    "  for similar_word in model.most_similar(word):\n",
    "    print (similar_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upaCdTNmx4Ye"
   },
   "source": [
    "Nice visualization of some pretrained embeddings: https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6yQ0BjBwCqm"
   },
   "source": [
    "## Exercise: Use word embeddings in sentiment analysis representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYvnotcnzYWG"
   },
   "source": [
    "*Exercise*: Define your own vectorizer based on the pretrained word embeddings. Compare the evaluation results with the previous methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8MuY55lwOht"
   },
   "source": [
    "Type your solution in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqltfbVnv9u-"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jquk5hoGFCqp"
   },
   "source": [
    "Hint: See in the preprocessing part for how to represent documents using given embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnjcHNZWv-iS"
   },
   "source": [
    "### <font color=\"grey\">Exercise solution:  Use word embeddings in sentiment analysis representation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUEyKL6HnvIN"
   },
   "outputs": [],
   "source": [
    "# Represent documents as mean of embeddings.\n",
    "import numpy as np\n",
    "num_dimension = model.vector_size  # Get dimensionality out of the model.\n",
    "document_embeddings = []\n",
    "for document in texts:\n",
    "  document_word_embeddings = \\\n",
    "      [model[token] for token in document if token in model]\n",
    "  if (len(document_word_embeddings) > 0):\n",
    "    document_embeddings.append(np.mean(document_word_embeddings, axis=0))\n",
    "  else:\n",
    "    document_embeddings.append(np.zeros(num_dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ego1PbC3QVgQ"
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "lgb = lightgbm.LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6000,
     "status": "ok",
     "timestamp": 1563892534586,
     "user": {
      "displayName": "Denis Smirnov",
      "photoUrl": "https://lh5.googleusercontent.com/-cU8bCLINHdw/AAAAAAAAAAI/AAAAAAAAJco/e-BbiVggGKw/s64/photo.jpg",
      "userId": "05254297392724699439"
     },
     "user_tz": -180
    },
    "id": "BzNiafEDofKB",
    "outputId": "36b9ef92-2643-4a70-9667-ee232597eba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "0.5875\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "0.5985\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "0.55\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "0.5865\n"
     ]
    }
   ],
   "source": [
    "for classifier in [LogisticRegression, LinearSVC, RandomForestClassifier, lgb]:\n",
    "  print (classifier)\n",
    "  print (cross_val_score(\n",
    "            classifier(), \n",
    "            document_embeddings,\n",
    "            labels,\n",
    "            cv = 5)\n",
    "         .mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3SfBdEQLo1eD"
   },
   "source": [
    "## *Challenges of sentiment analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ro-1o2qZUOM8"
   },
   "source": [
    "Though we have simply applied some transformations that we have studies, there is usually more than that to each particular problem.\n",
    "<br>While for some problems Bag-Of-Words (BOW) representations could work pretty well,\n",
    "<br>some problems might require more specific preprocessing, data representation, etc.\n",
    "\n",
    "For example, in case of sentiment analysis the following phenomena should be treated:\n",
    "1. Negation\n",
    "<br>\n",
    "Plain BOW representation -  \\[\"not\", \"hate\"\\] vs. \\[\"hate\"\\] - might seem to be quite similar, whereas they represent completely opposite polarities.\n",
    "<br>Usually, some rule based approaches could be used on the preprocessing or data representation step\n",
    "<br>(like creating syntactic negated term ('NOT_hate') or merge all negations to the same bucket/feature).\n",
    "2. Modulated/contextual polarity\n",
    "<br>\n",
    "Some words could have completely different meanings and polarity in different contexts.\n",
    "<br>E.g., \"high resolution\" is rather positive, while \"high price\" is usually negative one.\n",
    "<br>Of course, such information is never stated explicitly and rather is common sense knowledge to us.\n",
    "3. Domain specificity\n",
    "<br>\n",
    "Some expressions might be either positive and negative in different domains.\n",
    "<br>For example, in the teenage movie review swearing words might be considered positive, while,\n",
    "<br>for the documentary such words are either out of place or mostly negative.\n",
    "<br>Or, depending on the news source, 'education abroad' could be negative for the country\n",
    "<br>(as people tend to stay abroad afterwards), or rather positive in the context of educational opportunities, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqhreqpeo4DM"
   },
   "source": [
    "## More advanced techniques for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXheD8hQyTzl"
   },
   "source": [
    "There already exist some deep learning solutions with great accuracy in predicting polarity of texts.\n",
    "<br>You can even try and see if such system can catch your sentiments.\n",
    "<br>\n",
    "One of the approaches that outperforms \"old-school\" feature/rule based systems is based on the [reccurent neural network](http://en.wikipedia.org/wiki/Recurrent_neural_network) and made in [Stanford](https://nlp.stanford.edu/sentiment/).\n",
    "\n",
    "Let's try the [live demo](http://nlp.stanford.edu:8080/sentiment/rntnDemo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53mXd8Lhg_Ra"
   },
   "source": [
    "# ---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MDuZ1Bj95_Tg",
    "ctAAN8LC577n",
    "ZnCAcjjCgpWC",
    "1yIQpdKm9n7Q",
    "4PrjZyLUhArH",
    "kc5PfB3nh21k",
    "S21l8P2fh5qY",
    "WWCgpFuBhfHK",
    "V2ixTbyaYHdA",
    "_87XglopYOWB",
    "8esm8NmvhlRl",
    "BV5y4isKEfW1",
    "J6FKVcc3O6nf",
    "FztH9XpHj2Hl",
    "YJXXKnhlM6fK",
    "wRzww7KH0jZp",
    "hB7WabSt0n32",
    "BgLa9KfoP0Ia",
    "xrKoShoLRjU3",
    "oCt-eCyULukW",
    "bNCOH7GBSx4Q",
    "i6yQ0BjBwCqm",
    "HnjcHNZWv-iS",
    "3SfBdEQLo1eD",
    "vqhreqpeo4DM"
   ],
   "name": "Copy of 3.Classification.ipynb",
   "provenance": [
    {
     "file_id": "1yfcZoLREXgDSI2VvTUhaR08oJTywYQzf",
     "timestamp": 1563881553132
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
